<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
    "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<meta name="generator" content="AsciiDoc 8.5.2" />
<title>Flume User Guide</title>
<style type="text/css">
/* Debug borders */
p, li, dt, dd, div, pre, h1, h2, h3, h4, h5, h6 {
/*
  border: 1px solid red;
*/
}

body {
  margin: 1em 5% 1em 5%;
}

a {
  color: blue;
  text-decoration: underline;
}
a:visited {
  color: fuchsia;
}

em {
  font-style: italic;
  color: navy;
}

strong {
  font-weight: bold;
  color: #083194;
}

tt {
  color: navy;
}

h1, h2, h3, h4, h5, h6 {
  color: #527bbd;
  font-family: sans-serif;
  margin-top: 1.2em;
  margin-bottom: 0.5em;
  line-height: 1.3;
}

h1, h2, h3 {
  border-bottom: 2px solid silver;
}
h2 {
  padding-top: 0.5em;
}
h3 {
  float: left;
}
h3 + * {
  clear: left;
}

div.sectionbody {
  font-family: serif;
  margin-left: 0;
}

hr {
  border: 1px solid silver;
}

p {
  margin-top: 0.5em;
  margin-bottom: 0.5em;
}

ul, ol, li > p {
  margin-top: 0;
}

pre {
  padding: 0;
  margin: 0;
}

span#author {
  color: #527bbd;
  font-family: sans-serif;
  font-weight: bold;
  font-size: 1.1em;
}
span#email {
}
span#revnumber, span#revdate, span#revremark {
  font-family: sans-serif;
}

div#footer {
  font-family: sans-serif;
  font-size: small;
  border-top: 2px solid silver;
  padding-top: 0.5em;
  margin-top: 4.0em;
}
div#footer-text {
  float: left;
  padding-bottom: 0.5em;
}
div#footer-badges {
  float: right;
  padding-bottom: 0.5em;
}

div#preamble {
  margin-top: 1.5em;
  margin-bottom: 1.5em;
}
div.tableblock, div.imageblock, div.exampleblock, div.verseblock,
div.quoteblock, div.literalblock, div.listingblock, div.sidebarblock,
div.admonitionblock {
  margin-top: 1.0em;
  margin-bottom: 1.5em;
}
div.admonitionblock {
  margin-top: 2.0em;
  margin-bottom: 2.0em;
  margin-right: 10%;
  color: #606060;
}

div.content { /* Block element content. */
  padding: 0;
}

/* Block element titles. */
div.title, caption.title {
  color: #527bbd;
  font-family: sans-serif;
  font-weight: bold;
  text-align: left;
  margin-top: 1.0em;
  margin-bottom: 0.5em;
}
div.title + * {
  margin-top: 0;
}

td div.title:first-child {
  margin-top: 0.0em;
}
div.content div.title:first-child {
  margin-top: 0.0em;
}
div.content + div.title {
  margin-top: 0.0em;
}

div.sidebarblock > div.content {
  background: #ffffee;
  border: 1px solid silver;
  padding: 0.5em;
}

div.listingblock > div.content {
  border: 1px solid silver;
  background: #f4f4f4;
  padding: 0.5em;
}

div.quoteblock, div.verseblock {
  padding-left: 1.0em;
  margin-left: 1.0em;
  margin-right: 10%;
  border-left: 5px solid #dddddd;
  color: #777777;
}

div.quoteblock > div.attribution {
  padding-top: 0.5em;
  text-align: right;
}

div.verseblock > div.content {
  white-space: pre;
}
div.verseblock > div.attribution {
  padding-top: 0.75em;
  text-align: left;
}
/* DEPRECATED: Pre version 8.2.7 verse style literal block. */
div.verseblock + div.attribution {
  text-align: left;
}

div.admonitionblock .icon {
  vertical-align: top;
  font-size: 1.1em;
  font-weight: bold;
  text-decoration: underline;
  color: #527bbd;
  padding-right: 0.5em;
}
div.admonitionblock td.content {
  padding-left: 0.5em;
  border-left: 3px solid #dddddd;
}

div.exampleblock > div.content {
  border-left: 3px solid #dddddd;
  padding-left: 0.5em;
}

div.imageblock div.content { padding-left: 0; }
span.image img { border-style: none; }
a.image:visited { color: white; }

dl {
  margin-top: 0.8em;
  margin-bottom: 0.8em;
}
dt {
  margin-top: 0.5em;
  margin-bottom: 0;
  font-style: normal;
  color: navy;
}
dd > *:first-child {
  margin-top: 0.1em;
}

ul, ol {
    list-style-position: outside;
}
ol.arabic {
  list-style-type: decimal;
}
ol.loweralpha {
  list-style-type: lower-alpha;
}
ol.upperalpha {
  list-style-type: upper-alpha;
}
ol.lowerroman {
  list-style-type: lower-roman;
}
ol.upperroman {
  list-style-type: upper-roman;
}

div.compact ul, div.compact ol,
div.compact p, div.compact p,
div.compact div, div.compact div {
  margin-top: 0.1em;
  margin-bottom: 0.1em;
}

div.tableblock > table {
  border: 3px solid #527bbd;
}
thead, p.table.header {
  font-family: sans-serif;
  font-weight: bold;
}
tfoot {
  font-weight: bold;
}
td > div.verse {
  white-space: pre;
}
p.table {
  margin-top: 0;
}
/* Because the table frame attribute is overriden by CSS in most browsers. */
div.tableblock > table[frame="void"] {
  border-style: none;
}
div.tableblock > table[frame="hsides"] {
  border-left-style: none;
  border-right-style: none;
}
div.tableblock > table[frame="vsides"] {
  border-top-style: none;
  border-bottom-style: none;
}


div.hdlist {
  margin-top: 0.8em;
  margin-bottom: 0.8em;
}
div.hdlist tr {
  padding-bottom: 15px;
}
dt.hdlist1.strong, td.hdlist1.strong {
  font-weight: bold;
}
td.hdlist1 {
  vertical-align: top;
  font-style: normal;
  padding-right: 0.8em;
  color: navy;
}
td.hdlist2 {
  vertical-align: top;
}
div.hdlist.compact tr {
  margin: 0;
  padding-bottom: 0;
}

.comment {
  background: yellow;
}

.footnote, .footnoteref {
  font-size: 0.8em;
}

span.footnote, span.footnoteref {
  vertical-align: super;
}

#footnotes {
  margin: 20px 0 20px 0;
  padding: 7px 0 0 0;
}

#footnotes div.footnote {
  margin: 0 0 5px 0;
}

#footnotes hr {
  border: none;
  border-top: 1px solid silver;
  height: 1px;
  text-align: left;
  margin-left: 0;
  width: 20%;
  min-width: 100px;
}


@media print {
  div#footer-badges { display: none; }
}

div#toc {
  margin-bottom: 2.5em;
}

div#toctitle {
  color: #527bbd;
  font-family: sans-serif;
  font-size: 1.1em;
  font-weight: bold;
  margin-top: 1.0em;
  margin-bottom: 0.1em;
}

div.toclevel1, div.toclevel2, div.toclevel3, div.toclevel4 {
  margin-top: 0;
  margin-bottom: 0;
}
div.toclevel2 {
  margin-left: 2em;
  font-size: 0.9em;
}
div.toclevel3 {
  margin-left: 4em;
  font-size: 0.9em;
}
div.toclevel4 {
  margin-left: 6em;
  font-size: 0.9em;
}
/* Workarounds for IE6's broken and incomplete CSS2. */

div.sidebar-content {
  background: #ffffee;
  border: 1px solid silver;
  padding: 0.5em;
}
div.sidebar-title, div.image-title {
  color: #527bbd;
  font-family: sans-serif;
  font-weight: bold;
  margin-top: 0.0em;
  margin-bottom: 0.5em;
}

div.listingblock div.content {
  border: 1px solid silver;
  background: #f4f4f4;
  padding: 0.5em;
}

div.quoteblock-attribution {
  padding-top: 0.5em;
  text-align: right;
}

div.verseblock-content {
  white-space: pre;
}
div.verseblock-attribution {
  padding-top: 0.75em;
  text-align: left;
}

div.exampleblock-content {
  border-left: 3px solid #dddddd;
  padding-left: 0.5em;
}

/* IE6 sets dynamically generated links as visited. */
div#toc a:visited { color: blue; }
</style>
<script type="text/javascript">
/*<![CDATA[*/
window.onload = function(){asciidoc.footnotes(); asciidoc.toc(2);}
var asciidoc = {  // Namespace.

/////////////////////////////////////////////////////////////////////
// Table Of Contents generator
/////////////////////////////////////////////////////////////////////

/* Author: Mihai Bazon, September 2002
 * http://students.infoiasi.ro/~mishoo
 *
 * Table Of Content generator
 * Version: 0.4
 *
 * Feel free to use this script under the terms of the GNU General Public
 * License, as long as you do not remove or alter this notice.
 */

 /* modified by Troy D. Hanson, September 2006. License: GPL */
 /* modified by Stuart Rackham, 2006, 2009. License: GPL */

// toclevels = 1..4.
toc: function (toclevels) {

  function getText(el) {
    var text = "";
    for (var i = el.firstChild; i != null; i = i.nextSibling) {
      if (i.nodeType == 3 /* Node.TEXT_NODE */) // IE doesn't speak constants.
        text += i.data;
      else if (i.firstChild != null)
        text += getText(i);
    }
    return text;
  }

  function TocEntry(el, text, toclevel) {
    this.element = el;
    this.text = text;
    this.toclevel = toclevel;
  }

  function tocEntries(el, toclevels) {
    var result = new Array;
    var re = new RegExp('[hH]([2-'+(toclevels+1)+'])');
    // Function that scans the DOM tree for header elements (the DOM2
    // nodeIterator API would be a better technique but not supported by all
    // browsers).
    var iterate = function (el) {
      for (var i = el.firstChild; i != null; i = i.nextSibling) {
        if (i.nodeType == 1 /* Node.ELEMENT_NODE */) {
          var mo = re.exec(i.tagName);
          if (mo && (i.getAttribute("class") || i.getAttribute("className")) != "float") {
            result[result.length] = new TocEntry(i, getText(i), mo[1]-1);
          }
          iterate(i);
        }
      }
    }
    iterate(el);
    return result;
  }

  var toc = document.getElementById("toc");
  var entries = tocEntries(document.getElementById("content"), toclevels);
  for (var i = 0; i < entries.length; ++i) {
    var entry = entries[i];
    if (entry.element.id == "")
      entry.element.id = "_toc_" + i;
    var a = document.createElement("a");
    a.href = "#" + entry.element.id;
    a.appendChild(document.createTextNode(entry.text));
    var div = document.createElement("div");
    div.appendChild(a);
    div.className = "toclevel" + entry.toclevel;
    toc.appendChild(div);
  }
  if (entries.length == 0)
    toc.parentNode.removeChild(toc);
},


/////////////////////////////////////////////////////////////////////
// Footnotes generator
/////////////////////////////////////////////////////////////////////

/* Based on footnote generation code from:
 * http://www.brandspankingnew.net/archive/2005/07/format_footnote.html
 */

footnotes: function () {
  var cont = document.getElementById("content");
  var noteholder = document.getElementById("footnotes");
  var spans = cont.getElementsByTagName("span");
  var refs = {};
  var n = 0;
  for (i=0; i<spans.length; i++) {
    if (spans[i].className == "footnote") {
      n++;
      // Use [\s\S] in place of . so multi-line matches work.
      // Because JavaScript has no s (dotall) regex flag.
      note = spans[i].innerHTML.match(/\s*\[([\s\S]*)]\s*/)[1];
      noteholder.innerHTML +=
        "<div class='footnote' id='_footnote_" + n + "'>" +
        "<a href='#_footnoteref_" + n + "' title='Return to text'>" +
        n + "</a>. " + note + "</div>";
      spans[i].innerHTML =
        "[<a id='_footnoteref_" + n + "' href='#_footnote_" + n +
        "' title='View footnote' class='footnote'>" + n + "</a>]";
      var id =spans[i].getAttribute("id");
      if (id != null) refs["#"+id] = n;
    }
  }
  if (n == 0)
    noteholder.parentNode.removeChild(noteholder);
  else {
    // Process footnoterefs.
    for (i=0; i<spans.length; i++) {
      if (spans[i].className == "footnoteref") {
        var href = spans[i].getElementsByTagName("a")[0].getAttribute("href");
        href = href.match(/#.*/)[0];  // Because IE return full URL.
        n = refs[href];
        spans[i].innerHTML =
          "[<a href='#_footnote_" + n +
          "' title='View footnote' class='footnote'>" + n + "</a>]";
      }
    }
  }
}

}
/*]]>*/
</script>
</head>
<body>
<div id="header">
<h1>Flume User Guide</h1>
<span id="author">flume-dev@cloudera.org</span><br />
<span id="revnumber">version 0.9.1,</span>
<span id="revdate">August 9th 2010</span>
<div id="toc">
  <div id="toctitle">Table of Contents</div>
  <noscript><p><b>JavaScript must be enabled in your browser to display the table of contents.</b></p></noscript>
</div>
</div>
<div id="content">
<h2 id="_abstract">Abstract</h2>
<div class="sectionbody">
<div class="quoteblock">
<div class="quoteblock-content">
<div class="paragraph"><p>Flume is a distributed, reliable, and available service for
efficiently collecting, aggregating, and moving large amounts of log
data.  It has a simple and flexible architecture based on streaming
data flows.  It is robust and fault tolerant with tunable reliability
mechanisms and many failover and recovery mechanisms.  The system is
centrally managed and allows for intelligent dynamic management. It
uses a simple extensible data model that allows for online analytic
applications.</p></div>
</div>
<div class="quoteblock-attribution">
</div></div>
</div>
<h2 id="Introduction">Introduction</h2>
<div class="sectionbody">
<div class="paragraph"><p>Flume is a distributed, reliable, and available service for
efficiently moving large amounts of data soon after the data is
produced.  This release provides a scalable conduit to move data
around a cluster as well as reliable logging.</p></div>
<div class="paragraph"><p>The primary use case for Flume is as a logging system that gathers a
set of log files on every machine in a cluster and aggregates them to
a centralized persistent store such as the Hadoop Distributed File
System (HDFS).</p></div>
<div class="paragraph"><p>The system was designed with these four key goals in mind:</p></div>
<div class="ulist"><ul>
<li>
<p>
Reliability
</p>
</li>
<li>
<p>
Scalability
</p>
</li>
<li>
<p>
Manageability
</p>
</li>
<li>
<p>
Extensibility
</p>
</li>
</ul></div>
<div class="paragraph"><p>This section provides a high-level overview of Flume&#8217;s architecture
and describes how the four design goals are achieved.</p></div>
<h3 id="_architecture">Architecture</h3><div style="clear:left"></div>
<div class="paragraph"><p>Flume&#8217;s architecture is simple, robust, and flexible.  The main
abstraction in Flume is a stream-oriented <strong>data flow</strong>.  A data flow
describes the way a single stream of data is transferred and processed
from its point of generation to its eventual destination. Data flows
are composed of <strong>logical nodes</strong> that can transform or aggregate the
events they receive.  Logical nodes are wired together in chains to
form a data flow. The way in which they are wired is called the
logical node&#8217;s <strong>configuration</strong>.</p></div>
<div class="paragraph"><p>Controlling all this is the <strong>Flume Master</strong>, which is a separate
service with knowledge of all the physical and logical nodes in a
Flume installation. The Master assigns configurations to logical
nodes, and is responsible for communicating configuration updates by
the user to all logical nodes.  In turn, the logical nodes
periodically contact the master so they can share monitoring
information and check for updates to their configuration.</p></div>
<div class="imageblock">
<div class="content">
<img src="architecture.png" alt="architecture.png" />
</div>
</div>
<div class="paragraph"><p>The graph above shows a typical deployment of Flume that collects log
data from a set of application servers.  The deployment consists of a
number of <em>logical nodes</em>, arranged into three tiers. The first tier
is the <em>agent</em> tier.  Agent nodes are typically installed on the
machines that generate the logs and are your data&#8217;s initial point of
contact with Flume.  They forward data to the next tier of <em>collector
nodes</em>, which aggregate the separate data flows and forward them to
the final <em>storage tier</em>.</p></div>
<div class="paragraph"><p>For example, the agents could be machines listening for <tt>syslog</tt> data
or monitoring the logs of a service such as a web server or the Hadoop
JobTracker. The agents produce streams of data that are sent to the
collectors; the collectors then aggregate the streams into larger
streams which can be written efficiently to a storage tier such as
HDFS.</p></div>
<div class="paragraph"><p>Logical nodes are a very flexible abstraction. Every logical node has
just two components - a <em>source</em> and a <em>sink</em>. The source tells a
logical node where to collect data, and the sink tells it where to
send the data. The only difference between two logical nodes is how
the source and sink are configured. Both source and sink can
additionally be configured with <em>decorators</em> which perform some simple
processing on data as it passes through. In the previous example, the
collector and the agents are <em>running the same node software</em>.  The
Master assigns a configuration to each logical node at run-time - all
components of a node&#8217;s configuration are instantiated dynamically at
run-time, and therefore configurations can be changed many times
throughout the lifetime of a Flume service without having to restart
any Java processes or log into the machines themselves. In fact,
logical nodes themselves can be created and deleted dynamically.</p></div>
<div class="paragraph"><p>The source, sink, and optional decorators are a powerful set of
primitives. Flume uses this architecture to provide per-flow data
properties (for example durability guarantees, compression, or
batching), or to compute event metadata, or even generate new events
that are inserted into data flow.  A logical node can also send data
downstream to several logical nodes.  This allows for multiple flows
and each subflow can potentially be configured differently.  For
example, it’s possible to have one flow be a collection path,
delivering data reliably to a persistent store, while another branch
computes lightweight analytics to be delivered to an alerting system.</p></div>
<div class="sidebarblock">
<div class="sidebar-content">
<div class="sidebar-title">Logical and Physical Nodes</div>
<div class="paragraph"><p>It&#8217;s important to make the distinction between <em>logical nodes</em> and
<em>physical nodes</em>. A physical node corresponds to a single Java process
running on one machine in a single JVM instance. Usually there is just
one physical node per machine. Physical nodes act as containers for
<em>logical nodes</em>, which are wired together to form data flows. Each
physical node can play host to many logical nodes, and takes care of
arbitrating the assignment of machine resources between them. So,
although the agents and the collectors in the preceding example are
<em>logically</em> separate processes, they could be running on the same
<em>physical</em> node. Flume gives users the flexibility to specify where
the computation and data transfer are done. The remainder of this
guide describes logical nodes unless stated otherwise.</p></div>
</div></div>
<h3 id="_reliability">Reliability</h3><div style="clear:left"></div>
<div class="paragraph"><p>Reliability, the ability to continue delivering events in the face of
failures without losing data, is a vital feature of Flume. Large
distributed systems can and do suffer partial failures in many ways -
physical hardware can fail, resources such as network bandwidth or
memory can become scarce, or software can crash or run slowly. Flume
emphasizes fault-tolerance as a core design principle and keeps
running and collecting data even when many components have failed.</p></div>
<div class="paragraph"><p>Flume can guarantee that all data received by an agent node will
eventually make it to the collector at the end of its flow as long as
the agent node keeps running.  That is, data can be <strong>reliably</strong>
delivered to its eventual destination.</p></div>
<div class="paragraph"><p>However, reliable delivery can be very resource intensive and is often
a stronger guarantee than some data sources require. Therefore, Flume
allows the user to specify, on a per-flow basis, the level of
reliability required. There are three supported reliability levels:</p></div>
<div class="ulist"><ul>
<li>
<p>
End-to-end
</p>
</li>
<li>
<p>
Store on failure
</p>
</li>
<li>
<p>
Best effort
</p>
</li>
</ul></div>
<div class="sidebarblock">
<div class="sidebar-content">
<div class="sidebar-title">A Note About Reliability</div>
<div class="paragraph"><p>Although Flume is extremely tolerant to machine, network, and software
failures, there is never any such thing as <em>100% reliability</em>. If all
the machines in a Flume installation were irrevocably destroyed in
some terrible data center incident, all copies of Flume&#8217;s data would
be lost and there would be no way to recover them. Therefore all of
Flume&#8217;s reliability levels make guarantees about data delivery <em>until
some maximum number of failures have occurred</em>. Flume&#8217;s failure modes
- in terms of what can fail and what will keep running if they do -
are described in detail later in this guide.</p></div>
</div></div>
<div class="paragraph"><p>The <strong>end-to-end</strong> reliability level guarantees that once Flume accepts
an event, that event will make it to the endpoint - as long as the
agent that accepted the event remains live long enough. The first
thing the agent does in this setting is write the event to disk in a
<em>'write-ahead log</em>' (WAL) so that, if the agent crashes and restarts,
knowledge of the event is not lost. After the event has successfully
made its way to the end of its flow, an acknowledgment is sent back to
the originating agent so that it knows it no longer needs to store the
event on disk. This reliability level can withstand any number of
failures downstream of the initial agent.</p></div>
<div class="paragraph"><p>The <strong>store on failure</strong> reliability level causes nodes to only require
an acknowledgement from the node one hop downstream.  If the sending
node detects a failure, it will store data on its local disk until the
downstream node is repaired, or an alternate downstream destination
can be selected.  While this is effective, data can be lost if a
compound or silent failure occurs.</p></div>
<div class="paragraph"><p>The <strong>best-effort</strong> reliability level sends data to the next hop with no
attempts to confirm or retry delivery.  If nodes fail, any data that
they were in the process of transmitting or receiving can be
lost. This is the weakest reliability level, but also the most
lightweight.</p></div>
<div class="sidebarblock">
<div class="sidebar-content">
<div class="sidebar-title">Eliminating Single Points of Failure</div>
<div class="paragraph"><p>Even when using the end-to-end reliability level, a Flume flow can
fail to make progress if there aren&#8217;t any nodes available to process
events. The data will continue to be collected by the agents, but the
agents may not be able to deliver the data downstream until a suitable
node comes online. In order to maintain high availability, Flume
allows flows to fail over from one downstream node to another without
any user intervention. The Flume Master can also be replicated for
high availability which means that both the data plane and the control
plane can tolerate a number of faults.</p></div>
</div></div>
<h3 id="_scalability">Scalability</h3><div style="clear:left"></div>
<div class="paragraph"><p>Scalability is the ability to increase system performance linearly -
or better - by adding more resources to the system.  Flume&#8217;s goal is
horizontal scalability&#8201;&#8212;&#8201;the ability to incrementally add more
machines to the system to increase throughput. A key performance
measure in Flume is the number or size of events entering the system
and being delivered.  When load increases, it is simple to add more
resources to the system in the form of more machines to handle the
increased load.</p></div>
<div class="paragraph"><p>As seen in the preceding example installation, there are three
separate components of Flume that require different approaches to
scalability: the collector tier, the master, and the storage tier.</p></div>
<div class="paragraph"><p>The collector tier needs to be able to scale in order to handle large
volumes of data coming from large numbers of agent nodes.  This
workload is write heavy, partitionable, and thus parallelizable.  By
adding more machines to the collector tier, you can increase the
number of agents and the maximum available throughput of the system.</p></div>
<div class="paragraph"><p>An individual collector can usually handle many agents (up to
hundreds) because each individual agent often produces only small
amounts of log data compared to the full bandwidth available to the
collector. Therefore, Flume balances flows from agents across
different collectors. (One flow from an agent will talk to the same
collector.) Flume uses a randomized algorithm to evenly assign lists
of collectors to flows.  This automatically spreads the load, and also
keeps the load spread in the case where a collector fails.</p></div>
<div class="paragraph"><p>As the number of nodes in the system increases, the volume of traffic
on the control path to and from the Flume Master may become a
bottleneck. The Flume Master also supports horizontal scaling by
adding more machines - although just a small number of commodity
servers can serve a large installation of nodes. The state of the
Flume Master is kept synchronized and fully replicated, which ensures
that it is both fault tolerant and highly scalable.</p></div>
<div class="paragraph"><p>Finally, Flume can only write data through a flow at the rate that the
final destinations can accept. Although Flume is able to buffer data
inside a flow to smooth out high-volume bursts, the output rate needs
to be equal on average to the input rate to avoid log jams. Thus,
writing to a scalable storage tier is advisable.  For example, HDFS
has been shown to scale to thousands of machines and can handle many
petabytes of data.</p></div>
<h3 id="_manageability">Manageability</h3><div style="clear:left"></div>
<div class="paragraph"><p>Manageability is the ability to control data flows, monitor nodes,
modify settings, and control outputs of a large system.  Manually
managing the data flow from the sources to the end point is tedious,
error prone, and a major pain point.  With the potential to have
thousands of log-generating applications and services, it&#8217;s important
to have a centralized management point to monitor and change data
flows, and the ability to dynamically handle different conditions or
problems.</p></div>
<div class="paragraph"><p>The Flume Master is the point where global state such as the data
flows can be managed. Via the Flume Master, users can monitor flows
and reconfigure them on the fly.  The Flume Master has the information
required to automatically respond to system changes such as load
imbalances, partial failures, or newly provisioned hardware.</p></div>
<div class="paragraph"><p>You can dynamically reconfigure nodes by using the Flume Master.
Although this guide describes examples of traditional three-tier
deployments, the flexibility of the nodes allow for arbitrary node
topologies.  You can reconfigure nodes by using small scripts written
in a flexible dataflow specification language, which can be submitted
via the Flume Master interface.</p></div>
<div class="paragraph"><p>You can administer the Flume Master by using either of two interfaces:
a web interface or the scriptable Flume command shell.  The web
interface provides interactive updates of the system’s state.  The
shell enables administration via manually crafted scripts or
machine-generated scripts.</p></div>
<h3 id="_extensibility">Extensibility</h3><div style="clear:left"></div>
<div class="paragraph"><p>Extensibility is the ability to add new functionality to a system. For
example, you can extend Flume by adding connectors to existing storage
layers or data platforms.  This is made possible by simple interfaces,
separation of functional concerns into simple composable pieces, a
flow specification language, and a simple but flexible data model.</p></div>
<div class="paragraph"><p>Flume provides many common input and output connectors.  When new
input connectors (sources) are added, extra metadata fields specific
to that source can be attached to each event it produces.  Flume
reuses the common components that provide particular reliability and
resource usage properties.  Some general sources include files from
the file system, syslog and syslog-ng emulation, or the standard
output of a process.  More specific sources such as IRC channels and
Twitter streams can also be added.  Similarly, there are many output
destinations for events.  Although HDFS is the primary output
destination, events can be sent to local files, or to monitoring and
alerting applications such as Ganglia or communication channels such
as IRC.</p></div>
<div class="paragraph"><p>To enable easy integration with HDFS, MapReduce, and Hive, Flume
provides simple mechanisms for output file management and output
format management.  Data gathered by Flume can be processed easily
with Hadoop and Hive.</p></div>
<h3 id="_section_summary">Section summary</h3><div style="clear:left"></div>
<div class="paragraph"><p>The preceding Introduction section describes the high level goals and
features of Flume. The following sections of this guide describe how
to set up and use Flume:</p></div>
<div class="ulist"><ul>
<li>
<p>
Step-by-step tutorial that introduces a single Flume node
</p>
</li>
<li>
<p>
Introduction to the Flume Master and a pseudo-distributed mode that
  includes multiple nodes coordinated by the Flume Master
</p>
</li>
<li>
<p>
Description of a fully-distributed setup that also removes single
  points of failure
</p>
</li>
<li>
<p>
Flume use cases and a description of how to integrate Flume with
  existing sources of data
</p>
</li>
<li>
<p>
How to set up Flume&#8217;s output so that integration with heavyweight
  analysis systems such as Hadoop and Hive
</p>
</li>
<li>
<p>
How to deploy Flume, set up arbitrary flows, and a specification of
  Flume&#8217;s data flow specification language
</p>
</li>
<li>
<p>
Catalog of components available via the language
</p>
</li>
<li>
<p>
A description of experimental features
</p>
</li>
<li>
<p>
Troubleshooting information
</p>
</li>
</ul></div>
</div>
<h2 id="Quickstart">Flume Single Node Quick Start</h2>
<div class="sectionbody">
<div class="paragraph"><p>In this section, you will learn how to get a single Flume node running and
transmitting data. You will also learn about some data <strong>sources</strong>, and how to
configure Flume flows on a per-node basis.</p></div>
<div class="paragraph"><p>Each logical node consists of a event-producing <strong>source</strong> and an event-
consuming <strong>sink</strong>.  Nodes pull data from their sources, and push data out
through their sink.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">This section assumes that the Flume node and Flume Master are running in
the foreground and not as daemons. You can stop the daemons by using <em>/etc/
init.d/flume-master stop</em> and <em>/etc/init.d/flume-node stop</em>.</td>
</tr></table>
</div>
<h3 id="_sources_and_the_tt_dump_tt_command">Sources and the <tt>dump</tt> command</h3><div style="clear:left"></div>
<div class="paragraph"><p>Start by getting a Flume node running that echoes data written to standard
input from the console back out to the console on stdout. You do this by using
the <tt>dump</tt> command.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ flume dump console</tt></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">The Flume program has the general form <tt>flume &lt;command&gt; [args ...]</tt>.  If
you installed from the tarball package, the command can be found in
<tt>$FLUME_HOME/bin/</tt>. If you installed from either RPM or DEB, then <tt>flume</tt>
should already be in your path.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">The example above uses the <tt>dump</tt> command and <tt>console</tt> is the argument.
The command’s syntax is <tt>flume dump &lt;source&gt;</tt>. It prints data from <tt>&lt;source&gt;</tt>
to the console.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Some flume configurations by default write to local disk.
Initially the default is <em>/tmp/flume</em>.  This is good for initial
testing but for production environments the <tt>flume.agent.logdir</tt>
property should be set to a more durable location.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">If the node refuses to run and exits with this message,
<tt>agent.FlumeNode: Aborting: Unexpected problem with
environment.Failure to write in log directory: <em>/tmp/flume</em>.  Check
permissions?</tt>, then check the <tt>/ tmp/flume</tt> directory to make sure you
have write permissions to it (change the owner or have the user join
the group).  This is, by default, where various logging information is
kept.</td>
</tr></table>
</div>
<div class="paragraph"><p>You have started a Flume node where <tt>console</tt> is the source of incoming data.
When you run it, you should see some logging messages displayed to the
console.  For now, you can ignore messages about Masters, back-off and failed
connections (these are explained in later sections).  When you type at the
console and press a new line, you should see a new log entry line appear
showing the data that you typed. If you entered <tt>This is a test</tt>, it should
look similar to this:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>hostname [INFO Thu Nov 19 08:37:13 PST 2009] This is a test</tt></pre>
</div></div>
<div class="paragraph"><p>To exit the program, press ^C.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Some sources do not automatically exit and require a manual ^C to
exit.</td>
</tr></table>
</div>
<h4 id="_reading_from_a_text_file_tt_text_tt">Reading from a text file, <tt>text</tt></h4>
<div class="paragraph"><p>You can also specify other sources of events.  For example, if you want a text
file where each line represents a new event, run the following command.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ flume dump 'text("/etc/services")'</tt></pre>
</div></div>
<div class="paragraph"><p>This command reads the file, and then outputs each line as a new event.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The default console output escapes special characters with Java-style
escape sequences. Characters such as <em>"</em> and <em>\</em> are prefaced with an extra
<em>\</em>.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">You can try this command with other files such as <tt>/var/log/messages</tt>,
<tt>/var/log/syslog</tt>, or <tt>/var/log/hadoop/hadoop.log</tt> also.  However, Flume must
run with appropriate permissions to read the files.</td>
</tr></table>
</div>
<h4 id="_tailing_a_file_name_tt_tail_tt_and_tt_multitail_tt">Tailing a file name, <tt>tail</tt> and <tt>multitail</tt></h4>
<div class="paragraph"><p>If you want to tail a file instead of just reading it, specify another source
by using <tt>tail</tt> instead of <tt>text</tt>.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ flume dump 'tail("testfile")'</tt></pre>
</div></div>
<div class="paragraph"><p>This command pipes data from the file into Flume and then out to the console.</p></div>
<div class="paragraph"><p>This message appears: "File <em>testfile</em> does not currently exist, waiting for
file to appear".</p></div>
<div class="paragraph"><p>In another terminal, you can create and write data to the file:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ echo Hello world! &gt;&gt; testfile</tt></pre>
</div></div>
<div class="paragraph"><p>New data should appear.</p></div>
<div class="paragraph"><p>When you delete the file:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ rm testfile</tt></pre>
</div></div>
<div class="paragraph"><p>The <tt>tail</tt> sink detects this.  If you then recreate the file, the <tt>tail</tt>
source detects the new file and follows it:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ echo Hello world again! &gt;&gt; testfile</tt></pre>
</div></div>
<div class="paragraph"><p>You should see your new message appear in the Flume node console.</p></div>
<div class="paragraph"><p>You can also use the <tt>multitail</tt> source to follow multiple files by file name:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ flume dump 'multitail("test1", "test2")'</tt></pre>
</div></div>
<div class="paragraph"><p>And send it data coming from the two different files:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ echo Hello world test1! &gt;&gt; test1
$ echo Hello world test2! &gt;&gt; test2</tt></pre>
</div></div>
<h4 id="_synthetic_sources_tt_synth_tt">Synthetic sources, <tt>synth</tt></h4>
<div class="paragraph"><p>Here&#8217;s one more example where you use the <tt>synth</tt> sources to generate events:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ flume dump 'asciisynth(20,30)'</tt></pre>
</div></div>
<div class="paragraph"><p>You should get 20 events, each with 30 random ASCII bytes.</p></div>
<h4 id="_syslog_as_a_source_tt_syslogudp_tt_and_tt_syslogtcp_tt">Syslog as a source, <tt>syslogUdp</tt> and <tt>syslogTcp</tt></h4>
<div class="paragraph"><p>As with files, you can also accept data from well known wire formats such as
syslog. For example, you can start a traditional syslog-like UDP server
listening on port 5140 (the normal syslog UDP port is the privileged port 514)
by running this command:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ flume dump 'syslogUdp(5140)'</tt></pre>
</div></div>
<div class="paragraph"><p>You can feed the source data by using netcat to send syslog formatted data as
shown in the example below:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ echo "<b>&lt;37&gt;</b>hello via syslog"  | nc -u localhost 5140</tt></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">You may need to press ^C to exit this command.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The extra <tt>&lt;37&gt;</tt> is a syslog wireformat encoding of a message category
and priority level.</td>
</tr></table>
</div>
<div class="paragraph"><p>Similarly, you can set up a syslog-ng compatible source that listens on TCP
port 5140 (the normal syslog-ng TCP port is the privileged port 514):</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ flume dump 'syslogTcp(5140)'</tt></pre>
</div></div>
<div class="paragraph"><p>And send it data:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ echo "<b>&lt;37&gt;</b>hello via syslog" | nc -t localhost 5140</tt></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">You may need to press ^C to exit this command.</td>
</tr></table>
</div>
<div class="paragraph"><p>Syslog backwards-compatibility allows data normally created from syslog,
rsyslog, or syslog-ng to be sent to and processed by Flume.</p></div>
<h3 id="_anatomy_of_an_event">Anatomy of an Event</h3><div style="clear:left"></div>
<div class="paragraph"><p>This section describes a number of sources of data that Flume can interoperate
with. Before going any further, it will be helpful for you to understand what
Flume is actually sending and processing internally.</p></div>
<div class="paragraph"><p>Flume internally converts every external source of data into a stream of
<strong>events</strong>. Events are Flume&#8217;s unit of data and are a simple and flexible
representation. An event is composed of a <strong>body</strong> and <strong>metadata</strong>. The event
body is a string of bytes representing the content of an event. For example, a
line in a log file is represented as an event whose body was the actual byte
representation of that line. The event metadata is a table of key / value
pairs that capture some detail about the event, such as the time it was
created or the name of the machine on which it originated. This table can be
appended as an event travels along a Flume flow, and the table can be read to
control the operation of individual components of that flow. For example, the
machine name attached to an event can be used to control the output path where
the event is written at the end of the flow.</p></div>
<div class="paragraph"><p>An event&#8217;s body can be up to 32KB long - although this limit can be controlled
via a system property, it is recommended that it is not changed in order to
preserve performance.</p></div>
<h3 id="_section_summary_2">Section Summary</h3><div style="clear:left"></div>
<div class="paragraph"><p>In this section, you learned how to use Flume&#8217;s <tt>dump</tt> command to print data
from a variety of different input sources to the console. You also learned
about the <strong>event</strong>, the fundamental unit of data transfer in Flume.</p></div>
<div class="paragraph"><p>The following table summarizes the sources described in this section.</p></div>
<div class="dlist"><div class="title">Flume Event</div><dl>
<dt class="hdlist1">
Sources <tt>console</tt> 
</dt>
<dd>
<p>
Stdin console
</p>
</dd>
<dt class="hdlist1">
<tt>text("filename")</tt> 
</dt>
<dd>
<p>
One shot text file source.  One line is one event
</p>
</dd>
<dt class="hdlist1">
<tt>tail("filename")</tt> 
</dt>
<dd>
<p>
Similar to Unix&#8217;s <tt>tail -F</tt>. One line is one event.
Stays open for more data and follows filename if file rotated.
</p>
</dd>
<dt class="hdlist1">
<tt>multitail("file1"[, "file2"[, &#8230;]])</tt> 
</dt>
<dd>
<p>
Similar to <tt>tail</tt> source but follows
multiple files.
</p>
</dd>
<dt class="hdlist1">
<tt>asciisynth(msg_count,msg_size)</tt> 
</dt>
<dd>
<p>
A source that synthetically generates
msg_count random messages of size msg_size.  This converts all characters into
printable ASCII characters.
</p>
</dd>
<dt class="hdlist1">
<tt>syslogUdp(port)</tt> 
</dt>
<dd>
<p>
Syslog over UDP port, port.  This is syslog compatible.
</p>
</dd>
<dt class="hdlist1">
<tt>syslogTcp(port)</tt> 
</dt>
<dd>
<p>
Syslog over TCP port, port. This is syslog-ng compatible.
</p>
</dd>
</dl></div>
</div>
<h2 id="_pseudo_distributed_mode">Pseudo-distributed Mode</h2>
<div class="sectionbody">
<div class="paragraph"><p>Flume is intended to be run as a distributed system with processes spread out
across <em>many</em> machines.  It can also be run as several processes on a <em>single</em>
machine, which is called &#8220;pseudo-distributed&#8221; mode.  This mode is useful for
debugging Flume data flows and getting a better idea of how Flume components
interact.</p></div>
<div class="paragraph"><p>The previous section described a Flume node and introduced the concept of
Flume sources.  This section introduces some new concepts required for a
distributed setup: the Flume <strong>master</strong> server, the specification of sources and
<strong>sinks</strong>, and connecting multiple Flume <strong>nodes</strong>.</p></div>
<h3 id="_starting_pseudo_distributed_flume_daemons">Starting Pseudo-distributed Flume Daemons</h3><div style="clear:left"></div>
<div class="paragraph"><p>There are two kinds of processes in the system: the Flume <strong>master</strong> and the
Flume <strong>node</strong>.  The Flume Master is the central management point and controls
the data flows of the nodes.  It is the single logical entity that holds
global state data and controls the Flume node data flows and monitors Flume
nodes.  Flume nodes serve as the data path for streams of events.  They can be
the sources, conduits, and consumers of event data.  The nodes periodically
contact the Master to transmit a heartbeat and to get their data flow
configuration.</p></div>
<div class="paragraph"><p>In order to get a distributed Flume system working, you must start a single
Flume Master and some Flume nodes that interact with the Master. You&#8217;ll start
with a Master and one Flume node and then expand.</p></div>
<h4 id="_the_master">The Master</h4>
<div class="paragraph"><p>The Master can be manually started by executing the following command:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ flume master</tt></pre>
</div></div>
<div class="paragraph"><p>After the Master is started, you can access it by pointing a web browser to
<a href="http://localhost:35871/">http://localhost:35871/</a>.  This web page displays the status of all Flume nodes
that have contacted the Master, and shows each node&#8217;s currently assigned
configuration.  When you start this up without Flume nodes running, the status
and configuration tables will be empty.</p></div>
<div class="imageblock">
<div class="content">
<img src="master-empty.png" alt="master-empty.png" />
</div>
</div>
<div class="paragraph"><p>The web page contains four tables&#8201;&#8212;&#8201;the <em>Node status</em> table, the <em>Node
configuration</em> table, the <em>Physical/Logical Node mapping</em> table, and a
<em>Command history</em> table.  The information in these tables represent the
current global state of the Flume system.</p></div>
<div class="paragraph"><p>The Master&#8217;s <em>Node status</em> table contains the names of all of the Flume Nodes
talking to the Master, their current configuration version (initially "none"),
their status (such as IDLE), and when it last reported to the Master.  The
name of each Flume node should be the same as running <tt>hostname</tt> from Unix
prompt.</p></div>
<div class="paragraph"><p>The Master&#8217;s <em>Node configuration</em> table contains the logical name of a node,
the configuration version assigned to it, and a specification of its sources
and its sinks.  Initially, this table is empty, but after you change values
you can view this web page to see the updates. There are two sets of columns -
- the user entered version/source/sink, and translated version/source/sink. A
later section of this guide describes translated configs.</p></div>
<div class="paragraph"><p>The Master&#8217;s <em>Physical/Logical Node mapping</em> table contains the mapping of
logical nodes to their physical nodes.</p></div>
<div class="paragraph"><p>The Master&#8217;s <em>Command history</em> table contains the state of commands. In
general, <strong>commands</strong> modify the Master&#8217;s global state.  Commands are processed
serially on a Master and are assigned a unique command ID number. Each command
has a state (for example, SUCCEEDED, FAILED, or PENDING), a command line, and
a message which often contains information about its execution attempt.</p></div>
<h4 id="_the_flume_node">The Flume Node</h4>
<div class="paragraph"><p>To start a Flume node, invoke the following command in another terminal.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ flume node_nowatch</tt></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Normally, you start a node using <tt>flume node</tt> or run the node as a
daemon.  For these examples, you disable the watchdog feature by starting with
the <tt>node_nowatch</tt> option.  This option enables you to interact with a node
via the console. The other options disable <tt>stdin</tt>.</td>
</tr></table>
</div>
<div class="paragraph"><p>To check whether a Flume node is up, point your browser to the <em>Flume Node
status page</em> at <a href="http://localhost:35862/">http://localhost:35862/</a>.  Each node displays its own data on a
single table that includes diagnostics and metrics data about the node, its
data flows, and the system metrics about the machine it is running on.</p></div>
<div class="paragraph"><p>If the node is up, you should also refresh the Master&#8217;s status page (http://
localhost:35871) to make sure that the node has contacted the Master. You
brought up one node (assume the node is named <tt><em>host</em></tt>), so you should have
one node listed in the Master&#8217;s node status table, and an entry in the logical
node mapping table that links the <tt><em>host</em></tt> logical node to the <tt><em>host</em></tt>
physical nodes.</p></div>
<h3 id="_configuring_a_node_via_the_master">Configuring a Node via the Master</h3><div style="clear:left"></div>
<div class="paragraph"><p>Requiring nodes to contact the Master to get their configuration enables you
to dynamically change the configuration of nodes without having to log in to
the remote machine to restart the daemon.  You can quickly change the node&#8217;s
previous data flow configuration to a new one.</p></div>
<div class="paragraph"><p>The following describes how to "wire" nodes using the Master&#8217;s web interface.</p></div>
<div class="paragraph"><p>On the Master&#8217;s web page, click on the config link. You are presented with two
forms.  These are web interfaces for setting the node&#8217;s data flows.  When
Flume nodes contact the Master, they will notice that the data flow version
has changed, instantiate, and activate the configuration.</p></div>
<div class="paragraph"><p>For this example, you will do the steps you did in the Quick Start section.
Enter the following values into the "Configure a node" form, and then click
Submit.</p></div>
<div class="tableblock">
<table rules="all"
frame="hsides"
cellspacing="0" cellpadding="4">
<col width="125" />
<col width="194" />
<tbody valign="top">
  <tr>
    <td align="left">
    Node name:
    </td>
    <td align="left">
    <tt><em>host</em></tt>
    </td>
  </tr>
  <tr>
    <td align="left">
    Source:
    </td>
    <td align="left">
    <tt>console</tt>
    </td>
  </tr>
  <tr>
    <td align="left">
    Sink:
    </td>
    <td align="left">
    <tt>console</tt>
    </td>
  </tr>
</tbody>
</table>
</div>
<div class="paragraph"><p>Refresh the Master page and notice that the version stamp changed to a current
time, and that the src and sink fields of the configs updated.  After the
status changes to "ACTIVE", it is ready to receive console traffic.</p></div>
<div class="paragraph"><p>On the master, a node can be in one of several states:</p></div>
<div class="ulist"><ul>
<li>
<p>
HELLO : A new node instance initially contacted the master.
</p>
</li>
<li>
<p>
IDLE : A node has completed its configuration or has no configuration.
</p>
</li>
<li>
<p>
CONFIGURING: A node has received a configuration and is activating the configuration.
</p>
</li>
<li>
<p>
ACTIVE: A node is actively pulling data from the source and pushing data into the sink.
</p>
</li>
<li>
<p>
LOST: A node has not contacted the master for an extended period of time (default is after 10x the expected heartbeat period&#8201;&#8212;&#8201;50s by default)
</p>
</li>
<li>
<p>
DECOMMISSIONED: A node has been purposely decommissioned from a master.
</p>
</li>
<li>
<p>
ERROR: A node has stopped in an error state.
</p>
</li>
</ul></div>
<div class="paragraph"><p>On the terminal where your Flume node is running, you should be able to type a
few lines and then get output back showing your new log message.</p></div>
<div class="tableblock">
<table rules="all"
frame="hsides"
cellspacing="0" cellpadding="4">
<col width="125" />
<col width="194" />
<tbody valign="top">
  <tr>
    <td align="left">
    Node name:
    </td>
    <td align="left">
    <tt><em>host</em></tt>
    </td>
  </tr>
  <tr>
    <td align="left">
    Source:
    </td>
    <td align="left">
    <tt>text("/etc/services")</tt>
    </td>
  </tr>
  <tr>
    <td align="left">
    Sink:
    </td>
    <td align="left">
    <tt>console</tt>
    </td>
  </tr>
</tbody>
</table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">You may need to press Enter in the Flume node console.</td>
</tr></table>
</div>
<div class="paragraph"><p>Or use these new values to tail a file:</p></div>
<div class="tableblock">
<table rules="all"
frame="hsides"
cellspacing="0" cellpadding="4">
<col width="125" />
<col width="194" />
<tbody valign="top">
  <tr>
    <td align="left">
    Node name:
    </td>
    <td align="left">
    <tt><em>host</em></tt>
    </td>
  </tr>
  <tr>
    <td align="left">
    Source:
    </td>
    <td align="left">
    <tt>tail("/etc/services")</tt>
    </td>
  </tr>
  <tr>
    <td align="left">
    Sink:
    </td>
    <td align="left">
    <tt>console</tt>
    </td>
  </tr>
</tbody>
</table>
</div>
<div class="paragraph"><p>You can now change the configuration of different nodes in the system to
gather data from a variety of sources by going through the Master.</p></div>
<h3 id="_introducing_sinks">Introducing Sinks</h3><div style="clear:left"></div>
<div class="paragraph"><p>Thus far, you have seen that Flume has a variety of sources that generate or
accept new events that are fed into the system.  You have limited the output
of these messages to the <tt>console</tt> sink.  As you would expect, Flume also
provides a wide variety of event <strong>sinks</strong>&#8201;&#8212;&#8201;destinations for all of the
events.</p></div>
<div class="paragraph"><p>There are many possible destinations for events&#8201;&#8212;&#8201;to local disk, to HDFS, to
the console, or forwarding across the network. You use the sink abstractions
as an interface for forwarding events to any of these destinations.</p></div>
<div class="paragraph"><p>You can connect different sources to different sinks by specifying the new
configuration and submitting it to the Master.  For example, with the data
flow below, you can make a copy of <tt>/etc/services</tt>.</p></div>
<div class="tableblock">
<table rules="all"
frame="hsides"
cellspacing="0" cellpadding="4">
<col width="125" />
<col width="194" />
<tbody valign="top">
  <tr>
    <td align="left">
    Node name:
    </td>
    <td align="left">
    <tt><em>host</em></tt>
    </td>
  </tr>
  <tr>
    <td align="left">
    Source:
    </td>
    <td align="left">
    <tt>text("/etc/services")</tt>
    </td>
  </tr>
  <tr>
    <td align="left">
    Sink:
    </td>
    <td align="left">
    <tt>text("services.copy")</tt>
    </td>
  </tr>
</tbody>
</table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">The <tt>text</tt> sinks overwrite if a file previously exists.</td>
</tr></table>
</div>
<div class="paragraph"><p>Notice that the file is copied as is. Sinks have optional arguments for output
format, which you can use to write data in other serialization formats.  For
example, instead of copying a file using the default "raw" formatter, you can
format the output using other formatters such as "avrojson" (the Avro
serialization json format), "avrodata" (the Avro serialization binary data
format), or a "debug" mode (this is the formatter used by the console sink).</p></div>
<div class="paragraph"><p>If you enter:</p></div>
<div class="tableblock">
<table rules="all"
frame="hsides"
cellspacing="0" cellpadding="4">
<col width="125" />
<col width="194" />
<tbody valign="top">
  <tr>
    <td align="left">
    Node name:
    </td>
    <td align="left">
    <tt><em>host</em></tt>
    </td>
  </tr>
  <tr>
    <td align="left">
    Source:
    </td>
    <td align="left">
    <tt>text("/etc/services")</tt>
    </td>
  </tr>
  <tr>
    <td align="left">
    Sink:
    </td>
    <td align="left">
    <tt>console("avrojson")</tt>
    </td>
  </tr>
</tbody>
</table>
</div>
<div class="paragraph"><p>You get the file with each record in JSON format displayed to the console.</p></div>
<div class="paragraph"><p>If you enter:</p></div>
<div class="tableblock">
<table rules="all"
frame="hsides"
cellspacing="0" cellpadding="4">
<col width="125" />
<col width="194" />
<tbody valign="top">
  <tr>
    <td align="left">
    Node name:
    </td>
    <td align="left">
    <tt><em>host</em></tt>
    </td>
  </tr>
  <tr>
    <td align="left">
    Source:
    </td>
    <td align="left">
    <tt>text("/etc/services")</tt>
    </td>
  </tr>
  <tr>
    <td align="left">
    Sink:
    </td>
    <td align="left">
    <tt>text("services.json", "avrojson")</tt>
    </td>
  </tr>
</tbody>
</table>
</div>
<div class="paragraph"><p>The newly written local <tt>services.json</tt> file is output in avro&#8217;s json format.</p></div>
<div class="sidebarblock">
<div class="sidebar-content">
<div class="sidebar-title">Default Arguments</div>
<div class="paragraph"><p>Many sinks, sources, and decorators have arguments that modify their behavior.
Some of these arguments are required; others are optional and use a default.</p></div>
<div class="paragraph"><p>The following notation describes these parameters:</p></div>
<div class="paragraph"><p><tt><em>name</em> (<em>req1</em>, <em>req2</em>[, <em>opt1</em>[, <em>opt2</em>=<em>foo</em>]] )</tt></p></div>
<div class="paragraph"><p>where a component with name <em>name</em>, has two required arguments, <em>req1</em> and
<em>req2</em>. There are two optional arguments, <em>opt1</em> and <em>opt2</em>.  However, <em>opt1</em>
<strong>must</strong> be specified if <em>opt2</em> is specified.  <em>foo</em> in this case is the default
argument for <em>opt2</em>+.</p></div>
<div class="paragraph"><p>For the <tt>console</tt> examples above, console could be specified as:</p></div>
<div class="paragraph"><p><tt>console[(<em>format</em>="debug")]</tt></p></div>
<div class="paragraph"><p>and text could be specified as:</p></div>
<div class="paragraph"><p><tt>text("<em>file</em>"[, <em>format</em>="raw"])</tt></p></div>
</div></div>
<div class="paragraph"><p>There are several sinks you can use. The following list is a subset; see the
Appendix for more sinks.</p></div>
<div class="hdlist"><div class="title">Flume Event Sinks</div><table>
<tr>
<td class="hdlist1">
<tt>null</tt>  
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Null sink. Events are dropped.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>console[("format")]</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Console sink.  Display to console&#8217;s stdout. The
"format" argument is optional and defaults to the "debug" output format.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>text("<em>txtfile</em>"[,"format"])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Textfile sink.  Write the events to text
file <tt><em>txtfile</em></tt> using output format "format".  The default format is "raw"
event bodies with no metadata.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>dfs("<em>dfsfile</em>")</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
DFS seqfile sink.  Write serialized Flume events
to a dfs path such as <tt>hdfs://namenode/file</tt> or <tt>file:///file</tt> in
Hadoop&#8217;s seqfile format.  Note that because of the HDFS write
semantics, no data for this sink write until the sink is closed.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>syslogTcp("<em>host</em>",<em>port</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Syslog TCP sink.  Forward to events to <tt>host</tt>
on TCP port <tt>port</tt> in syslog wire format (syslog-ng compatible), or to other
Flume nodes setup to listen for syslogTcp.
</p>
</td>
</tr>
</table></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">Using <tt>dfs</tt> has some restrictions and requires some extra
setup.  Files contents will not become available until after the sink
has been closed.  See the Troubleshooting section for details.</td>
</tr></table>
</div>
<h3 id="_aggregated_configurations">Aggregated Configurations</h3><div style="clear:left"></div>
<div class="paragraph"><p>Using the form for single node configuration of a small number of
machines is manageable, but for larger numbers it is more efficient to
maintain or auto-generate the configuration for all of the machines in
a single file. Flume allows you to set the configurations of many
machines in a single aggregated configuration submission.</p></div>
<div class="paragraph"><p>Instead of the method you used in the "Configuring a Node via the Master"
section, put the following configuration line into the "Configure Nodes" form
and then submit:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>host : text("/etc/services") | console ;</tt></pre>
</div></div>
<div class="paragraph"><p>Or:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>host: text("/etc/services") | text("services.copy");</tt></pre>
</div></div>
<div class="paragraph"><p>The general format is:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>&lt;node1&gt; : &lt;source&gt; | &lt;sink&gt; ;
&lt;node2&gt; : &lt;source&gt; | &lt;sink&gt; ;
&lt;node3&gt; : &lt;source&gt; | &lt;sink&gt; ;
...</tt></pre>
</div></div>
<div class="paragraph"><p>The remainder of this guide uses this format to configure nodes.</p></div>
<h3 id="_tiering_flume_nodes_agents_and_collectors">Tiering Flume Nodes: Agents and Collectors</h3><div style="clear:left"></div>
<div class="paragraph"><p>A simple network connection is abstractly just another sink.  It would be
great if sending events over the network was easy, efficient, and reliable.
In reality, collecting data from a distributed set of machines and relying on
networking connectivity greatly increases the likelihood and kinds of failures
that can occur.  The bottom line is that providing reliability guarantees
introduces complexity and many tradeoffs.</p></div>
<div class="paragraph"><p>Flume simplifies these problems by providing a predefined topology and tunable
reliability that only requires you to give each Flume node a role.  One simple
Flume node topology classifies Flume nodes into two roles&#8201;&#8212;&#8201;a Flume <strong>agent</strong>
tier and the Flume <strong>collector</strong> tier. The agent tier Flume nodes are co-located
on machines with the service that is producing logs.  For example, you could
specify a Flume agent configured to have <tt>syslogTcp</tt> as a source, and
configure the syslog generating server to send its logs to the specified local
port.  This Flume agent would have an <tt>agentSink</tt> as its sink which is
configured to forward data to a node in the collector tier.</p></div>
<div class="paragraph"><p>Nodes in the collector tier listen for data from multiple agents, aggregates
logs, and then eventually write the data to HDFS.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">In the next few sections, all <tt><em>host</em></tt> arguments currently use the
physical IP or DNS name of the node as opposed to the Flume node name (set by
<tt>-n name</tt> option). The default Flume node name is the host name unless you
override it on the command line.</td>
</tr></table>
</div>
<div class="paragraph"><p>To demonstrate the new sinks in pseudo-distributed mode, you will instantiate
another Flume node (a physical node) on the local box.  To do this, you need
to start a Flume node with some extra options.  The command line below starts
a physical node named <tt>collector</tt> (<tt>-n collector</tt>):</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ flume node_nowatch -n collector</tt></pre>
</div></div>
<div class="paragraph"><p>On the Master&#8217;s web page, you should eventually see two nodes: <tt><em>host</em></tt> and
<tt>collector</tt>.  The Flume Node status web pages should be available at http://
localhost:35862 and <a href="http://localhost:35863">http://localhost:35863</a>.  Port bindings are dependent on
instantiation order&#8201;&#8212;&#8201;the first physical node instantiated binds on 35862 and
the second binds to 35863.</p></div>
<div class="paragraph"><p>Next, configure <tt>collector</tt> to take on the role of a collector, set up
<tt><em>host</em></tt> to send data from the console to the collector by using the
aggregated multiple configuration form.  The agent uses the <tt>agentSink</tt>, a
high reliability network sink.  The collector node&#8217;s source is configured to
be a <tt>collectorSource</tt>, and its sink is configured to be the <tt>console</tt>.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>host : console | agentSink("localhost",35853) ;
collector : collectorSource(35853) | console ;</tt></pre>
</div></div>
<div class="imageblock">
<div class="content">
<img src="tiers-console.png" alt="tiers-console.png" />
</div>
</div>
<div class="paragraph"><p>When you type lines in <tt><em>host</em></tt>'s console, events are forwarded to the
collector.  Currently, there is a bit of latency (15s or so) before the
forwarded message shows up on <tt>collector</tt>.  This is actually a configurable
setting whose default is set to a value that is amenable for high event
throughputs.  Later sections describe how to tune Flume.</p></div>
<div class="paragraph"><p>You have successfully made a event flow from an agent to the collector.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">You can check to see if <tt>agentSink</tt> is working by looking in the
write- ahead logging directory for the sink.  The default location is
<tt>/tmp/flume/ &lt;nodename&gt;</tt>.  Because the OS periodically cleans this
directory, this configuration property (<tt>flume.agent.logdir</tt>) should
be changed a production environment.</td>
</tr></table>
</div>
<div class="paragraph"><p>A more interesting setup is to have the agent tailing a local file (using the
<tt>tail</tt> source) or listening for local syslog data (using the <tt>syslogTcp</tt> or
<tt>syslogUdp</tt> sources and modifying the syslog daemon&#8217;s configuration).  Instead
of writing to a console, the collector would write to a <tt>collectorSink</tt>, a
smarter sink that writes to disk or HDFS, periodically rotates files, and
manages acknowledgements.</p></div>
<div class="imageblock">
<div class="content">
<img src="tiers-hdfs.png" alt="tiers-hdfs.png" />
</div>
</div>
<div class="paragraph"><p>The following configuration is for an agent that listens for syslog messages
and forwards to a collector which writes files to local directory <tt>/tmp/flume/
collected/</tt>.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>host : syslogTcp(5140) | agentSink("localhost",35853) ;
collector : collectorSource(35853) | collectorSink("file:///tmp/flume/collected", "syslog");</tt></pre>
</div></div>
<div class="paragraph"><p>In the following slightly modified configuration, the collector writes to an
HDFS cluster (assuming the HDFS nameNode is called <tt>namenode</tt>):</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>host : syslogTcp(5140) | agentSink("localhost",35853) ;
collector : collectorSource(35853) | collectorSink("hdfs://namenode/user/flume/
","syslog");</tt></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">There are no guarantees that data written to an HDFS file is
durable until the HDFS file is properly closed.  Because of this, the
collector sink periodically closes a file and creates a new one in
HDFS.  The default time between file rolls (close then open new) is
30s.  If you are writing data at low throughput (&lt;2MB/s) you may want
to increase the default time by modifying the
<tt>flume.collector.roll.millis</tt> and <tt>flume.agent.logdir.retransmit</tt> time
properties in your flume-site.xml file.</td>
</tr></table>
</div>
<h3 id="_section_summary_3">Section Summary</h3><div style="clear:left"></div>
<div class="paragraph"><p>This section describes how to start a Master and a node, and configure a node
via the Master. The next section describes a more concise way of specifying
many configurations, explains agents and collectors, and how to build an
agent-collector pipeline on a single machine in a three-tier topology.</p></div>
</div>
<h2 id="_fully_distributed_mode">Fully-distributed Mode</h2>
<div class="sectionbody">
<div class="paragraph"><p>The main goal for Flume is to collect logs and data from many
different hosts and to scale and intelligently handle different
cluster and network topologies.</p></div>
<div class="paragraph"><p>To deploy Flume on your cluster, do the following steps.</p></div>
<div class="ulist"><div class="title">Steps to Deploy Flume On a Cluster</div><ul>
<li>
<p>
Install Flume on each machine.
</p>
</li>
<li>
<p>
Select one or more nodes to be the Master.
</p>
</li>
<li>
<p>
Modify a static configuration file to use site specific properties.
</p>
</li>
<li>
<p>
Start the Flume Master node on at least <strong>one</strong> machine.
</p>
</li>
<li>
<p>
Start a Flume node on <strong>each</strong> machine.
</p>
</li>
</ul></div>
<div class="paragraph"><p>The following section describes how to manually configure the
properties file to specify the Master for each node, and how to set
default values for parameters.  Sections afterwards describe a data
flow configuration for a larger system, how to add more capacity by
adding collectors, and how to improve the reliability of the Master by
adding multiple Masters.</p></div>
<h3 id="_static_configuration_files">Static Configuration Files</h3><div style="clear:left"></div>
<div class="paragraph"><p>In the previous sections, you used Flume on a single machine with the
default configuration settings. With the default settings, nodes
automatically search for a Master on <tt>localhost</tt> on a standard
port. In order for the Flume nodes to find the Master in a fully
distributed setup, you must specify site-specific static configuration
settings.</p></div>
<div class="paragraph"><p>Site-specific settings for Flume nodes and Masters are configured by
properties in the <tt>conf/flume-site.xml</tt> file found on each machine.
If this file is not present, the commands default to the settings
found in <tt>conf/flume-conf.xml</tt>.  In the following example, you set up
the property that points a Flume node to search for its Master at a
machine called <tt>master</tt>.</p></div>
<div class="listingblock">
<div class="title"><tt>conf/flume-site.xml</tt></div>
<div class="content"><!-- Generator: GNU source-highlight 3.1.3
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">&lt;?xml</span></span> <span style="color: #009900">version</span><span style="color: #990000">=</span><span style="color: #FF0000">"1.0"</span><span style="font-weight: bold"><span style="color: #000080">?&gt;</span></span>
<span style="font-weight: bold"><span style="color: #000080">&lt;?xml</span></span>-<span style="color: #009900">stylesheet</span> <span style="color: #009900">type</span><span style="color: #990000">=</span><span style="color: #FF0000">"text/xsl"</span>  <span style="color: #009900">href</span><span style="color: #990000">=</span><span style="color: #FF0000">"configuration.xsl"</span><span style="font-weight: bold"><span style="color: #000080">?&gt;</span></span>

<span style="font-weight: bold"><span style="color: #0000FF">&lt;configuration&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;property&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;name&gt;</span></span>flume.master.servers<span style="font-weight: bold"><span style="color: #0000FF">&lt;/name&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;value&gt;</span></span>master<span style="font-weight: bold"><span style="color: #0000FF">&lt;/value&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/property&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/configuration&gt;</span></span></tt></pre></div></div>
<h4 id="_using_default_values">Using Default Values</h4>
<div class="paragraph"><p>When you are using agent/collector roles, you can add the following
configuration properties to your <tt>flume-site.xml</tt> file to set up the
default hosts used as collector.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.3
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>...
<span style="font-weight: bold"><span style="color: #0000FF">&lt;property&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;name&gt;</span></span>flume.collector.event.host<span style="font-weight: bold"><span style="color: #0000FF">&lt;/name&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;value&gt;</span></span>collector<span style="font-weight: bold"><span style="color: #0000FF">&lt;/value&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;description&gt;</span></span>This is the host name of the default "remote"     collector.
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/description&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/property&gt;</span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">&lt;property&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;name&gt;</span></span>flume.collector.port<span style="font-weight: bold"><span style="color: #0000FF">&lt;/name&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;value&gt;</span></span>35853<span style="font-weight: bold"><span style="color: #0000FF">&lt;/value&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;description&gt;</span></span>This default tcp port that the collector listens to     in order to receive events it is collecting.
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/description&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/property&gt;</span></span>

...</tt></pre></div></div>
<div class="paragraph"><p>This will make the <tt>agentSink</tt> with no arguments default to using
<tt>flume.collector.event.host</tt> and <tt>flume.collector.port</tt> for their
default target and port.</p></div>
<div class="paragraph"><p>In the following example, a larger setup with several agents push data
to a collector. There are seven Flume nodes&#8201;&#8212;&#8201;six in the agent tier,
and one in the collector tier.</p></div>
<div class="imageblock">
<div class="content">
<img src="singleCollector.png" alt="singleCollector.png" />
</div>
</div>
<div class="paragraph"><p>An explicit configuration fills in all of the parameters:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>agentA : src | agentSink("collector",35853);
agentB : src | agentSink("collector",35853);
agentC : src | agentSink("collector",35853);
agentD : src | agentSink("collector",35853);
agentE : src | agentSink("collector",35853);
agentF : src | agentSink("collector",35853);
collector : collectorSource(35853) | collectorSink("hdfs://namenode/flume/","srcdata");</tt></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">When specifying destinations for agentSinks, use the <em>hostname
and port of the target machine</em>.  The default name for a node is its
hostname.  However, if there are multiple logical nodes, you must use
the machine&#8217;s host name, <em>not the name of the logical node</em>. In the
preceding examples, agent[A-F] and collector are the <em>physical host
names</em> of the machines where these configurations are running.</td>
</tr></table>
</div>
<div class="paragraph"><p>You can rely on the default ports set in the configuration files:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>agentA : src | agentSink("collector");
agentB : src | agentSink("collector");
agentC : src | agentSink("collector");
agentD : src | agentSink("collector");
agentE : src | agentSink("collector");
agentF : src | agentSink("collector");
collector : collectorSource | collectorSink("hdfs://namenode/flume/","srcdata");</tt></pre>
</div></div>
<div class="paragraph"><p>You can rely on the default ports and default collector host:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>agentA : src | agentSink
agentB : src | agentSink
agentC : src | agentSink
agentD : src | agentSink
agentE : src | agentSink
agentF : src | agentSink
collector : collectorSource | collectorSink("hdfs://namenode/flume/","srcdata");</tt></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">Using defaults can make writing data flow configurations more concise, but may obscure the details about how different nodes are connected to each other.</td>
</tr></table>
</div>
<div class="sidebarblock">
<div class="sidebar-content">
<div class="sidebar-title">Reliability Modes</div>
<div class="paragraph"><p>You can tune the reliability level of the agents, simply by specifying a different kind of
agent sink.  There are three levels available, and three corresponding agents:</p></div>
<div class="dlist"><dl>
<dt class="hdlist1">
<tt>agentE2ESink[("<em>machine</em>"[,<em>port</em>])]</tt> 
</dt>
<dd>
<p>
End to end.  This version uses the WAL, relies on an acknowledgement, and will retry if no acknowledgement is received.
</p>
</dd>
<dt class="hdlist1">
<tt>agentDFOSink[("<em>machine</em>"[,<em>port</em>])]</tt> 
</dt>
<dd>
<p>
Disk Failover (store on failure).  This agent writes to disk only if it can detect a failure on the collector.  If any data is stored to disk, it periodically retires the network connection and attempts to resend.
</p>
</dd>
<dt class="hdlist1">
<tt>agentBESink[("<em>machine</em>"[,<em>port</em>])]</tt> 
</dt>
<dd>
<p>
Best Effort.  This agent does not write to disk at all, and drops messages in the event of collector failures.
</p>
</dd>
</dl></div>
<div class="paragraph"><p>The previous examples use the <tt>agentSink</tt>.  This is an alias for the <tt>agentE2ESink</tt>.</p></div>
</div></div>
<h3 id="_multiple_collectors">Multiple Collectors</h3><div style="clear:left"></div>
<div class="paragraph"><p>Having multiple collectors can increase the log collection throughput
and can improve the timeliness of event delivery by increasing
collector availability.  Data collection is parallelizable; thus, load
from many agents can be shared across many several collectors.</p></div>
<h4 id="_partitioning_agents_across_multiple_collectors">Partitioning Agents across Multiple Collectors</h4>
<div class="paragraph"><p>The preceding graph and dataflow spec shows a typical topology for
Flume nodes.  For reliable delivery, in the event that the collector
stops operating or disconnects from the agents, the agents would need
to store their events to their respective disks locally.  The agents
would then periodically attempt to recontact a collector.  Because the
collector is down, any analysis or processing downstream is blocked.</p></div>
<div class="imageblock">
<div class="content">
<img src="multiCollector.png" alt="multiCollector.png" />
</div>
</div>
<div class="paragraph"><p>When you have multiple collectors as in the preceding graph and
dataflow spec, downstream progress is still made even in the face of a
collector&#8217;s failure.  If collector B goes down, agent A, agent B,
agent E, and agent F continue to deliver events via collector A and
collector C respectively.  Agent C and agent D may have to queue their
logs until collector B (or its replacement) comes back online.</p></div>
<div class="paragraph"><p>The following configuration partitions the work from the set of agents
across many collectors.  In this example, each of the collectors
specify the same DFS output directory and file prefixes, aggregating
all of the logs into the same directory.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>agentA : src | agentE2ESink("collectorA",35853);
agentB : src | agentE2ESink("collectorA",35853);
agentC : src | agentE2ESink("collectorB",35853);
agentD : src | agentE2ESink("collectorB",35853);
agentE : src | agentE2ESink("collectorC",35853);
agentF : src | agentE2ESink("collectorC",35853);
collectorA : collectorSource(35853) | collectorSink("hdfs://...","src");
collectorB : collectorSource(35853) | collectorSink("hdfs://...","src");
collectorC : collectorSource(35853) | collectorSink("hdfs://...","src");</tt></pre>
</div></div>
<h4 id="_manually_specifying_failover_chains">Manually Specifying Failover Chains</h4>
<div class="imageblock">
<div class="content">
<img src="failoverCollector.png" alt="failoverCollector.png" />
</div>
</div>
<div class="paragraph"><p>When you have multiple collectors writing to the same storage
location, instead of having agent C and agent D queue indefinitely,
you can instead have them fail over to other collectors.  In this
scenario, you could have agent C and agent D fail over to collector A
and collector C respectively, while periodically checking to see if
collector B has returned.</p></div>
<div class="paragraph"><p>To specify these setups, use agents with <strong>failover chains</strong>. Similarly
to single collector agents, there are three levels of reliability for
the failover chain agents: <tt>agentE2EChain</tt>, <tt>agentDFOChain</tt>, and
<tt>agentBEChain</tt>.</p></div>
<div class="paragraph"><p>In the following example, you manually specify the failover chain
using <tt>agentE2EChain</tt>, an agent with end-to-end reliability with
multiple failover collectors.  <tt>agentA</tt> in this situation will
initially attempt to send to <tt>collectorA</tt> on port <tt>35853</tt>.  The second
argument in <tt>agentA</tt> 's sink specifies the collector to fall back onto
if the first fails.  You can specify an arbitrary number of
collectors, but you must specify at least one.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>agentA : src | agentE2EChain("collectorA:35853","collectorB:35853");
agentB : src | agentE2EChain("collectorA:35853","collectorC:35853");
agentC : src | agentE2EChain("collectorB:35853","collectorA:35853");
agentD : src | agentE2EChain("collectorB:35853","collectorC:35853");
agentE : src | agentE2EChain("collectorC:35853","collectorA:35853");
agentF : src | agentE2EChain("collectorC:35853","collectorB:35853");
collectorA : collectorSource(35853) | collectorSink("hdfs://...","src");
collectorB : collectorSource(35853) | collectorSink("hdfs://...","src");
collectorC : collectorSource(35853) | collectorSink("hdfs://...","src");</tt></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">In this section, <tt>agent[A-F]</tt> and <tt>collector[A-B]</tt> are physical host names.</td>
</tr></table>
</div>
<div class="paragraph"><p>As in the single collector case, if no port number is specified, the agent defaults to using the <tt>flume.collector.port</tt>.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>agentA : src | agentE2EChain("collectorA","collectorB");
agentB : src | agentE2EChain("collectorA","collectorC");
agentC : src | agentE2EChain("collectorB","collectorA");
agentD : src | agentE2EChain("collectorB","collectorC");
agentE : src | agentE2EChain("collectorC","collectorA");
agentF : src | agentE2EChain("collectorC","collectorB"); collectorA : collectorSource | collectorSink("hdfs://...","src");
collectorB : collectorSource | collectorSink("hdfs://...","src");
collectorC : collectorSource | collectorSink("hdfs://...","src");</tt></pre>
</div></div>
<div class="sidebarblock">
<div class="sidebar-content">
<div class="sidebar-title">Reliability and failover chain semantics</div>
<div class="ulist"><ul>
<li>
<p>
The <tt>agentE2EChain</tt> always <em>writes to WAL first</em> and then attempts
  to send to the different collectors.  In the event of a collector
  failure, it fails over to another collector.
</p>
</li>
<li>
<p>
The <tt>agentDFOChain</tt> always attempts to send data to the different
  collectors, and <em>only writes to disk if all collectors fail</em>.
</p>
</li>
<li>
<p>
The <tt>agentBEChain</tt> attempts to send message to each collector in
  succession in the event of failures.  If all collectors fail, it
  drops messages.
</p>
</li>
</ul></div>
</div></div>
<h4 id="_automatic_failover_chains">Automatic Failover Chains</h4>
<div class="paragraph"><p>Flume also provides a mechanism that automatically assigns failover chains based on how nodes are configured.  As collector nodes are assigned in the Flume Master, the Master attempts to distribute the agents evenly amongst the collectors.  In the face of failure, each agent is assigned a different failover chain.  This mitigates the chances of another collector becoming overloaded in the event of failure of a collector.</p></div>
<div class="paragraph"><p>To specify a node to use the failover chains, use either the <tt>autoE2EChain</tt>, <tt>autoDFOChain</tt>, or <tt>autoBEChain</tt> agent sink.  Because the Master calculates the failover chains, these sinks take no explicit arguments.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>agentA : src | autoE2EChain ;
agentB : src | autoE2EChain ;
agentC : src | autoE2EChain ;
agentD : src | autoE2EChain ;
agentE : src | autoE2EChain ;
agentF : src | autoE2EChain ;
collectorA : autoCollectorSource | collectorSink("hdfs://...", "src");
collectorB : autoCollectorSource | collectorSink("hdfs://...", "src");
collectorC : autoCollectorSource | collectorSink("hdfs://...", "src");</tt></pre>
</div></div>
<div class="paragraph"><p>The Master updates the configuration of the agents based on the current collectors in the system.  When new collectors are added to the system, the Master updates the failover chains of agents to rebalance.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">If the Master has no nodes with <tt>autoCollectorSource</tt> as its source, the agent&#8217;s automatic chains will report a <tt>fail("&#8230;")</tt> chain which will wait for <tt>autoCollectorSource</tt> s to be specified.  If the nodes are not mapped, they will report a different <tt>fail</tt> sink notifying you that the node is unmapped (isn&#8217;t associated with a host/port).</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">You can see the translation of the auto*Chain configuration in the node configuration table under the translated configuration column.  This is a little declarative specification of the failure recovery behavior of the sink.  More details on this are in the Advanced section of this guide, and in future revisions the translations for the agents and other chains will also be presented.</td>
</tr></table>
</div>
<h3 id="_logical_configurations">Logical Configurations</h3><div style="clear:left"></div>
<div class="paragraph"><p>Manually configuring nodes in Flume is manageable for a small number of nodes,
but can become burdensome for an operator as demands inevitably grow.
Ideally, the operator only has to assign a role to a particular machine.
Because configuration management is centralized via the Master, the Master
potentially has all the information necessary to intelligently create a node
topology and isolate flows of data from each other.</p></div>
<div class="paragraph"><p>To explain how this can be done, the concept of a <strong>logical node</strong> is
introduced. To manage communications between logical nodes, the concepts of
<strong>logical sources</strong> and <strong>logical sinks</strong> are introduced.  To isolate different
groups of nodes, the concept of a <strong>flow</strong> is introduced that allows you to
group agents and collectors into separate and isolated groups.</p></div>
<h4 id="_logical_nodes">Logical Nodes</h4>
<div class="paragraph"><p>The logical node abstraction allows for each JVM instance (a physical node) to
contain multiple logical nodes.  This allows for the processing of many source
sink combinations on many threads of execution to occur on a single JVM
instance.</p></div>
<div class="paragraph"><p>Each logical node has a name that may be completely different from its
physical name or hostname.  You now need new operations that enable you to
spawn a new node, map logical nodes to physical nodes, and decommission
existing logical nodes.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The following commands are entered via the web interface using the "raw
command" web page on the Master.  You might prefer using the Flume command
shell (described in a later section) for these operations.  The same commands
described in this section can be entered in web interface or entered at the
command shell by prefixing the command with <em>exec</em>.</td>
</tr></table>
</div>
<div class="paragraph"><p>Suppose that initially you know you want an agent-collector topology, but you
don’t know the particular names of the exact machines.  For now, you can
specify the configuration of the logical nodes without specifying any physical
machine names.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>agent1 : _source_ | autoBEChain ;
collector1 : collectorSource | collectorSink("hdfs://....") ;</tt></pre>
</div></div>
<div class="paragraph"><p>Later you learn that host1 is the name of the agent1 machine and host2 is the
name of the collector machine.  You can <em>map</em> logical nodes onto the
physical Flume instances on host1 and host2 by issuing the following map
commands:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>map host1 agent1
map host2 collector1</tt></pre>
</div></div>
<div class="paragraph"><p>Afterwards, the node status table should display a new row of information for
each logical node.  Each logical node reports its own execution state,
configuration, and heartbeat.  There is also a new entry in the logical node
mapping table showing that the logical node has been placed on the specified
physical node. To configure the node&#8217;s sources and sinks, use exactly the same
mechanisms described in the previous sections.</p></div>
<div class="paragraph"><p>You can also remove a logical node by using the <em>decommission</em> command.
Suppose you no longer needed agent1 and wanted to "turn it off".  You can do
so by entering the following command:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>decommission agent1</tt></pre>
</div></div>
<div class="paragraph"><p>This terminates the thread and removes the configuration associated with a
logical node, and the mapping between the logical node and physical node.</p></div>
<div class="paragraph"><p>You can also move a logical node from one physical node to another by first
unmapping a logical node and then mapping it on another physical node.  In
this scenario, you change the collector1 from being on host2 to host3.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>unmap host2 collector1</tt></pre>
</div></div>
<div class="paragraph"><p>At this point, the logical node mapping is removed, and collector1 is not
active anywhere. You can then map collector1 onto host3 by using the map
command:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>map host3 collector1</tt></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">There are some limitations that need to be further described in this
section.</td>
</tr></table>
</div>
<h4 id="_logical_sources_and_logical_sinks">Logical Sources and Logical Sinks</h4>
<div class="paragraph"><p>In the previous example, we used two abstractions under-the-covers that allow
the specifications of a graph topology for communications <em>without having to
use physical hostnames and ports</em>. These abstractions&#8201;&#8212;&#8201;the <strong>logical source</strong>
and <strong>logical sink</strong>&#8201;&#8212;&#8201;allow you to create a different graph topology without
having to know physical machines until they are mapped.</p></div>
<div class="paragraph"><p>Suppose you have two nodes producing data and sending it to the consumer:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>dataProducer1 : console | logicalSink("dataConsumer") ;
dataProducer2 : console | logicalSink("dataConsumer") ;
dataConsumer : logicalSource | console ;</tt></pre>
</div></div>
<div class="paragraph"><p>Note that in this example, the destination argument is the <strong>logical name</strong> of
the node and not a specific host/port combination.</p></div>
<div class="paragraph"><p>To implement these features, there is a generalized mechanism where users
enter logical configurations that are <em>translated</em> by the Master to a physical
configuration.</p></div>
<div class="paragraph"><p>When the logical nodes get mapped to physical nodes:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>map host1 dataProducer1
map host2 dataProducer2
map host3 dataConsumer</tt></pre>
</div></div>
<div class="paragraph"><p>and after the Master learns the host names (the host1, host2, and host3
machine’s heartbeat against the Master), the Master has enough information to
translate configurations with physical hostnames and ports.  A possible
translation would replace the logicalSource with a rpcSources and the
logicalSink with an rpcSinks:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>dataProducer1 : console | rpcSink("host3",56789) ;
dataProducer2 : console | rpcSink("host3",56789) ;
dataConsumer : rpcSource(56789) | console ;</tt></pre>
</div></div>
<div class="paragraph"><p>In fact, auto agents and collectors, are another example of <strong>translated
sources and sinks</strong>.  These translate auto*Chain sinks and collectorSource into
a configuration that uses logicalSinks and logicalSources which in turn are
translated into physical rpcSource and rpcSinks instances.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">Translations are powerful and can be fairly smart; if new collectors are
added, they will become new failover options.  If collectors are removed, then
the removed collectors will be automatically replaced by other failover nodes.</td>
</tr></table>
</div>
<h4 id="_flow_isolation">Flow Isolation</h4>
<div class="paragraph"><p>What happens if you want collect different kinds of data from the same
physical node?  For example, suppose you wanted to collect httpd logs as well
as syslog logs from the same physical machine.  Suppose also you want to write
all of the syslog data from the cluster in one directory tree, and all of the
httpd logs from the cluster in another.</p></div>
<div class="paragraph"><p>One approach is to tag all the data with source information and then push all
the data down the same pipe.  This could then be followed by some post-
processing that demultiplexes (demuxes) the data into different buckets.
Another approach is to keep the two sets of data logically isolated from each
other the entire time and avoid post processing.</p></div>
<div class="paragraph"><p>Flume can do both approaches but enables the latter lower-latency
approach, by introducing the concept of grouping nodes into
<strong>flows</strong>. Since there are logical nodes that allow for multiple nodes
on a single JVM, you can have a node for each different kinds of
produced data.</p></div>
<div class="paragraph"><p>The following example shows how flows can be used. Start by having six logical
nodes in the system.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>fooNode1 : fooSrc | autoBEChain ;
barNode1 : barSrc | autoBEChain ;
forNode2 : fooSrc | autoBEChain ;
barNode2 : barSrc | autoBEChain ;
fooConsumer : autoCollectorSource | collectorSink("hdfs://nn/foodir") ;
barConsumer : autoCollectorSource | collectorSink("hdfs://nn/bardir") ;</tt></pre>
</div></div>
<div class="paragraph"><p>In this scenario, there are two physical machines that produce both
kinds of data&#8201;&#8212;&#8201;foo data and bar data. You want to send data to
single collector that collects both foo data and bar data and writes
it to different HDFS directories.  You could then map the nodes onto
physical nodes:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>map host1 fooNode1
map host1 barNode1
map host2 fooNode2
map host2 barNode2
map host3 fooConsumer
map host3 barConsumer</tt></pre>
</div></div>
<div class="imageblock">
<div class="content">
<img src="single-flow.png" alt="single-flow.png" />
</div>
<div class="image-title">Figure 1: Flume Flows: Single Flow</div>
</div>
<div class="paragraph"><p>This setup essentially instantiates the first approach.  It mixes foo and bar
data together since the translation of autoBEChain would see two
collectorSources that the Master considers to be equivalent.  Foo data will
likely be sent to the barConsumer and bar data will likely be sent to
fooConsumer.</p></div>
<div class="paragraph"><p>You really wanted to separate sources of information into logically isolated
streams of data.  Flume provides a grouping abstraction called a <strong>flow</strong>.  A
flow groups particular logical nodes together so that the different logical
data types remain isolated.</p></div>
<div class="paragraph"><p>More concretely, it allows for a different failover chain for each kind of
data in the Flume cluster.  The auto*Chain based agents would only send data
to collectors in the same flow group.  This isolates data so that it only
flows to nodes within the group.</p></div>
<div class="paragraph"><p>Currently, the compact form of the configuration language does not
allow you to specify flows.  Instead you must add an extra argument to
the config command to specify a flow.</p></div>
<div class="paragraph"><p>This example shows commands that would be entered in the Flume shell
without flow group information.  In this case all of the nodes are in
the same flow.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>exec config fooNode1 fooSrc autoBEChain
exec config barNode1 barSrc autoBEChain
exec config fooNode2 fooSrc autoBEChain
exec config barNode2 barSrc autoBEChain
exec config fooConsumer autoCollectorSource 'collectorSink("hdfs://nn/foodir")'
exec config barConsumer autoCollectorSource 'collectorSink("hdfs://nn/bardir")'</tt></pre>
</div></div>
<div class="paragraph"><p>Now using the following commands you can specify flows by adding an
extra parameter after the node name.  In this example we have two
flows: flowfoo and flowbar.  flowfoo contains fooNode1, fooNode2 and
fooConsumer.  flowbar contains barNode1, barNode2 and barConsumer.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>exec config fooNode1 flowfoo fooSrc autoBEChain
exec config barNode1 flowbar barSrc autoBEChain
exec config fooNode2 flowfoo fooSrc autoBEChain
exec config barNode2 flowbar barSrc autoBEChain
exec config fooConsumer flowfoo autoCollectorSource 'collectorSink("hdfs://nn/foodir")'
exec config barConsumer flowbar autoCollectorSource 'collectorSink("hdfs://nn/bardir")'</tt></pre>
</div></div>
<div class="imageblock">
<div class="content">
<img src="multi-flow.png" alt="multi-flow.png" />
</div>
<div class="image-title">Figure 2: Flume Flows: Multiple Flows</div>
</div>
<div class="paragraph"><p>By using these commands, the data from fooNode1 and fooNode2 will only
be sent to fooConsumer, and barNode1 and barNode2&#8217;s data will only be
sent to barConsumer.  Data from one node is not mixed with other data
from other nodes unless explicitly connected.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">In practice it is a good idea to use different node names and
different flow ids for different kinds of data.  When node names are
reused, the default behavior is to attempt to recover from failures
assuming that leftover data from a crashed execution or previous
source/sink configuration version are still producing the same kind of
data.</td>
</tr></table>
</div>
<h4 id="_section_summary_4">Section Summary</h4>
<div class="paragraph"><p>This section introduced logical nodes, logical sources, logical sinks, and
flows and showed how these abstractions enable you to automatically deal with
manageability problems.</p></div>
<div class="ulist"><ul>
<li>
<p>
Only one input source per physical node.
</p>
</li>
<li>
<p>
Multiple sets of isolated flows.
</p>
</li>
<li>
<p>
Being machine specific, having to know all physical host names and ports.
</p>
</li>
</ul></div>
<div class="paragraph"><p>The translation mechanism can be quite powerful.  When coupled with
metrics information, this could be used to perform automated dynamic
configuration changes.  A possible example would be to automatically
commission or decommission new collectors to match diurnal traffic and
load patterns.</p></div>
<h3 id="_multiple_masters">Multiple Masters</h3><div style="clear:left"></div>
<div class="paragraph"><p>The Master has two main jobs to perform. The first is to keep track of all the nodes in a Flume deployment and to keep them informed of any changes to their configuration. The second is to track acknowledgements from the end of a Flume flow that is operating in <strong>reliable mode</strong> so that the source at the top of that flow knows when to stop transmitting an event.</p></div>
<div class="paragraph"><p>Both these jobs are critical to the operation of a Flume deployment. Therefore, it is ill-advised to have the Master live on a single machine, as this represents a single point of failure for the whole Flume service (see <em>failure modes</em> for more detail).</p></div>
<div class="paragraph"><p>Flume therefore supports the notion of multiple Masters which run on physically separate nodes and co-ordinate amongst themselves to stay synchronized. If a single Master should fail, the other Masters can take over its duties and keep all live flows functioning. This all happens transparently with a little effort at configuration time. Nodes will automatically fail over to a working Master when they lose contact with their current Master.</p></div>
<h4 id="_standalone_mode_compared_to_distributed_mode">Standalone Mode Compared to Distributed Mode</h4>
<div class="paragraph"><p>The Flume Master can be run in one of two ways.</p></div>
<div class="ulist"><ul>
<li>
<p>
<strong>Standalone</strong> mode - this is where the Master runs on a single machine. This is easy to administer, and simple to set-up, but has disadvantages when it comes to scalability and fault-tolerance.
</p>
</li>
<li>
<p>
<strong>Distributed</strong> mode - this is where the Master is configured to run on several machines - usually three or five. This option scales to serve many Flows, and also has good fault-tolerance properties.
</p>
</li>
</ul></div>
<div class="paragraph"><p>Large production deployments of Flume should run a distributed Master so that inevitable machine failures do not impact the availability of Flume itself. For small deployments the issue is less clear-cut - a distributed Master means reserving more computing resources that could be used instead for nodes or other services, and it is possible to recover from many failure modes in a timely manner with human intervention. The choice between distributed and standalone Masters is ultimately dependent both on your use case and your operating requirements.</p></div>
<h4 id="_running_in_standalone_mode">Running in Standalone Mode</h4>
<div class="sidebarblock">
<div class="sidebar-content">
<div class="paragraph"><p>Standalone mode is where Flume has only one Master node. The configuration described below needs to be done on that machine only.</p></div>
</div></div>
<div class="paragraph"><p>Whether the Flume Master starts in distributed or standalone mode is indirectly controlled by how many machines are configured to run as Master servers. To run in standalone mode, a single configuration property <tt>flume.master.servers</tt> must be set:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>&lt;property&gt;
&lt;name&gt;flume.master.servers&lt;/name&gt;
&lt;value&gt;hostA&lt;/value&gt;
&lt;/property&gt;</tt></pre>
</div></div>
<div class="paragraph"><p>The value of <tt>flume.master.servers</tt> is a comma-separated list of all the machine names (or IP addresses) that will be Master servers. If this list contains only machine name, the Flume Master will start in standalone mode. If there is more than one machine name in the list, the Flume Master will start in distributed mode.</p></div>
<div class="paragraph"><p>There&#8217;s no other configuration required for standalone mode. Flume will use reasonable default values for any other master-related variables. To start the Master, from the command prompt type:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ flume master</tt></pre>
</div></div>
<div class="paragraph"><p>from <tt>$FLUME_HOME</tt>. A number of log messages should print to the screen. After the server is running, you can check that everything is working properly by visiting the web interface at <tt>http://master-node-ip:35871/</tt>,
where <tt>master-node-ip</tt> is the IP address (or hostname) of the Master node. If you see a web page, the Master is running.</p></div>
<h4 id="_running_in_distributed_mode">Running in Distributed Mode</h4>
<div class="sidebarblock">
<div class="sidebar-content">
<div class="paragraph"><p>Distributed mode runs the Flume Master on several machines. Therefore the configuration described below should be done on <strong>every</strong> Master machine, except where noted. <strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong>**</p></div>
<div class="paragraph"><p>Running the Flume Master in distributed mode provides better fault tolerance than in standalone mode, and scalability for hundreds of nodes.</p></div>
<div class="paragraph"><p>Configuring machines to run as part of a distributed Flume Master is nearly as simple as standalone mode. As before, <tt>flume.master.servers</tt> needs to be set, this time to a list of machines:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>&lt;property&gt;
&lt;name&gt;flume.master.servers&lt;/name&gt;
&lt;value&gt;masterA,masterB,masterC&lt;/value&gt;
&lt;/property&gt;</tt></pre>
</div></div>
</div></div>
<div class="paragraph"><div class="title">How many machines do I need?</div><p>The distributed Flume Master will continue to work correctly as long as more than half the physical machines running it are still working and haven&#8217;t crashed. Therefore if you want to survive one fault, you need three machines (because 3-1 = 2 &gt; 3/2). For every extra fault you want to tolerate, add another two machines, so for two faults you need five machines. Note that having an even number of machines doesn&#8217;t make the Flume Master any more fault-tolerant - four machines only tolerate one failure, because if two were to fail only two would be left functioning, which is not more than half of four. Common deployments should be well served by three or five machines.</p></div>
<div class="sidebarblock">
<div class="sidebar-content">
<div class="paragraph"><p>The final property to set is <strong>not</strong> the same on every machine - every node in the Flume Master must have a unique value for <tt>flume.master.serverid</tt>.</p></div>
</div></div>
<div class="paragraph"><div class="title">Note</div><p><tt>flume.master.serverid</tt> is the only Flume Master property that <em>must</em> be different on every machine in the ensemble. <strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong><strong>*</strong></p></div>
<div class="listingblock">
<div class="title">masterA</div>
<div class="content"><!-- Generator: GNU source-highlight 3.1.3
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">&lt;property&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;name&gt;</span></span>flume.master.serverid<span style="font-weight: bold"><span style="color: #0000FF">&lt;/name&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;value&gt;</span></span>0<span style="font-weight: bold"><span style="color: #0000FF">&lt;/value&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/property&gt;</span></span></tt></pre></div></div>
<div class="listingblock">
<div class="title">masterB</div>
<div class="content"><!-- Generator: GNU source-highlight 3.1.3
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">&lt;property&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;name&gt;</span></span>flume.master.serverid<span style="font-weight: bold"><span style="color: #0000FF">&lt;/name&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;value&gt;</span></span>1<span style="font-weight: bold"><span style="color: #0000FF">&lt;/value&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/property&gt;</span></span></tt></pre></div></div>
<div class="listingblock">
<div class="title">masterC</div>
<div class="content"><!-- Generator: GNU source-highlight 3.1.3
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">&lt;property&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;name&gt;</span></span>flume.master.serverid<span style="font-weight: bold"><span style="color: #0000FF">&lt;/name&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;value&gt;</span></span>2<span style="font-weight: bold"><span style="color: #0000FF">&lt;/value&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/property&gt;</span></span></tt></pre></div></div>
<div class="paragraph"><p>The value for <tt>flume.master.serverid</tt> for each node is the index of
that node&#8217;s hostname in the list in <tt>flume.master.ensemble</tt>, starting
at 0. For example <tt>masterB</tt> has index 1 in that list. The purpose of
this property is to allow each node to uniquely identify itself to the
other nodes in the Flume Master.</p></div>
<div class="paragraph"><p>This is all the configuration required to start a three-node
distributed Flume Master. To test this out, we can start the Master
process on all three machines:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>[flume@masterA] flume master

[flume@masterB] flume master

[flume@masterC] flume master</tt></pre>
</div></div>
<div class="paragraph"><p>Each Master process will initially try and contact all other nodes in
the ensemble. Until more than half (in this case, two) nodes are alive
and contactable, the configuration store will be unable to start, and
the Flume Master will not be able to read or write configuration data.</p></div>
<div class="paragraph"><p>You can check the current state of the ensemble by inspecting the web page for any of the Flume Master machines which by default will be found at, for example, <tt>http://masterA:35871</tt>.</p></div>
<h4 id="_configuration_stores">Configuration Stores</h4>
<div class="paragraph"><p>The Flume Master stores all its data in a <strong>configuration store</strong>. Flume has a pluggable configuration store architecture, and supports two implementations.</p></div>
<div class="ulist"><ul>
<li>
<p>
The Memory-Backed Config Store (MBCS) stores configurations temporarily in memory. If the master node fails and reboots, all the configuration data will be lost. The MBCS is incompatible with distributed masters.  However, it is very easy to administer, computationally lightweight, and good for testing and experimentation.
</p>
</li>
<li>
<p>
The ZooKeeper-Backed Config Store (ZBCS) stores configurations   persistently and takes care of synchronizing them between multiple masters.
</p>
</li>
</ul></div>
<div class="paragraph"><div class="title">Flume and Apache ZooKeeper <strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong></div><p>Flume relies on the Apache ZooKeeper coordination platform to provide reliable, consistent, and persistent storage for node configuration data. A ZooKeeper ensemble is made up of two or more nodes which communicate regularly with each other to make sure each is up to date. Flume embeds a ZooKeeper server inside the Master process, so starting and maintaining the service is taken care of. However, if you have an existing ZooKeeper service running, Flume supports using that external cluster as well. <strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong><strong><strong></strong></strong></p></div>
<h4 id="_which_configuration_store_should_i_use">Which Configuration Store Should I Use?</h4>
<div class="paragraph"><p>In almost all cases, you should use the ZBCS. It is more reliable and fault-tolerant, and will recover configurations after a restart. It is compatible with both standalone and distributed deployments of the Flume Master.</p></div>
<div class="paragraph"><p>The MBCS is appropriate if you are experimenting with Flume and can stand to lose configuration if the machine fails.</p></div>
<div class="paragraph"><p>ZBCS is the default configuration store. The choice of which configuration store to use is controlled by the <tt>flume.master.store</tt> system property.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.3
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">&lt;property&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;name&gt;</span></span>flume.master.store<span style="font-weight: bold"><span style="color: #0000FF">&lt;/name&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;value&gt;</span></span>zookeeper<span style="font-weight: bold"><span style="color: #0000FF">&lt;/value&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/property&gt;</span></span></tt></pre></div></div>
<div class="paragraph"><p>If set to <tt>memory</tt>, the Flume Master will use MBCS instead. This is only supported in standalone mode.</p></div>
<h4 id="_configuring_the_zbcs">Configuring the ZBCS</h4>
<div class="paragraph"><p>Most deployments using the ZBCS can use Flume&#8217;s default configuration. However, where more control over the precise configuration of the Flume Master is needed, there are several properties that you can set.</p></div>
<div class="dlist"><dl>
<dt class="hdlist1">
Log Directory - <tt>flume.master.zk.logdir</tt> 
</dt>
<dd>
<p>
To ensure reliability and the ability to restore its state in the event of a failure, ZBCS continually logs all updates it sees to the directory in <tt>flume.master.zk.logdir</tt>. This directory must be writable by the user as which Flume is running, and will be created if it doesn&#8217;t exist at start-up time. WARNING: Do not delete this directory, or any files inside it. If deleted, all your configuration information will be lost.
</p>
</dd>
<dt class="hdlist1">
ZBCS Server Ports 
</dt>
<dd>
<p>
Each machine in the distributed Flume Master communicates with every other on the TCP ports set by <tt>flume.master.zk.server.quorum.port</tt> and <tt>flume.master.zk.server.election.port</tt>. The defaults are 3182 and 3183 respectively. Note that these settings control both the port on which the ZBCS listens, and on which it looks for other machines in the ensemble.
</p>
</dd>
<dt class="hdlist1">
ZBCS Client Port - <tt>flume.master.zk.client.port</tt> 
</dt>
<dd>
<p>
The Flume Master process communicates with ZooKeeper (either on the same machine, or remotely on another Master server) via a client TCP port, which is set by <tt>flume.master.zk.client.port</tt>. The default is 3181.
</p>
</dd>
</dl></div>
<h4 id="_gossip_in_distributed_mode">Gossip in Distributed Mode</h4>
<div class="paragraph"><p>Flume Master servers also use a <em>gossip</em> protocol to exchange information between themselves. Each server periodically wakes and picks another machine to send new data to. This protocol by default uses TCP port 57890, but this is controlled via the <tt>flume.master.gossipport</tt> property:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>&lt;property&gt;
&lt;name&gt;flume.master.gossip.port&lt;/name&gt;
&lt;value&gt;57890&lt;/value&gt;
&lt;/property&gt;</tt></pre>
</div></div>
<div class="paragraph"><p>In standalone mode, there is no need to use gossip, so this port is unused.</p></div>
<h4 id="_diagrams_how_the_masters_and_nodes_talk_to_each_other">Diagrams: How the Masters and Nodes talk to each other</h4>
<div class="imageblock">
<div class="content">
<img src="master-zk-standalone.png" alt="master-zk-standalone.png" />
</div>
<div class="image-title">Figure 3: Flume Master: Standalone Mode</div>
</div>
<div class="imageblock">
<div class="content">
<img src="master-zk-internal.png" alt="master-zk-internal.png" />
</div>
<div class="image-title">Figure 4: Flume Master: Distributed Mode</div>
</div>
<h4 id="_configuring_flume_nodes_to_connect_to_multiple_master_servers">Configuring Flume Nodes to Connect to Multiple Master Servers</h4>
<div class="paragraph"><p>One property needs to be set to configure a Flume Node to connect to multiple Masters: <tt>flume.master.servers</tt>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.3
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">&lt;property&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;name&gt;</span></span>flume.master.servers<span style="font-weight: bold"><span style="color: #0000FF">&lt;/name&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;value&gt;</span></span>masterA,masterB,masterC<span style="font-weight: bold"><span style="color: #0000FF">&lt;/value&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/property&gt;</span></span></tt></pre></div></div>
<div class="paragraph"><p>The nodes connect over the port <tt>flume.master.heartbeat.port</tt> on each machine in the Flume Master - this is the port that the Master servers listen on for node heartbeats.</p></div>
<div class="paragraph"><p>If a Master server fails, nodes will automatically fail over to the next randomly selected Master server that they can establish a connection to.</p></div>
<h3 id="_external_zookeeper_cluster">External ZooKeeper Cluster</h3><div style="clear:left"></div>
<div class="paragraph"><p>In some cases you may want a ZBCS that relies on an externally managed ZooKeeper service. The most common example of this is where multiple services which rely on ZooKeeper are being used (Flume and Hbase for example). In the following example zkServer{A,B,C}:2181 should be replaced with the hostname/port of the ZooKeeper servers which make up your ensemble.</p></div>
<div class="listingblock">
<div class="title"><tt>conf/flume-site.xml</tt></div>
<div class="content"><!-- Generator: GNU source-highlight 3.1.3
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">&lt;property&gt;</span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">&lt;name&gt;</span></span>flume.master.zk.use.external<span style="font-weight: bold"><span style="color: #0000FF">&lt;/name&gt;</span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">&lt;value&gt;</span></span>true<span style="font-weight: bold"><span style="color: #0000FF">&lt;/value&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/property&gt;</span></span>

<span style="font-weight: bold"><span style="color: #0000FF">&lt;property&gt;</span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">&lt;name&gt;</span></span>flume.master.zk.servers<span style="font-weight: bold"><span style="color: #0000FF">&lt;/name&gt;</span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">&lt;value&gt;</span></span>zkServerA:2181,zkServerB:2181,zkServerC:2181<span style="font-weight: bold"><span style="color: #0000FF">&lt;/value&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/property&gt;</span></span></tt></pre></div></div>
<h3 id="_section_summary_5">Section Summary</h3><div style="clear:left"></div>
<div class="paragraph"><p>This section described installing, deploying, and configuring a set of
Flume nodes in a fully distributed setting. You should now be able to
collect streams of logs with Flume.</p></div>
<div class="paragraph"><p>You also used some roles for sources and sinks to connect nodes
together.  You now have an understanding of the basics of setting up a
set of Flume nodes.  Here&#8217;s the new sources and sinks introduced in
this subsection.</p></div>
<div class="dlist"><div class="title">Flume&#8217;s Tiered Event Sources</div><dl>
<dt class="hdlist1">
<tt>collectorSource[(<em>port</em>)]</tt> 
</dt>
<dd>
<p>
Collector source. Listens for data from
agentSinks forwarding to port <tt><em>port</em></tt>.  If port is not specified, the
node default collector TCP port, 35853.
</p>
</dd>
</dl></div>
</div>
<h2 id="_integrating_flume_with_your_data_sources">Integrating Flume with your Data Sources</h2>
<div class="sectionbody">
<div class="sidebarblock">
<div class="sidebar-content">
<div class="sidebar-title">WARNING</div>
<div class="paragraph"><p>This section is incomplete.</p></div>
</div></div>
<div class="paragraph"><p>Flume&#8217;s source interface is designed to be simple yet powerful and enable logging
of all kinds of data&#8201;&#8212;&#8201;from unstructured blobs of byte, semi-structured blobs
with structured metadata, to completely structured data.</p></div>
<div class="paragraph"><p>In this section we describe some of the basic mechanisms that can be used to
pull in data.  Generally, this approach has three flavors. <strong>Pushing</strong> data to
Flume, having Flume <strong>polling</strong> for data, or <strong>embedding</strong> Flume or Flume
components into an application.</p></div>
<div class="paragraph"><p>These mechanisms have different trade-offs&#8201;&#8212;&#8201;based on the semantics of the
operation.</p></div>
<div class="paragraph"><p>Also, some sources can be <strong>one shot</strong> or <strong>continuous</strong> sources.</p></div>
<h3 id="_push_sources">Push Sources</h3><div style="clear:left"></div>
<div class="dlist"><dl>
<dt class="hdlist1">
<tt>syslogTcp</tt>, <tt>syslogUdp</tt> 
</dt>
<dd>
<p>
wire-compatibility with syslog, and syslog-ng
logging protocols.
</p>
</dd>
<dt class="hdlist1">
<tt>scribe</tt> 
</dt>
<dd>
<p>
wire-compatibility with the scribe log collection system.
</p>
</dd>
</dl></div>
<h3 id="_polling_sources">Polling Sources</h3><div style="clear:left"></div>
<div class="dlist"><dl>
<dt class="hdlist1">
<tt>tail</tt>, <tt>multitail</tt> 
</dt>
<dd>
<p>
watches a file(s) for appends.
</p>
</dd>
<dt class="hdlist1">
<tt>exec</tt> 
</dt>
<dd>
<p>
This is good for extracting custom data by using existing programs.
</p>
</dd>
<dt class="hdlist1">
<tt>poller</tt> 
</dt>
<dd>
<p>
We can gather information from Flume nodes themselves.
</p>
</dd>
</dl></div>
<h3 id="_embedding_sources">Embedding Sources</h3><div style="clear:left"></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">these features are incomplete.</td>
</tr></table>
</div>
<div class="paragraph"><p><tt>log4j</tt></p></div>
<div class="paragraph"><p><tt>simple client library</tt></p></div>
<div class="literalblock">
<div class="content">
<pre><tt>// move this to gathering data from sources</tt></pre>
</div></div>
<h3 id="_logging_via_log4j_directly">Logging via log4j Directly</h3><div style="clear:left"></div>
<h4 id="_example_of_logging_hadoop_jobs">Example of Logging Hadoop Jobs</h4>
<h4 id="_logging_hadoop_daemons">Logging Hadoop Daemons</h4>
</div>
<h2 id="_using_data_collected_by_flume">Using Data Collected by Flume</h2>
<div class="sectionbody">
<div class="paragraph"><p>The first goal of Flume is to collect data and reliably write it to HDFS.
Once data arrives, one wants the ability to control where and in what format
data is stored.  Flume provides basic output control mechanisms via the
properties configuration and in the dataflow language.  This gives the user
the ability to control the output format and output bucketing of incoming
data, and simplifies integration with other HDFS data consumers such as Hive
and HBase.</p></div>
<div class="paragraph"><p>Here are some example use cases:</p></div>
<div class="ulist"><ul>
<li>
<p>
When monitoring a web server, you want to bucket logs based on time,
  the page hit, and the browser being used.
</p>
</li>
<li>
<p>
When tracking particular data nodes, you want to bucket logs based on
  time and the data node name.
</p>
</li>
<li>
<p>
When tracking a feed of JIRA tickets from the Apache feed, you want
  to group based on the project identifier or a particular person.
</p>
</li>
<li>
<p>
When collecting data from scribe sources, you want to use its bucket
  data based on its the event&#8217;s category information.
</p>
</li>
</ul></div>
<div class="paragraph"><p>To support these kinds of features, Flume uses a simple data model,
provides a mechanism for bucketing events, and also provides basic
extraction operations for specifying custom bucketing discriminators.</p></div>
<h3 id="_the_data_model_of_a_flume_event">The Data Model of a Flume Event</h3><div style="clear:left"></div>
<div class="paragraph"><p>A Flume event has these six main fields:</p></div>
<div class="ulist"><ul>
<li>
<p>
Unix timestamp
</p>
</li>
<li>
<p>
Nanosecond timestamp
</p>
</li>
<li>
<p>
Priority
</p>
</li>
<li>
<p>
Source host
</p>
</li>
<li>
<p>
Body
</p>
</li>
<li>
<p>
Metadata table with an arbitrary number of attribute value pairs.
</p>
</li>
</ul></div>
<div class="paragraph"><p>All events are guaranteed to have all of these elements.  However, the
body may have zero length, and the metadata table can be empty.</p></div>
<div class="paragraph"><p>The Unix timestamp is measured in milliseconds and is Unix time stamp from the
source machine.  The nanosecond timestamp is machine specific nanosecond
counter also from the source machine.  It is safe to assume that the nanotime
from a machine is monotonically increasing&#8201;&#8212;&#8201;i.e. if event A has a larger
nanotime than event B from the same machine, event A was initially received
before event B.</p></div>
<div class="paragraph"><p>Currently the priority of a message can have one of 6 values: TRACE, DEBUG,
INFO, WARN, ERROR, or FATAL.  These values are often provided by logging
systems such as syslog or log4j.</p></div>
<div class="paragraph"><p>The source host is the name of the machine or the IP (whatever hostname call
returns).</p></div>
<div class="paragraph"><p>The body is the raw log entry body.  The default is to truncate the body to a
maximum of 32KB per event.  This is a configurable value and can be changed by
modifying the <em>flume.event.max.size.bytes</em> property.</p></div>
<div class="paragraph"><p>Finally there is the metadata table which is a map from a string attribute
name to an arbitrary array of bytes.  This allows for custom bucketing
attributes and will be described in more depth in the Advanced Usage section
of this guide.</p></div>
<h3 id="_output_bucketing">Output Bucketing</h3><div style="clear:left"></div>
<div class="paragraph"><p>You can control the output of events to particular directories or files based
on the values of an event&#8217;s fields.  To enable this, you provide an escaping
mechanism that outputs data to a particular path.</p></div>
<div class="paragraph"><p>For example, here is an output spec:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>collectorSink("hdfs://namenode/flume/webdata/%H00/", "%{host}-")</tt></pre>
</div></div>
<div class="paragraph"><p>The first argument is the directory where data is to be written.  The second
is a filename prefix where events are written.  Suppose you get an event from a
machine called server1 generated at time 18:58.  The events would get written
to HDFS with namenode namenode, in a directory called /flume/webdata/1800/,
with files named server1-xxx where xxx is some extra data for unique file
names.</p></div>
<div class="paragraph"><p>What happened here?  Flume replaced the <em>%H</em> with a string that represent the
hour of the timestamp found in the event&#8217;s data. Likewise, the <em>%o</em> was
replace with the hostname field from the event.</p></div>
<div class="paragraph"><p>What happens if the server1&#8217;s message had been delayed and the message wasn&#8217;t
sent downstream until 19:05?  Since the value of the timestamp on the event
was during the 18:00 hour, the event would be written into that directory.</p></div>
<div class="dlist"><div class="title">Event data escape sequences</div><dl>
<dt class="hdlist1">
[horizontal] %{host} 
</dt>
<dd>
<p>
host
</p>
</dd>
<dt class="hdlist1">
%{nanos} 
</dt>
<dd>
<p>
nanos
</p>
</dd>
<dt class="hdlist1">
%{priority} 
</dt>
<dd>
<p>
priority string
</p>
</dd>
<dt class="hdlist1">
%{body} 
</dt>
<dd>
<p>
body
</p>
</dd>
<dt class="hdlist1">
%% 
</dt>
<dd>
<p>
a <em>%</em> character.
</p>
</dd>
<dt class="hdlist1">
%t 
</dt>
<dd>
<p>
Unix time in millis
</p>
</dd>
</dl></div>
<div class="paragraph"><p>Because bucketing by date is a frequently-requested feature, there are escape
sequences for finer control of date values that allow you to bucket data based
on date.</p></div>
<div class="paragraph"><p>Here is another output spec:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>collectorSink("hdfs://namenode/flume/webdata/%Y-%m-%d/%H00/", "web-")</tt></pre>
</div></div>
<div class="paragraph"><p>This would create directories for each day, each with a subdirectory for each
hour with filenames prefixed "web-".</p></div>
<div class="hdlist"><div class="title">Fine grained escape sequences date and times</div><table>
<tr>
<td class="hdlist1">
%a 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
locale&#8217;s short weekday name (Mon, Tue, &#8230;)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%A 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
locale&#8217;s full weekday name (Monday, Tuesday, &#8230;)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%b 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
locale&#8217;s short month name (Jan, Feb,&#8230;)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%B 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
locale&#8217;s long month name (January, February,&#8230;)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%c 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
locale&#8217;s date and time (Thu Mar  3 23:05:25 2005)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%d 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
day of month (01)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%D 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
date; same as %m/%d/%y
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%H 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
hour (00..23)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%I 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
hour (01..12)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%j 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
day of year (001..366)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%k 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
hour ( 0..23)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%l 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
hour ( 1..12)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%m 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
month (01..12)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%M 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
minute (00..59)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%P 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
locale&#8217;s equivalent of am or pm
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%s 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
seconds since 1970-01-01 00:00:00 UTC
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%S 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
second (00..60)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%y 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
last two digits of year (00..99)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%Y 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
year (2010)
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
%z 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
+hhmm numeric timezone (for example, -0400)
</p>
</td>
</tr>
</table></div>
<h3 id="_output_format">Output Format</h3><div style="clear:left"></div>
<div class="paragraph"><p>Now that you have control of where files go, this section describes how you can
control the output format of data.  Currently, this is set via the
<em>flume.collector.output.format</em> property set in the flume-site.xml file.  The output
formats are:</p></div>
<div class="hdlist"><div class="title">Output formats</div><table>
<tr>
<td class="hdlist1">
syslog 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
a syslog like text output format
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
log4j 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
a log4j pattern similar to that used by CDH output pattern.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
raw 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Event body only.  This is most similar to copying a file but
does not preserve any uniqifying metadata like host/timestamp/nanos.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
avro 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Avro Native file format.  Default currently is uncompressed.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
avrojson 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
JSON encoded date generated by avro
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
avrodata 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Binary encoded data written in the avro binary format.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
default 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
a debugging format.
</p>
</td>
</tr>
</table></div>
<div class="listingblock">
<div class="content">
<pre><tt>&lt;property&gt;
  &lt;name&gt;flume.collector.output.format&lt;/name&gt;
  &lt;value&gt;avrojson&lt;/value&gt;
  &lt;description&gt;This is the output format for the data written to the
  collector.  There are several formats available:
    syslog - outputs events in a syslog-like format
    log4j - outputs events in a pattern similar to Hadoop's log4j pattern
    raw - Event body only.  This is most similar to copying a file but
      does not preserve any uniqifying metadata like host/timestamp/nanos.
    avro - Avro Native file format.  Default currently is uncompressed.
    avrojson - this outputs data as json encoded by avro
    avrodata - this outputs data as an avro binary encoded data
    debug - this is a format for debugging
  &lt;/description&gt;
&lt;/property&gt;</tt></pre>
</div></div>
<h3 id="_small_files_compared_to_high_latency">Small Files Compared to High Latency</h3><div style="clear:left"></div>
<div class="paragraph"><p>For all versions Hadoop&#8217;s file system that are <= 0.20.x, HDFS has write-once
read-many semantics. Thus, the only way to reliably flush an HDFS file is
to close the file.  Moreover, once a file is closed, no new data can be
appended to the file.  This presents a tension between getting data written
quickly to HDFS and potentially having many small files (which is a potential
scalability bottleneck of HDFS).</p></div>
<div class="paragraph"><p>On one side, to minimize the load and data stored throughput the system,
ideally one would flush data to HDFS as soon as it arrives. Flushing
frequently is in conflict with efficiently storing data to HDFS because this
could result in many small files, which eventually will stress an HDFS
namenode.  A compromise is to pick a reasonable trigger that has a collector
close "reasonably-sized" files (ideally larger than a single HDFS block, 64MB
by default).</p></div>
<div class="paragraph"><p>When Flume is deployed at a scale where data collection volumes are small, it
may take a long time to reach the ideal minimum file size (a block size,
typically 64MB).  For example, if a single web server produces 10k of logs a
second (approx. 100 hit logs/s at 100B per log), it will take about 2 hours
(6400 seconds) before an ideal file size can reached.</p></div>
<div class="paragraph"><p>In these situations, lean towards having more small files.  Small files
cause a few problems downstream.  These include potential scaling limitations
of Hadoop&#8217;s HDFS, and performance penalties when using MapReduce&#8217;s default
input processing mechanisms within Hadoop.</p></div>
<div class="paragraph"><p>The following sections describe two mechanisms to mitigate these
potential problems:</p></div>
<div class="ulist"><ul>
<li>
<p>
Rolling up many small data files into larger batches
</p>
</li>
<li>
<p>
Using a CombinedFileInputFormat
</p>
</li>
</ul></div>
<div class="paragraph"><p>This particular problem becomes less of an issue when the scale of logging
goes up.  If a hundred machines were generating the same amount of logs, you
would reach reasonable files sizes every 64 seconds.</p></div>
<div class="paragraph"><p>Future versions of Hadoop will mitigate this problem by providing a flush/sync
operation for currently open HDFS files (patch is already slated for
Hadoop HDFS 0.21.x).</p></div>
</div>
<h2 id="_compression_for_files_written_to_hdfs">Compression for files written to HDFS.</h2>
<div class="sectionbody">
<div class="paragraph"><p>Flume supports basic compression for all log files that are written to
HDFS.  Compressed files are automatically suffixed with an extension
and follow the same naming format + directory structure as regular log
files.</p></div>
<div class="paragraph"><p>If GZipCodec is selected, ".gz" is appended to the file name, if
BZip2Codec is selected, ".bz2" is appended.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>  &lt;property&gt;
    &lt;name&gt;flume.collector.dfs.compress.codec&lt;/name&gt;
    &lt;value&gt;None&lt;/value&gt;
    &lt;description&gt;Writes formatted data compressed in specified codec to
    dfs. Value is None, GZipCodec, DefaultCodec (deflate), BZip2Codec,
    or any other Codec Hadoop is aware of &lt;/description&gt;
  &lt;/property&gt;</tt></pre>
</div></div>
</div>
<h2 id="_advanced_flume_usage">Advanced Flume Usage</h2>
<div class="sectionbody">
<div class="paragraph"><p>This section describes in further detail, how to automate the
control of Flume nodes via the FlumeShell, a deep dive into Flume&#8217;s dataflow
specification language, internals of the reliability mechanisms, how to do
metadata manipulations, and how to install source and sink plugins.</p></div>
<h3 id="_the_flume_command_shell">The Flume Command Shell</h3><div style="clear:left"></div>
<div class="paragraph"><p>So far, you have been modifying the state of Flume using a simple (but
primitive) web interface to a Master server.</p></div>
<div class="paragraph"><p>Flume also provides a shell, which allows the user to type commands into a
terminal and have them executed on a Flume deployment.</p></div>
<div class="paragraph"><p>All of the commands available in the web form are available in the
Flume Shell.  The Flume Shell, however, actually has extra controls
for command submission control and state checking that aid
scriptability.</p></div>
<h4 id="_using_the_flume_command_shell">Using the Flume Command Shell</h4>
<div class="paragraph"><p>You can start the FlumeShell by running <tt>flume shell</tt> in a terminal window.
The <tt>connect</tt> command can be used to establish a connection to any Master
server.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>hostname:~/flume$ flume shell
[flume (disconnected)] connect localhost:35873
Connecting to Flume master localhost:35873...
[flume localhost:35873]</tt></pre>
</div></div>
<div class="listingblock">
<div class="content">
<pre><tt>hostname:~/flume$ flume shell -c localhost:35873
Connecting to Flume master localhost:35873...
[flume localhost:35873]</tt></pre>
</div></div>
<div class="sidebarblock">
<div class="sidebar-content">
<div class="paragraph"><p>The port to use is the value of <tt>flume.config.admin.port</tt> and defaults to 35873.</p></div>
</div></div>
<div class="paragraph"><p>The command line parameters for the Flume Shell are as follows:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>usage: FlumeShell [-c &lt;arg&gt;] [-e &lt;arg&gt;] [-q] [-s &lt;arg&gt;]
 -?         Command line usage help
 -c &lt;arg&gt;   Connect to master:port
 -e &lt;arg&gt;   Run a single command
 -q         Run in quiet mode - only print command results
 -s &lt;arg&gt;   Run a FlumeShell script</tt></pre>
</div></div>
<div class="paragraph"><p>The FlumeShell makes scripting Flume possible - either by using a single
invocation with <tt>-e</tt> or by running a script of commands with <tt>-s</tt>. It is also
possible to pipe <tt>stdin</tt> to the FlumeShell as in the following example:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>echo "connect localhost:35873\ngetconfigs\nquit" | flume shell -q</tt></pre>
</div></div>
<div class="paragraph"><div class="title">Flume Commands</div><p>You can press Tab any time for some hints on available commands.  If you start
typing a command you can use TAB to complete command.</p></div>
<div class="dlist"><dl>
<dt class="hdlist1">
<tt>help</tt> 
</dt>
<dd>
<p>
List the commands available in the shell.
</p>
</dd>
<dt class="hdlist1">
<tt>connect <em>master:port</em></tt> 
</dt>
<dd>
<p>
connect to a master at machine <em>master</em> on port
<em>port</em>.
</p>
</dd>
<dt class="hdlist1">
<tt>config</tt> <em>logicalnode</em> <em>source</em> <em>sink</em> 
</dt>
<dd>
<p>
configure a single logical node
<em>logicalnode</em> with source <em>source</em> and sink <em>sink</em>.  <em>source</em> and <em>sink</em> will
likely need quotes to support some of the Flume configuration syntax.
</p>
</dd>
<dt class="hdlist1">
<tt>getnodestatus</tt> 
</dt>
<dd>
<p>
Output the status of the nodes the master knows
about. Nodes are in either HELLO, CONFIGURING, ACTIVE, IDLE, ERROR,
DECOMMISSIONED, or LOST states.  When a node shows up initially it is
HELLO state.  When a node is being configured, it is in CONFIGURING
state.  Once events are being pumped from source to sink, the node is
in ACTIVE state.  If a node has drained its source (and the source is
not "endless") it will enter IDLE state.  If a node encountered an
unrecoverable error or exited without flushing, it will be in ERROR
state.  A node is DECOMMISSIONED if it is removed on the master, and
LOST if it has not been seen by the master for a "long time".
</p>
</dd>
<dt class="hdlist1">
<tt>getconfigs</tt> 
</dt>
<dd>
<p>
This gets and dumps the configuration specifications of all
the logical nodes the master knows about.
</p>
</dd>
<dt class="hdlist1">
<tt>getmappings [<em>physical node</em>]</tt> 
</dt>
<dd>
<p>
Display all logical nodes mapped to
<em>physical node</em> or all mappings if <em>physical node</em> is omitted.
</p>
</dd>
<dt class="hdlist1">
<tt>exec</tt> 
</dt>
<dd>
<p>
Synchronously execute a command on the master.  This command will
block until it is completed.
</p>
</dd>
<dt class="hdlist1">
<tt>source <em>file</em></tt> 
</dt>
<dd>
<p>
Reads the specified file and attempts to execute all of the
specified commands.
</p>
</dd>
<dt class="hdlist1">
<tt>submit</tt> 
</dt>
<dd>
<p>
Asynchronously execute a command on the master.  This command will
return immediately and allows the submission of other commands.  The command
ID of the last command submitted is recorded.
</p>
</dd>
<dt class="hdlist1">
<tt>wait <em>ms</em> [<em>cmdid</em>]</tt> 
</dt>
<dd>
<p>
This commands blocks for up to <tt>ms</tt> milliseconds
until <tt>cmdid</tt> has entered the SUCCEEDED or FAILED state. If <tt>ms</tt> is 0 the
command may block forever.  If the command times out, the shell will
disconnect.  This is useful in conjunction with <tt>submitted</tt> commands.
</p>
</dd>
<dt class="hdlist1">
<tt>waitForNodesActive <em>ms</em> node1 [node2 [&#8230;]]</tt> 
</dt>
<dd>
<p>
This command blocks for up to
<tt>ms</tt> milliseconds until the specified list of nodes have entered the ACTIVE or
CONFIGURING state.  If ms==0 then the command may block forever.
</p>
</dd>
<dt class="hdlist1">
<tt>waitForNodesDone <em>ms</em> node1 [node2 [&#8230;]]</tt> 
</dt>
<dd>
<p>
This command blocks for up to
<tt>ms</tt> milliseconds until the specified list of nodes have entered the IDLE,
ERROR, or LOST state.
</p>
</dd>
<dt class="hdlist1">
<tt>quit</tt> 
</dt>
<dd>
<p>
Exit the shell.
</p>
</dd>
</dl></div>
<div class="paragraph"><div class="title">Exec and Submit commands</div><p>Both the web form and the FlumeShell are interfaces to the same command
processing infrastructure inside Flume. This section introduces the
FlumeShell and show how you can use it to make administering Flume more
simple.</p></div>
<div class="paragraph"><p>These commands are issued and run as if run from the master.  In the command
shell they have the form:</p></div>
<div class="paragraph"><p><tt>exec <em>command</em> [<em>arg1 [_arg2</em> [ &#8230; ] ] ]</tt></p></div>
<div class="paragraph"><p><tt>submit <em>command</em> [<em>arg1 [_arg2</em> [ &#8230; ] ] ]</tt></p></div>
<div class="paragraph"><p>Complex arguments like those with spaces, or non alpha-numeric characters can
be expressed by using "double quotes"s and `single quotes&#8217;s.  If enclosed in
double quotes, the bodies of the strings are Java string unescaped.  If they
are enclosed in single quotes, arbitrary characters can be included except for
the ' character.</p></div>
<div class="paragraph"><p><tt>exec</tt> commands block until they are completed.  <tt>submit</tt> commands are
asynchronously sent to the master in order to be executed.  <tt>wait</tt> are
essentially joins for recently <tt>submit</tt> ted commands.</p></div>
<div class="dlist"><dl>
<dt class="hdlist1">
<tt>noop</tt> 
</dt>
<dd>
<p>
This command contacts the master and issues a noop (no
operation) command.
</p>
</dd>
<dt class="hdlist1">
<tt>config <em>logicalnode</em> <em>source</em> <em>sink</em></tt> 
</dt>
<dd>
<p>
This command configures a node.
This is nearly identical to the <em>config</em> command.
</p>
</dd>
<dt class="hdlist1">
<tt>multiconfig <em><em>flumespec</em></em></tt> 
</dt>
<dd>
<p>
This command configures a set of nodes on the
master using the aggregated format.
</p>
</dd>
<dt class="hdlist1">
<tt>unconfig <em>logicalnode</em></tt> 
</dt>
<dd>
<p>
This command changes the configuration of a
particular node to have a <tt>null</tt> source and a <tt>null</tt> state.  +refresh
</p>
</dd>
<dt class="hdlist1">
<em>logicalnode</em>+ 
</dt>
<dd>
<p>
This command refreshes the current configuration of a
logical node.  This forces the logicalnode to stop and then restart.  This
also causes a master re-evaluation that may change the failover lists.
</p>
</dd>
<dt class="hdlist1">
<tt>refreshAll <em>logicalnode</em></tt> 
</dt>
<dd>
<p>
This atomically issues a refresh command to all
of the logical nodes.
</p>
</dd>
<dt class="hdlist1">
<tt>save <em><em>filename</em></em></tt> 
</dt>
<dd>
<p>
This saves the current configuration to the master&#8217;s
disk.
</p>
</dd>
<dt class="hdlist1">
<tt>load <em><em>filename</em></em></tt> 
</dt>
<dd>
<p>
This augments the current configuration with the
logical node specifications found in <tt><em>filename</em></tt>.
</p>
</dd>
<dt class="hdlist1">
<tt>map <em>physicalnode</em> <em>logicalnode</em></tt>
</dt>
<dd>
<p>
This creates a new mapping between logical
node <em>logicalnode</em> and physical node <tt><em>physicalnode</em></tt>.  The node starts with a
<tt>null</tt> source and a <tt>null</tt> sink, and updates its configuration specified at the
master when it begins heartbeating.  Thus if a logical node configuration
already exists and is mapped, it will pick up the configuration for the logical
node.
</p>
</dd>
<dt class="hdlist1">
<tt>spawn <em>physicalnode</em> <em>logicalnode</em></tt>
</dt>
<dd>
<p>
The <tt>spawn: command is a synonym for the
+map</tt> command and has been deprecated.
</p>
</dd>
<dt class="hdlist1">
<tt>decommission <em>logicalnode</em></tt> 
</dt>
<dd>
<p>
This removes a logical node from the logical
node configuration table, and unmaps it from any physical nodes it may be
installed on.
</p>
</dd>
<dt class="hdlist1">
<tt>unmap <em>physicalnode</em> <em>logicalnode</em></tt> 
</dt>
<dd>
<p>
This command breaks the assignment of
a <em>logicalnode</em> from machine <em>physicalnode</em>.  A logical node can be reassigned
to another physical node using the <tt>map</tt> command.
</p>
</dd>
<dt class="hdlist1">
<tt>unmapAll</tt> 
</dt>
<dd>
<p>
This command breaks the assignment of all logical node from
physical nodes.  A logical node can be reassigned to another physical node
using the <tt>map</tt> command.
</p>
</dd>
</dl></div>
<h3 id="_flume_8217_s_dataflow_specification_language">Flume&#8217;s Dataflow Specification Language</h3><div style="clear:left"></div>
<div class="paragraph"><p>Using the Flume node roles (collector, agent) is the simplest method to get up
and running with Flume.  Under the covers, these sources and sinks
(collectorSink, collectorSource, and agentSource) are actually composed of
primitive sinks that have been augmented with <strong>translated components</strong> with
role defaults, <strong>special sinks</strong> and <strong>sink decorators</strong>. These components make
configuration more flexible, but also make configuration more complicated.
The combination of special sinks and decorators expose a lot of details of the
underlying mechanisms but are a powerful and expressive way to encode rich
behavior.</p></div>
<div class="paragraph"><p>Flume enables users to enter their own composition of sinks, sources, and
decorators by using a domain specific data flow language.  The following
sections will describe this in more details.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>nodeName      ::=  NodeId
simpleSource  ::=  SourceId args?
simpleSink    ::=  SinkId args?
decoratorSink ::=  DecoId args?

source ::= simpleSource

sink ::=   simpleSink            // single sink
     |     [ sink (, sink)* ]    // fanout sink
     |     { decoratorSink =&gt; sink }  // decorator sink
     |     &lt; sink ? sink &gt;           // failover / choice sink
     |     roll(...) { sink }           // roll sink
     |     let SinkId := sink in sink  // let expression

logicalNode ::= NodeId : source | sink ;

spec   ::=  (logicalNode)*</tt></pre>
</div></div>
<h4 id="_special_sinks_fan_out_fail_over_and_roll">Special Sinks: Fan out, Fail over, and Roll</h4>
<div class="paragraph"><p>Three special sinks are FanOutSinks, FailoverSinks, and RollSinks.</p></div>
<div class="paragraph"><p>Fanout sinks send any incoming events to all of the sinks specified to be its
children.  These can be used for data replication or for processing data off
of the main reliable data flow path.  Fanout is similar to the Unix <tt>tee</tt>
command or logically acts like an AND operator where the event is sent to each
subsink.</p></div>
<div class="paragraph"><p>The syntax for a FanoutSink is :</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>[ console, collectorSink ]</tt></pre>
</div></div>
<div class="paragraph"><p>FailoverSinks are used to handle failures when appending new events.
FailoverSinks can be used to specify alternate collectors to contact in the
event the primary collector fails, or a local disk sink to store data until
the primary collector recovers.  Failover is similar to exception handling and
logically acts like an OR operator. If a failover is successful, one of the
subsinks has received the event.</p></div>
<div class="paragraph"><p>The syntax for a FailoverSink is :</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>&lt; logicalSink("collector1") ? logicalSink("collector2") &gt;</tt></pre>
</div></div>
<div class="paragraph"><p>So, you could configure node "agent1" to have a failover to collector2 if
collector1 fails (for example, if the connection to <tt>collector1</tt> goes down or
if <tt>collector1</tt>'s HDFS becomes full):</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>agent1 : source | &lt; logicalSink("collector1") ? logicalSink("collector2")
&gt; ;</tt></pre>
</div></div>
<div class="paragraph"><p>Roll sink opens and closes a new instance of its subsink every
<em>millis</em> milliseconds.  A roll is an atomic transition that closes
the current instance of the sub-sink, and then opens a new instance of
as an escape sequence to differentiate data in different roll periods.
This can be used to make every roll produce a new file with a unique
name.</p></div>
<div class="paragraph"><p>The syntax for a roll sink is:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>roll(millis) sink</tt></pre>
</div></div>
<div class="paragraph"><p>These can be composed to have even richer behavior.  For example, this sink
outputs to the console and has a failover collector node.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>[ console, &lt; logicalSink("collector1") ? logicalSink("collector2") &gt; ]</tt></pre>
</div></div>
<div class="paragraph"><p>This one rolls the collector every 1000 milliseconds writing to a different
HDFS file after each roll.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>roll(1000) [ console, escapedCustomDfs("hdfs://namenode/flume/file-%{rolltag}") ]</tt></pre>
</div></div>
<h4 id="_introducing_sink_decorators">Introducing Sink Decorators</h4>
<div class="paragraph"><p>Fan out and failover affect where messages go in the system but do not modify
the messages themselves.  To augment or filter events as they pass through the
dataflow, you can use <strong>sink decorators</strong>.</p></div>
<div class="paragraph"><p>Sink decorators can add properties to the sink and can modify the data streams
that pass through them.  For example, you can use them to increase reliability
via write ahead logging, increase network throughput via batching/compression,
sampling, benchmarking, and even lightweight analytics.</p></div>
<div class="paragraph"><p>The following simple sampling example uses an intervalSampler which is
configured to send every 10th element from source "source" to the sink "sink":</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>flumenode: source | { intervalSampler(10) =&gt; sink };</tt></pre>
</div></div>
<div class="paragraph"><p>Here&#8217;s an example that batches every 100 events together.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>flumenode: source | { batch(100) =&gt; sink };</tt></pre>
</div></div>
<div class="paragraph"><p>Like fanout and failover, decorators are also composable. Here is an example
that creates batches of 100 events and then compresses them before moving
them off to the sink:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>flumenode: source | { batch(100) =&gt;  { gzip =&gt; sink } };</tt></pre>
</div></div>
<h4 id="_translations_of_high_level_sources_and_sinks">Translations of High-level Sources and Sinks</h4>
<div class="paragraph"><p>Internally, Flume translates sinks into compositions of other simpler
decorators, failovers, rollers to add properties.  The proper compositions
create pipelines that provide different levels of reliability in Flume.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">This section describes how agents work, but this version of
Flume does not expose the translations the same way the auto*Chains
are exposed.  A future version of Flume will expose these details.
The exact translations are still under development.</td>
</tr></table>
</div>
<div class="paragraph"><p>Suppose you have nodes using the different agent sinks:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>node1 : tail("foo") | agentE2ESink("bar");
node2 : tail("foo") | agentDFOSink("bar");
node3 : tail("foo") | agentBESink("bar");</tt></pre>
</div></div>
<div class="paragraph"><p>In the translation phases, agentE2ESink is actually converted into these Flume
sinks:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>node1 : tail("foo") | { ackedWriteAhead =&gt; { lazyOpen =&gt; { stubbornAppend
=&gt; logicalSink("bar") } } } ;
node2 : tail("foo") | let primary := { lazyOpen -&gt; {stubbornAppend =&gt; logicalSink("bar") } } in &lt; primary ? { diskFailover =&gt; { insistentOpen =&gt; primary} } &gt;;
node3 : tail("foo") | { lazyOpen =&gt; { stubbornAppend =&gt;  logicalSink("bar")  } };</tt></pre>
</div></div>
<div class="paragraph"><p><tt>ackedWriteAhead</tt> is actually a complicated decorator that internally
uses rolls and some other special decorators.  This decorator
interface allows us to manually specify wire batching and compression
options.  For example, you could compress every 100 messages using
gzip compression:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>node1 : tail("foo") | { ackedWriteAhead =&gt; { batch(100) =&gt; { gzip =&gt; {
lazyOpen =&gt; { stubbornAppend =&gt; logicalSink("bar") } } } } };</tt></pre>
</div></div>
<div class="paragraph"><p><tt>collectorSink("xxx","yyy",15000)</tt> is also a bit complicated with some custom
decorators to handle acks.  Under the covers however, it depends on a roller
with a escapedCustomDfsSink inside of it.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>roll(15000) { collectorMagic =&gt;  escapedCustomDfs("xxx", "yyy-%rolltag") }</tt></pre>
</div></div>
<div class="paragraph"><p>Another place translations happen is with logical nodes.  Lets start of with a
few nodes:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>node1 : tail("foo") | { .... =&gt; logicalSink("node2") };
node2 : logicalSource | collectorSink("...");</tt></pre>
</div></div>
<div class="paragraph"><p>The translation mechanisms converts logicalSources and logicalSinks into lower
level physical rpcSources and rpcSinks.  Lets assume that node1 is on machine
host1 and node2 is on machine host2.  After the translation you end up with the
following configurations:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>node1 : tail("foo") | { .... =&gt; rpcSink("host2",12345) };
node2 : rpcSource(12345) | collectorSink("..."");</tt></pre>
</div></div>
<div class="paragraph"><p>Suppose that you swap the mapping so that node2 is now on host1 and node1 is
on host2.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt># flume shell commands
exec unmapAll
exec map host1 node2
exec map host2 node1</tt></pre>
</div></div>
<div class="paragraph"><p>The original configuration will now be translated to:</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>node1 : tail("foo") | { .... =&gt; rpcSink("host1",12345) };
node2 : rpcSource(12345) | collectorSink("..."");</tt></pre>
</div></div>
<h3 id="_custom_metadata_extraction">Custom Metadata Extraction</h3><div style="clear:left"></div>
<div class="paragraph"><p>While Flume can take in raw data, you can add structure to data based on the
nodes they flowed through, and filter out portions of data to minimize the
amount of raw data needed if only a portion is needed.</p></div>
<div class="paragraph"><p>The simplest is the <tt>value</tt> decorator.  This takes two arguments: an attribute
name, and a value to add to the event&#8217;s metadata.  This can be used to
annotate constant information about the source, or arbitrary data to a
particular node.  It could also be used to naively track the provenance of
data through Flume.</p></div>
<h4 id="_extractors">Extractors</h4>
<div class="paragraph"><p>Extraction operations are also available to extract values from logs that
have known structure.</p></div>
<div class="paragraph"><p>One example is the <tt>regex</tt> decorator.  This takes three arguments: a regular
expression, an index, and an attribute name which allows the user to use a
extract a particular regex group out of the body of the event and write it as
the value of the specified attribute.</p></div>
<div class="paragraph"><p>Similarly the <tt>split</tt> decorator also takes three arguments: a regular
expression, an index, and an attribute name.  This splits the body
based on the regular expression, extracts the text group after the
instance of the separator, and writes the value to the specified
attribute.  One can think of this as a much simplified version of the
<tt>awk</tt> Unix utility.</p></div>
<h4 id="_meta_data_filtering_and_transformations">Meta Data Filtering and Transformations</h4>
<div class="paragraph"><p>Flume enforces an invariant that prevents the modification of an attribute
that has already been written upstream.  This simplifies debugging of
dataflows, and improves the visibility of data when debugging.</p></div>
<div class="paragraph"><p>If many stages are used, however, frequently a lot of extra metadata ends up
being moved around.  To deal with this, a few extra operations are available.</p></div>
<div class="paragraph"><p>A <tt>select</tt> operation is available.  This operation is like SQL select, which
provides a relational calculus&#8217;s set projection operation that modifies an
event so that the specified metadata fields are forwarded.</p></div>
<div class="paragraph"><p>A <tt>mask</tt> operation is also available that forwards all metadata attributes
except for the attributes specified.</p></div>
<div class="paragraph"><p>A <tt>format</tt> decorator is also available that uses the escape mechanisms
to rewrite the body of an event to a user customizable message.  This is
useful for outputting summary data to low volume sources.  Example: writing
summary information out to an IRC channel periodically.</p></div>
<h4 id="_role_defaults">Role Defaults</h4>
<div class="paragraph"><p>To simplify things for users, you can assign a particular role to a logical
node&#8201;&#8212;&#8201;think of these as an "automatic" specification that have
default settings.  Two roles currently provided are the agent role and the
collector role.  Since there are many nodes with the same role, each of
these are called a tier.  So, for example, the agent tier consists of all the nodes
that are in the agent role.  Nodes that have the collector role are in the
collector tier.</p></div>
<div class="paragraph"><p>Agents and collectors roles have defaults specified in the <tt>conf/flume-site.xml</tt>
file.  Look at the <tt>conf/flume-conf.xml</tt> file for properties prefixed with
<tt>flume.agent.*</tt> and <tt>flume.collector.*</tt> for descriptions of the configuration
options.</p></div>
<div class="paragraph"><p>Each node maintains it own node state and has its own configuration. If the
master does not have a data flow configuration for the logical node, the
logical node will remain in IDLE state.  If a configuration is present, the
logical node will attempt to instantiate the data flow and have it work
concurrently with other data flows.</p></div>
<div class="paragraph"><p>This means that each machine essentially only lives in one tier.  In a more
complicated setup, it is possible to have a machine that contains many logical
nodes, and because each of these nodes can take on different roles, the
machine lives in multiple tiers.</p></div>
<h4 id="_arbitrary_data_flows_and_custom_architectures">Arbitrary Data Flows and Custom Architectures</h4>
<div class="paragraph"><p>With tsinks and tsource, data can be sent through multiple nodes.  If ack
injection and ack checking decorators are properly inserted, you can achieve
reliability.</p></div>
<h3 id="_extending_via_sink_source_decorator_plugins">Extending via Sink/Source/Decorator Plugins</h3><div style="clear:left"></div>
<div class="paragraph"><p>An experimental plugin mechanism is provided that allows you to add new custom
sources, sinks, and decorators to the system.</p></div>
<div class="olist arabic"><div class="title">Two steps are required to use this feature.</div><ol class="arabic">
<li>
<p>
First, add the jar with the new plugin classes to flume&#8217;s classpath.
  If the plugin requires DLL&#8217;s/so&#8217;s make sure these are in the
  LD_LIBRARY_PATH (unix .so) or PATH (windows .dll)
</p>
</li>
<li>
<p>
Second, in <tt>flume-site.xml</tt>, add the class names of the new sources,
  sinks, and/or decorators to the <tt>flume.plugin.classes</tt> property.
  Multiple classes can be specified by comma separating the list. Java
  reflection is used to find some special static methods that add new
  components to the system and data flow language&#8217;s library.
</p>
</li>
</ol></div>
<div class="paragraph"><p>An example component has been "pluginified"&#8201;&#8212;&#8201;the "HelloWorld"
source, sink, and decorator. This plugin does something very simple;
the source generates the text "hello world!" every three seconds, the
sink writes events to a "helloworld.txt" text file, and the decorator
prepends "hello world!" to any event it encounters.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
cd into the <tt>plugins/helloworld</tt> directory and type <tt>ant</tt>, a <tt>helloworld_plugin.jar</tt> file will be generated
</p>
</li>
<li>
<p>
Add the following to flume-site.xml (create it if it doesn&#8217;t already
  exist)
  "helloworld.HelloWorldSink,helloworld.HelloWorldSource,helloworld.HelloWorldDecorator"
  to the <tt>flume.plugin.classes</tt> property in <tt>flume-site.xml</tt>.
</p>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">if you use the provided <tt>flume-site.xml.template</tt> file to
create your <tt>flume-site.xml</tt> be sure to comment out or remove any
example properties contained in the sample template.</td>
</tr></table>
</div>
<div class="listingblock">
<div class="title">Example flume-site.xml contents</div>
<div class="content"><!-- Generator: GNU source-highlight 3.1.3
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">&lt;configuration&gt;</span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">&lt;property&gt;</span></span>
    <span style="font-weight: bold"><span style="color: #0000FF">&lt;name&gt;</span></span>flume.plugin.classes<span style="font-weight: bold"><span style="color: #0000FF">&lt;/name&gt;</span></span>
    <span style="font-weight: bold"><span style="color: #0000FF">&lt;value&gt;</span></span>helloworld.HelloWorldSink,helloworld.HelloWorldSource,helloworld.HelloWorldDecorator<span style="font-weight: bold"><span style="color: #0000FF">&lt;/value&gt;</span></span>
    <span style="font-weight: bold"><span style="color: #0000FF">&lt;description&gt;</span></span>Comma separated list of plugins<span style="font-weight: bold"><span style="color: #0000FF">&lt;/description&gt;</span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">&lt;/property&gt;</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">&lt;/configuration&gt;</span></span></tt></pre></div></div>
</li>
<li>
<p>
Start the Flume master and at least one logical node in separate terminals
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
in each terminal cd into the top-level flume directory, should be just above <tt>plugins</tt>
</p>
</li>
<li>
<p>
Add <tt>helloworld_plugin.jar</tt> to the FLUME_CLASSPATH in <strong>both</strong> terminals
</p>
<div class="listingblock">
<div class="content">
<pre><tt>export FLUME_CLASSPATH=`pwd`/plugins/helloworld/helloworld_plugin.jar</tt></pre>
</div></div>
</li>
<li>
<p>
in terminal 1 run <tt>bin/flume master</tt>
</p>
</li>
<li>
<p>
in terminal 2 run <tt>bin/flume node -n hello1</tt>
</p>
</li>
</ol></div>
</li>
<li>
<p>
At this point the master and hello1 nodes should be started and will have loaded the plugin
</p>
<div class="listingblock">
<div class="title">You should see log output similar to the following in both master and hello1:</div>
<div class="content">
<pre><tt>10/07/29 17:35:28 INFO conf.SourceFactoryImpl: Found source builder helloWorldSource in helloworld.HelloWorldSource
10/07/29 17:35:28 INFO conf.SinkFactoryImpl: Found sink builder helloWorldSink in helloworld.HelloWorldSink
10/07/29 17:35:28 INFO conf.SinkFactoryImpl: Found sink decorator helloWorldDecorator in helloworld.HelloWorldDecorator</tt></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">Another way to verify that your plugin is loaded is to check if it is displayed on this page <a href="http://localhost:35871/masterext.jsp">http://localhost:35871/masterext.jsp</a></td>
</tr></table>
</div>
</li>
<li>
<p>
Configure hello1
</p>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">The easiest way to do this is open the configuration page of the master in a browser, typically this link <a href="http://localhost:35871/flumeconfig.jsp">http://localhost:35871/flumeconfig.jsp</a></td>
</tr></table>
</div>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
load the helloworld source/sink into our <tt>hello1</tt> node (the bottom text box, then submit button if you are using the master&#8217;s web interface"
</p>
<div class="listingblock">
<div class="content">
<pre><tt>hello1: helloWorldSource() | helloWorldSink();</tt></pre>
</div></div>
</li>
<li>
<p>
you could also try the hello world decorator
</p>
<div class="listingblock">
<div class="content">
<pre><tt>hello1: helloWorldSource() | { helloWorldDecorator() =&gt; helloWorldSink() };</tt></pre>
</div></div>
<div class="paragraph"><p>In either case <tt>hello1</tt> will output a <tt>helloworld.txt</tt> file into it&#8217;s current working directory. Every 3 seconds a new "hello world!" line will be output to the file.</p></div>
</li>
</ol></div>
</li>
</ol></div>
<h4 id="_semantics_of_flume_extensions">Semantics of Flume Extensions</h4>
<div class="paragraph"><p>Flume uses threading to support reconfiguration and multiple logical
nodes.  Sources, sinks, and decorator are extensions that can use
queues for buffering and can use clock sleeps for periodic rolls and
retries.  Because we use these blocking operations and because the
internal concurrency control mechanisms we use can cause deadlocks or
hangs, there are several rules that need to be followed and enforced
by test cases in order to be a well-behaved source, sink, or
decorator.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">This section is a draft and not exhaustive or completely formal.
More restrictions may be added to this in the future.</td>
</tr></table>
</div>
<div class="paragraph"><div class="title">Semantics  of sources</div><p>Sources must implement 4 methods</p></div>
<div class="ulist"><ul>
<li>
<p>
void open() throws IOException
</p>
</li>
<li>
<p>
Event next() throws IOException
</p>
</li>
<li>
<p>
void close() throws IOException
</p>
</li>
<li>
<p>
ReportEvent getReport()
</p>
</li>
</ul></div>
<div class="paragraph"><p>Along with these signatures, each of these methods can also throw
RuntimeExceptions.  These exceptions indicate a failure condition and
by default will make a logical node shutdown in ERROR state.  These
error messages are user visible and it is important that they have
actionable descriptions about the failure conditions without having to
dump stack.  Thus, NullPointerExceptions are not acceptable&#8201;&#8212;&#8201;they
are not descriptive or helpful without their stack traces.</p></div>
<div class="paragraph"><p>Some examples of valid runtime exceptions include invalid arguments
that prevent a source from opening and invalid state infractions
(attempting to next on a closed source).</p></div>
<h5 id="_simple_source_semantics">Simple source semantics.</h5>
<div class="imageblock">
<div class="content">
<img src="sourceStatesSimple.png" alt="sourceStatesSimple.png" />
</div>
</div>
<div class="paragraph"><p>Simple sources are assumed to be sources whose open and close
operation happen quickly and do not block for long periods of time.
The longest pauses tolerated here on the order of 10s (default time
for a failing DNS lookup).</p></div>
<div class="paragraph"><p>The constructor for the sources should be callable without grabbing
resources that can block or that require IO such as network connectors
or file handles.  If there are errors due to configuration settings
that can be caught in the constructor, an IllegalArgumentException
should be thrown.</p></div>
<div class="paragraph"><p><tt>open()</tt> is a call that grabs resources for the source so that the
<tt>next()</tt> call can be made.  The <tt>open</tt> call of a simple source should
attempt to fail fast. It can throw a IOException or a RuntimeException
such as IllegalStateException.  Open should only be called on a CLOSED
sink&#8201;&#8212;&#8201;if a sink is opened twice, the second call should throw an
IOException or IllegalStateException.</p></div>
<div class="paragraph"><p><tt>next()</tt> is a call that gets an event from an open source.  If a
source is not open, this should throw an IllegalStateException.  This
call can and will often block. If <tt>next()</tt> is blocking, a call to
<tt>close()</tt> should unblock the <tt>next()</tt> by having it exit cleanly
returning null.  Many resources such as TCP-sockets (and ideally rpc
frameworks) default to throwing an exception on blocked network reads
(like a <tt>next()</tt> call) when <tt>close()</tt>'d.</p></div>
<div class="paragraph"><p><tt>close()</tt> is a call that releases resources that the open call of a
source grabs.  The <tt>close()</tt> call itself blocks until all resources
are released.  This allows a subsequent <tt>open()</tt> in the same thread to
not fail due to resource contention (ex, closing a server socket on
port 12345 should not return until port 12345 is ready to be bound
again).</p></div>
<div class="paragraph"><p><tt>getReport()</tt> returns a ReportEvent.  These values should be available
regardless if the node is open or closed and this call should not
block the other source by other calls (due to potential lock inversion
issues).  The values retrieved are ideally atomically grabbed but this
is not required as long as no errors are caused by racy execution.</p></div>
<div class="paragraph"><p>If a source is opened or closed multiple times, it is up to the source to
determine if values are reset or persisted between open/close cycles.</p></div>
<h5 id="_buffered_source_semantics">Buffered source semantics</h5>
<div class="imageblock">
<div class="content">
<img src="sourceStatesBuffered.png" alt="sourceStatesBuffered.png" />
</div>
</div>
<div class="paragraph"><p>Some sources have in memory queues or buffers stored persistently on
disk.  The guiding principle here is that on <tt>close()</tt>, buffered
sources should prevent new data from entering and attempt to flush the
buffered data.  Also any subordinate threads should be released before
close returns.  If no progress is being made on <tt>close()</tt>, for a given
period of time (30s is the default currently) the controlling thread
will for an Thread.interrupt() call.  The source should be able to
handle InterruptedExceptions and percolate interrupted status up the
returning call stack.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">In v0.9.1, interruptions when caught should be handled by
re-flagging the Thread&#8217;s interrupted flag (a call to
<tt>Thread.currentThread().interrupt()</tt>) and then throwing an
IOException.  The API for extensions will likely change in the future
to throw either an IOException or an InterruptedException.</td>
</tr></table>
</div>
<div class="paragraph"><p><tt>open()</tt> no change.</p></div>
<div class="paragraph"><p><tt>close()</tt> This call is usually made from a separate thread than the
open() or next() calls.  Since this call should block until resources
are freed, it should attempt to flush its buffers before returning.
For example, so if a network source has some buffered data, the
network connection should be closed to prevent new data from entering,
and then the buffered data should be flushed.  The source should be
able to handle InterruptedExceptions and percolate interrupted status
up the returning call stack and indicate an error state.</p></div>
<div class="paragraph"><p>A <tt>next()</tt> call while in CLOSING state should continue pulling values
out of the buffer until it is empty.  This is especially true if the
next() call is happening in the main driver thread or a subordinate
driver thread.  One mechanism to get this effect is to add a special
DONE event to the queue/buffer that indicates a clean exit.</p></div>
<div class="paragraph"><p><tt>getReport()</tt> ideally includes metric information like the size of the
queues in the source, and the number of elements in the queue.</p></div>
<div class="paragraph"><div class="title">Semantics of sinks and decorators</div><p>Sinks and decorators must implement 4 methods</p></div>
<div class="ulist"><ul>
<li>
<p>
void open() throws IOException
</p>
</li>
<li>
<p>
void append(Event e) throws IOException
</p>
</li>
<li>
<p>
void close() throws IOException
</p>
</li>
<li>
<p>
ReportEvent getReport()
</p>
</li>
</ul></div>
<div class="paragraph"><p>Along with these signatures, each of these methods can also throw
RuntimeExceptions.  Run-time exceptions indicate a failure condition
and by default will make a logical node shutdown in ERROR state.
These error messages are user visible and it is important that they
have helpful descriptions about the failure conditions without having
to dump stack.  Thus, NullPointerExceptions are not acceptable&#8201;&#8212;&#8201;they
are not descriptive or helpful without their stack traces.</p></div>
<h5 id="_simple_sinks">Simple Sinks.</h5>
<div class="imageblock">
<div class="content">
<img src="sinkStatesSimple.png" alt="sinkStatesSimple.png" />
</div>
</div>
<div class="paragraph"><p>Simple sinks are assumed to have open, close, and append operations
that happen quickly and do not block for long periods of time.  The longest
pauses tolerated here on the order of 10s (default time for a failing
DNS lookup).</p></div>
<div class="paragraph"><p>The constructor for the sinks and decorators should be callable
without grabbing resources that can block or that require IO such as
network connectors or file handles.  If there are errors due to
configuration settings that can be caught in the constructor, an
IllegalArgumentException should be thrown.</p></div>
<div class="paragraph"><p><tt>open()</tt> is a call that grabs resources for the sink or decorator so
that the <tt>append(Event)</tt> call can be made.  If there are errors due to
configuration settings not detectable without IO in the constructor,
<tt>open()</tt> should attempt to fail fast and throw a IOException or
RuntimeException such as IllegalStateException or
IllegalArgumentException.  Open should only be called on a CLOSED sink&#8201;&#8212;&#8201;if a sink is opened twice, the second call should throw an
IOException or IllegalStateException.</p></div>
<div class="paragraph"><p><tt>append()</tt> is a call that delivers a event.  If a sink is not open,
this should throw an IllegalStateException.</p></div>
<div class="paragraph"><p>If a normal decorator fails to open or to append because of an
internal failure or a subsink fails to open, the decorator should
release its resources attempt to close the subsink and then throw an
exception.  There are some sink/decos that specifically manipulate
these semantics&#8201;&#8212;&#8201;this needs to be done with care.</p></div>
<div class="paragraph"><p><tt>close()</tt> is a call that releases resources that the open call of a
source grabs.  If <tt>open()</tt> or <tt>next()</tt> is blocking, a call to this
should unblock the call and have them exit.  <tt>close()</tt> should be
called called on an open sink, but we allow a closed sink to have
<tt>close()</tt> called on it without throwing an exception (generally LOG
warning however).</p></div>
<div class="paragraph"><p><tt>getReport()</tt> returns a ReportEvent.  These values should be available
regardless if the node is open or closed and this call should not
cause get blocked by other calls (due to potential lock inversion
issues).  The values retrieved are ideally atomically grabbed but this
is not required as long as no errors are caused by racy execution.  If
a sink is opened or closed multiple times, it is up to the sink to
determine if values are reset or persisted between open/close cycles.</p></div>
<h5 id="_buffered_sink_and_decorator_semantics">Buffered sink and decorator semantics</h5>
<div class="imageblock">
<div class="content">
<img src="sinkStatesBuffered.png" alt="sinkStatesBuffered.png" />
</div>
</div>
<div class="paragraph"><p>Some sinks have queues or buffers stored in memory or persistently on
disk.  The guiding principle here is that buffered sinks should
attempt to flush its buffers when prompted to <tt>close()</tt>.  This needs
to be balanced with the requirement that sinks and decorated sinks
should attempt to close in a relatively quick fashion.</p></div>
<div class="paragraph"><p><tt>open()</tt> Since this sink isn&#8217;t open this generally means there is no
buffered data.  However, an <tt>open()</tt> on a sink or decorator with
persistent data should attempt to recover data and enqueue it in the
<tt>open()</tt> call. Examples of these include a DFO or WAL log recovering
dfo/wla logs, when a network subsink is down.</p></div>
<div class="paragraph"><p>An <tt>append()</tt> call may buffer data before sending it (such as a
batching decorator).  A <tt>close()</tt> call, should attempt to append
buffered data to its (sub)sink before executing the close. Also, any
subordinate threads should be stopped before shutdown.</p></div>
<div class="paragraph"><p>If no progress is being made on <tt>close()</tt>, for a given period of time
(30s is the default currently) it will be interrupted and should
handle abruptly exiting because of this interruption.</p></div>
<h5 id="_retries_sleeps_and_unclean_exits">Retries, sleeps, and unclean exits.</h5>
<div class="imageblock">
<div class="content">
<img src="sinkStatesOpening.png" alt="sinkStatesOpening.png" />
</div>
</div>
<div class="paragraph"><p>Retry on open semantics</p></div>
<div class="imageblock">
<div class="content">
<img src="sinkStatesAppending.png" alt="sinkStatesAppending.png" />
</div>
</div>
<div class="paragraph"><p>Retry on Append semantics</p></div>
<div class="paragraph"><p>Some decorators introduce retries and sleeps. An author who uses these
needs to ensure that these operations behave well on open, append, and
close.  When combined with buffering sinks, flushing a buffer on close
may not be possible! (ex: wal trying to send data to a dead network
connection).  This means these sinks/decos need a way to exit abruptly
and report an error.  There are two operations that make these cases
 need to be handled: unbounded retries and unbounded/long
sleeps/await/blocking.</p></div>
<div class="paragraph"><p>Some decorators have potentially unbounded retry semantics.  For
example, InsistentOpen, InsistentAppend, and FailoverSinks have the
potential to retry <tt>open()</tt> and <tt>append()</tt> calls an unbounded number
of times.  These decorators can also be wrapped with in others&#8201;&#8212;&#8201;this
means we need to be able to percolate the hard exit and bypass the
retry logic.</p></div>
<div class="paragraph"><p>To do this we require that any sink/deco that has retry logic must
check for hard exit before retrying.  These sinks must to propagate
the hard exit interruption to its upstream decorators (in case they
have retry logic!).</p></div>
<div class="paragraph"><p>Some sinks have the potential to backoff or sleep for long or
potentially unbounded amounts of time.  Code using sleeps or
synchronization operations such as waiting on latches
(<tt>CountDownLatch.await()</tt>) or thread sleeps (<tt>Thread.sleep()</tt>) must
properly handle interruptions.  Since these yielding operations are
usually only used in retry operations (which meant there was a
failure), the sink/deco needs to propagate the interruption and fail
with error.</p></div>
<div class="paragraph"><p>There are some ramifications of these semantics.  Care must be taken
with locking open, close, and append operations.  If there are any
sleeps or blocking <tt>open()</tt> operations (ex: InsistentOpen,
FailoverSinks), ideally a close call will cause it to shutdown, it and
the open call should get unblocked.  <tt>append()</tt></p></div>
<div class="paragraph"><p>The sink signaled to be closed but blocked on <tt>append()</tt> or <tt>open()</tt>
should exit in a reasonable amount of time&#8201;&#8212;&#8201;ideally within a few
heartbeats (5s is the default, so ideally &lt;30s).  If the sink exits
and its buffers are empty, it should do a normal successful return.
If there were unflushed events, it should return error by throwing an
exception.  If there are subordinate threads, these should be
terminated before close returns.</p></div>
<h3 id="_limiting_data_transfer_rate_between_source_sink_pairs">Limiting Data Transfer Rate between Source-Sink pairs</h3><div style="clear:left"></div>
<div class="paragraph"><p>Flume has the ability to limit the rate of transfer of data between source-sink pairs.
This is useful to divide the network bandwidth non-uniformly among different
source-sink pairs based on the kind of logs they are transferring.
For example, you can ship some types of logs at a faster rate than others, or you can transfer logs at different rates at different times of the day.</p></div>
<div class="paragraph"><p>Another example of when it is beneficial to limit the rate of data transfer is when the network (or a collector) recovers after a failure.
In this case, the agents might have a lot of data backed up to ship; if there is no limit on the transfer rate,
the agents can exhaust all of the resources of the collector and possibly cause it to crash.</p></div>
<div class="paragraph"><p>In Flume, you can use a special sink-decorator called the <tt>choke</tt> decorator to limit the rate of transfer of data between source-sink pairs.
Each <tt>choke</tt> decorator (called a <tt>choke</tt>) has to be assigned to a <tt>choke-id</tt>.
Here is an example of using a <tt>choke</tt> between a source and sink of a node; the <tt>choke-id</tt> of this <tt>choke</tt> is "Cid".</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>node: source | { choke("Cid") =&gt; sink };</tt></pre>
</div></div>
<div class="paragraph"><p>The <tt>choke-ids</tt> are specific to a physical node. Before using a <tt>choke</tt> on <tt>node</tt>,
you must register the <tt>choke-id</tt> on the physical node containing the <tt>node</tt>.
You can register a <tt>choke-id</tt> on a physical node using the <tt>setChokeLimit</tt> command.
When registering a <tt>choke-id</tt>, you must also assign a rate limit (in KB/sec) to it.
Here is an example of registering a <tt>choke-id</tt> "Cid" on a physical node <tt>host</tt> and assigning a limit 1000 KB/sec.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>exec setChokeLimit host Cid 1000</tt></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">You can also use setChokeLimit at the run-time to change the limit assigned to a <tt>choke-id</tt>.</td>
</tr></table>
</div>
<div class="paragraph"><p>The limit on the <tt>choke-id</tt> specifies an upper bound on the rate at which the <tt>chokes</tt> using that <tt>choke-id</tt> can collectively transfer data.
In the preceding example, there is only one source-sink pair on the physical node <tt>host</tt> that uses a <tt>choke</tt> with <tt>choke-id</tt> "Cid". Consequently, the rate of data transfer between that source-sink pair is limited to 1000 KB/sec.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">For the purpose of rate limiting, only the size of the event body is taken into account.</td>
</tr></table>
</div>
<div class="paragraph"><p>The <tt>choke</tt> decorator works as follows: when <tt>append()</tt> is called on the sink to which the <tt>choke</tt>
is attached, the <tt>append()</tt> call works normally if the amount of data transferred
(during a small duration of time) is within the limit assigned to the <tt>choke-id</tt> corresponding to the <tt>choke</tt>.
If the limit has been exceeded, then <tt>append()</tt> is blocked for a small duration of time.</p></div>
<div class="paragraph"><p>Suppose there are multiple source-sink pairs using <tt>chokes</tt> between them, and they are using
the same <tt>choke-id</tt>.  Suppose further that both <tt>node1</tt> and <tt>node2</tt> are logical nodes on the same physical node <tt>host</tt>,
and a <tt>choke-id</tt> "Cid" with limit 1000KB/sec is registered on <tt>host</tt>.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>node1: source1 | { choke("Cid") =&gt; sink1 };
node2: source2 | { choke("Cid") =&gt; sink2 };</tt></pre>
</div></div>
<div class="paragraph"><p>In this example, because both <tt>source1-sink1</tt> and <tt>source2-sink2</tt> pairs are using <tt>chokes</tt> with the same <tt>choke-id</tt> "Cid", the total data going across these source-sink pairs collectively is limited to 1000KB/sec.
Flume does not control how this limit will be divided between the source-sink pairs,
but it does guarantee that neither source-sink pair will starve.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">If multiple source-sink pairs on the same physical node use chokes that have the same choke-id,
then there is no guarantee how the rate limit will be divided between these source-sink pairs.</td>
</tr></table>
</div>
</div>
<h2 id="_flume_and_hdfs_security_integration">Flume and HDFS Security Integration</h2>
<div class="sectionbody">
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">This section is only required if you are using a Kerberized HDFS
cluster.  If you are running CDH3b2 or a Hadoop version 0.21.x or
earlier, you can safely skip this section.</td>
</tr></table>
</div>
<div class="paragraph"><p>Flume&#8217;s datapath needs to be able to interact with "secured" Hadoop
and HDFS.  The Hadoop and HDFS designers have chosen to use the
Kerberos V5 system and protocols to authenticate communications
between clients and services.  Hadoop clients include users, MR jobs
on behalf of users, and services include HDFS, MapReduce.</p></div>
<div class="paragraph"><p>In this section we will describe how setup up a Flume node to be a
client as user <em>flume</em> to a kerberized HDFS service.  This section
will <strong>not</strong> talk about securing the communications between Flume nodes
and Flume masters, or the communications between Flume nodes in a
Flume flow.  The current implementation does not support writing
individual isolated flows as different users.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">This has only been tested with the security enhanced betas of
CDH (CDH3b3+), and the MIT Kerberos 5 implementation.</td>
</tr></table>
</div>
<h3 id="_basics">Basics</h3><div style="clear:left"></div>
<div class="paragraph"><p>Flume will act as a particular Kerberos principal (user) and needs
credentials.  The Kerberos credentials are needed in order to interact
with the kerberized service.</p></div>
<div class="paragraph"><p>There are two ways you can get credentials. The first is used by
interactive users because it requires an interactive logon.  The
second is generally used by services (like a Flume daemon) and uses a
specially protected key table file called a <em>keytab</em>.</p></div>
<div class="paragraph"><p>Interactively using the <tt>kinit</tt> program to contact the Kerberos KDC
(key distribution center) is one way is to prove your identity. This
approach requires a user to enter a password.  To do this you need a
two part principal setup in the KDC, which is generally of the form
<tt>user@REALM.COM</tt>.  Logging in via <tt>kinit</tt> will grant a ticket granting
ticket (TGT) which can be used to authenticate with other services.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">this user needs to have an account on the namenode machine as
well&#8201;&#8212;&#8201;Hadoop uses this user and group information from that machine
when authorizing access.</td>
</tr></table>
</div>
<div class="paragraph"><p>Authenticating a user or a service can alternately be done using a
specially protected <em>keytab</em> file.  This file contains a ticket
generating ticket (TGT) which is used to mutually authenticate the
client and the service via the Kerberos KDC.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The keytab approach is similar to an "password-less" ssh
connections.  In this case instead of an id_rsa private key file, the
service has a keytab entry with its private key.</td>
</tr></table>
</div>
<div class="paragraph"><p>Because a Flume node daemon is usually started unattended (via service
script), it needs to login using the keytab approach.  When using a
keytab, the Hadoop services requires a three part principal.  This has
the form <tt>user/host.com@REALM.COM</tt>.  We recommend using <tt>flume</tt> as the
user and the hostname of the machine as the service.  Assuming that
Kerberos and kerberized Hadoop has been properly setup, you just need
to a few parameters to the Flume node&#8217;s property file
(flume-site.xml).</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>&lt;property&gt;
&lt;name&gt;flume.kerberos.user&lt;/name&gt;
&lt;value&gt;flume/host1.com@REALM.COM &lt;/value&gt;
&lt;description&gt;&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
&lt;name&gt;flume.kerberos.keytab&lt;/name&gt;
&lt;value&gt;/etc/flume/conf/keytab.krb5 &lt;/value&gt;
&lt;description&gt;&lt;/description&gt;
&lt;/property&gt;</tt></pre>
</div></div>
<div class="paragraph"><p>In this case, <tt>flume</tt> is the user, <tt>host1.com</tt> is the service, and
<tt>REALM.COM</tt> is the Kerberos realm.  The <tt>/etc/keytab.krb5</tt> file contains
the keys necessary for <tt>flume/host1.com@REALM.COM</tt> to authenticate
with other services.</p></div>
<div class="paragraph"><p>Flume and Hadoop provides a simple keyword (_HOST) that gets expanded
to be the host name of the machine the service is running on.  This
allows you to have one flume-site.xml file with the same
flume.kerberos.user property on all of your machines.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>&lt;property&gt;
&lt;name&gt;flume.kerberos.user&lt;/name&gt;
&lt;value&gt;flume/_HOST@REALM.COM &lt;/value&gt;
&lt;description&gt;&lt;/description&gt;
&lt;/property&gt;</tt></pre>
</div></div>
<div class="paragraph"><p>You can test to see if your Flume node is properly setup by running
the following command.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>flume node_nowatch -1 -n dump -c 'dump: console |  collectorSink("hdfs://kerb-nn/user/flume/%Y%m%D-%H/","testkerb");'</tt></pre>
</div></div>
<div class="paragraph"><p>This should write data entered at the console to a kerberized HDFS
with a namenode named kerb-nn, into a <tt>/user/flume/YYmmDD-HH/</tt>
directory.</p></div>
<div class="paragraph"><p>If this fails, you many need to check to see if Flume&#8217;s Hadoop
settings (in core-site.xml and hdfs-site.xml) are using Hadoop&#8217;s
settings correctly.</p></div>
<h3 id="_setting_up_flume_users_on_kerberos">Setting up Flume users on Kerberos</h3><div style="clear:left"></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">These instructions are for MIT Kerb5.</td>
</tr></table>
</div>
<div class="paragraph"><p>There are several requirements to have a "properly setup" Kerberos<br />
HDFS + Flume.</p></div>
<div class="ulist"><ul>
<li>
<p>
Need to have a prinicipal for the Flume user on each machine.
</p>
</li>
<li>
<p>
Need to have a keytab that has keys for each principal on each machine.
</p>
</li>
</ul></div>
<div class="paragraph"><p>Much of this setup can be done by using the <tt>kadmin</tt> program, and
verified using the <tt>kinit</tt>, <tt>kdestroy</tt>, and <tt>klist</tt> programs.</p></div>
<h4 id="_administering_kerberos_principals">Administering Kerberos principals</h4>
<div class="paragraph"><p>First you need to have permissions to use the <tt>kadmin</tt> program and the
ability to add to principals to the KDCs.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ kadmin -p &lt;adminuser&gt; -w &lt;password&gt;</tt></pre>
</div></div>
<div class="paragraph"><p>If you entered this correctly, it will drop you do the kadmin prompt</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>kadmin:</tt></pre>
</div></div>
<div class="paragraph"><p>Here you can add a Flume principal to the KDC</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>kadmin: addprinc flume
WARNING: no policy specified for flume@REALM.COM; defaulting to no policy
Enter password for principal "flume@REALM.COM":
Re-enter password for principal "flume@REALM.COM":
Principal "flume@REALM.COM" created.
kadmin:</tt></pre>
</div></div>
<div class="paragraph"><p>You also need to add principals with hosts for each Flume node that
will directly write to HDFS.  Since you will be exporting the key to a
keytab file, you can use the -randkey option to generate a random key.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>kadmin: addprinc -randkey flume/host.com
WARNING: no policy specified for flume/host.com@REALM.COM; defaulting to no policy
Principal "flume/host.com@REALM.COM" created.
kadmin:</tt></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Hadoop&#8217;s Kerberos implementation requires a three part principal
name&#8201;&#8212;&#8201;user/host@REALM.COM.  As a user you usually only need the user
name, <a href="mailto:user@REALM.COM">user@REALM.COM</a>.</td>
</tr></table>
</div>
<div class="paragraph"><p>You can verify that the user has been added by using the <tt>kinit</tt>
program, and entering the password you selected.  Next you can verify
that you have your Ticket Granting Ticket (TGT) loaded.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ kinit flume/host.com
Password for flume/host.com@REALM.COM:
$ klist
Ticket cache: FILE:/tmp/krb5cc_1016
Default principal: flume/host.com@REALM

Valid starting     Expires            Service principal
09/02/10 18:59:38  09/03/10 18:59:38  krbtgt/REALM.COM@REALM.COM


Kerberos 4 ticket cache: /tmp/tkt1016
klist: You have no tickets cached
$</tt></pre>
</div></div>
<div class="paragraph"><p>You can ignore the Kerberos 4 info.  To "logout" you can use the
<tt>kdestroy</tt> command, and then verify that credentials are gone by
running <tt>klist</tt>.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ kdestroy
$ klist
klist: No credentials cache found (ticket cache FILE:/tmp/krb5cc_1016)


Kerberos 4 ticket cache: /tmp/tkt1016
klist: You have no tickets cached
$</tt></pre>
</div></div>
<div class="paragraph"><p>Next to enable automatic logins, we can create a keytab file so that
does not require manually entering a password.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">This keytab file contains secret credentials that should be
protected so that only the proper user can read the file.  After
created, it should be in 0400 mode (-r--------) and owned by the user
running the Flume process.</td>
</tr></table>
</div>
<div class="paragraph"><p>Then you can generate a keytab file (int this example called
<tt>flume.keytab</tt>) and add a user <tt>flume/host.com</tt> to it.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>kadmin: ktadd -k flume.keytab flume/host.com</tt></pre>
</div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">This will invalidate the ability for flume/host.com to manually
login of the account.  You could however have a Flume user does not
use a keytab and that could log in.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content"><tt>ktadd</tt> can add keytab entries for mulitple principals into a
single file and allow for a single keytab file with many keys.  This
however weakens the security stance and may make revoking credentials
from misbehaving machines difficult.  Please consult with your
security administrator when assessing this risk.</td>
</tr></table>
</div>
<div class="paragraph"><p>You can verify the names and the version (KVNO) of the keys by running
the following command.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ klist -Kk flume.keytab
Keytab name: FILE:flume.keytab
KVNO Principal
---- --------------------------------------------------------------------------
   5 flume/host.com@REALM.COM (0xaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa)
   5 flume/host.com@REALM.COM (0xbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb)
   5 flume/host.com@REALM.COM (0xcccccccccccccccc)
   5 flume/host.com@REALM.COM (0xdddddddddddddddd)</tt></pre>
</div></div>
<div class="paragraph"><p>You should see a few entries and your corresponding keys in hex after
your principal names.</p></div>
<div class="paragraph"><p>Finally, you can use <tt>kinit</tt> with the <tt>flume@REALM.COM</tt> principal to
interactively do a Kerberos login and use the Hadoop commands to browse HDFS.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>$ kinit flume
Password for flume@REALM.COM:  &lt;-- enter password
$ hadoop dfs -ls /user/flume/</tt></pre>
</div></div>
</div>
<h2 id="_appendix">Appendix</h2>
<div class="sectionbody">
<h3 id="_flume_source_catalog">Flume Source Catalog</h3><div style="clear:left"></div>
<div class="paragraph"><div class="title">Flume&#8217;s Tiered Event Sources</div><p>These sources and sinks are actually translated from their original
specification into compositions of more specific lower level configurations.
They generally have reasonable default arguments assigned by the properties
xml file or by the Master.  The defaults can be overridden by the users.</p></div>
<div class="hdlist"><table>
<tr>
<td class="hdlist1">
<tt>collectorSource[(<em>port</em>)]</tt>
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Collector source. Listens for data from
agentSinks forwarding to port <tt><em>port</em></tt>.  If port is not specified, the
node default collector TCP port, 35863.  This source registers itself
at the Master so that its failover chains can automatically be determined.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>autoCollectorSource</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Auto collector source. Creates a logical collector
that, when assigned to a physical node, will be included in the list of
collectors in a failover chain. This is the collector counterpart to
auto*Chain() sinks. See the section Automatic Failover Chains for additional
information.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>logicalSource</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Logical Source. This source has a port assigned to it by
the Master and listens for rpcSink formatted data.
</p>
</td>
</tr>
</table></div>
<div class="paragraph"><div class="title">Flume&#8217;s Basic Sources</div><p>These sources are untranslated and generally need all of their arguments.</p></div>
<div class="hdlist"><table>
<tr>
<td class="hdlist1">
<tt>null</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Null source. Opens, closes, and returns null (last record) on next().
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>console</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Stdin console source.  This is for inputting events as an
interactive user and provides features such as edit history and
keyboard edit shortcuts.  A flume node must be started with the <tt>flume
node_nowatch</tt>&#8201;&#8212;&#8201;the watchdog does not allow console input.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>stdin</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Stdin source.  This is for piping data into a flume node&#8217;s
standard input data source.  A flume node must be started with the
<tt>flume node_nowatch</tt>&#8201;&#8212;&#8201;the watchdog does not allow console input.
WARNING: although this can be used as an interactive console, it will
hang a flume node until a newline is entered.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>rpcSource(<em>port</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
A remote procedure call (RPC) server that is
configured to listen on TCP port <tt><em>port</em></tt>. Supports both Apache-Thrift and
Apache-Avro RPC framework.  The type of RPC framework is specified by <tt>event.rpc.type</tt>
property (THRIFT or AVRO), the default is THRIFT.  Note that same RPC framework is used for rpcSink.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>text("<em>filename</em>")</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
One-time text file source.  One event per <tt>\n</tt>
delimited line.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>tail("<em>filename</em>"[, <em>startFromEnd</em>=false])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Similar to Unix&#8217;s tail
utility. One line is one event. Generates events for the entire file
then stays open for more data, and follows filename.  (e.g. if tailing
file "foo" and then "foo" is moved to "bar" and a new file appears
named "foo", it will finish reading the new "bar" file and then start
from the beginning of "foo"). If the <tt>startFromEnd</tt> parameter is
false, tail will re-read from the beginning of the file.  If it is
true, it will only start reading from the current end of file.  If the
last line of a file does not end with a newline character (<em>\n</em>), the
<tt>tail</tt> source will only send an event with this last line when the
<tt>tail</tt> is closed.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>multitail("<em>filename</em>"[, <em>file2</em> [,<em>file3</em> &#8230; ] ])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Like <tt>tail</tt> but can
follow multiple files concurrently.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>tailDir("<em>dirname</em>"[, fileregex=".*"])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Tails all files in a
directory <tt>dirname</tt> that match the specified <tt>fileregex</tt>.  Be careful
and make sure because the regex argument requires java style escaping
of <em>\</em> and <em>\"</em>.  For example <em>\w+</em> would have to be written as
"\\w+".  If a new file appears, it is added to the list of files to
tail.  If pointed at a new directory, it will attempt to read all
files that match!
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>seqfile("<em>filename</em>")</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Read from a Hadoop sequence file formatted file,
with <tt>com.cloudera.flume.handlers.hdfs.WriteableEventKey</tt> and
<tt>com.cloudera.flume.handlers.hdfs.WriteableEvent</tt> values. Conveniently, this
source can read files generated by the seqfile sink.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>syslogUdp(<em>port</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Syslog over UDP <tt><em>port</em></tt>.  This is syslog compatible.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>syslogTcp(<em>port</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Syslog over TCP <tt><em>port</em></tt>. This is syslog-ng compatible.
This is a server that can listen and receive on many concurrent connections.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>syslogTcp1(<em>port</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Syslog over TCP <tt><em>port</em></tt>. This is syslog-ng
compatible.  This is only available for a single connection and then shuts
down afterwards.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>execPeriodic("<em>cmdline</em>", <em>ms</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Execute an arbitrary program
specified by <tt><em>cmdline</em></tt>.  The entire output of the execution becomes
the body of generated messages.  <tt><em>ms</em></tt> specifies the number of
milliseconds to wait before the next execution (and next
event). Ideally the program is short lived.  This does not process
shell pipes or redirection operations&#8201;&#8212;&#8201;for these write a script and
use the script as the <tt><em>cmdline</em></tt> argument.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>execStream("<em>cmdline</em>")</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Execute an arbitrary program specified by
<tt><em>cmdline</em></tt>.  Each line outputted will become a new event.  Ideally
the program is long lived.  This does not process shell pipes or
redirection operations&#8201;&#8212;&#8201;for these write a script and use the script
as the <tt><em>cmdline</em></tt> argument.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>exec("<em>cmdline</em>"[, <em>aggregate</em>=false[, <em>restart</em>=false[,<em>period</em>=0]]])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Execute an arbitrary program specified by <tt><em>cmdline</em></tt>.  If the <tt><em>aggregate</em></tt>
argument is <tt>true</tt> entire program output is considered an event; otherwise,
each line is considered a new event.  If the <tt><em>restart</em></tt> argument is <tt>true</tt>,
then the program is restarted after it exits after waiting for <tt><em>period</em></tt>
milliseconds. <tt>execStream("foo")</tt> is equivalent to <tt>exec("foo", false, false,
0)</tt>. <tt>execPeriodic("foo", 1000)</tt> is equivalent to <tt>exec("foo", true, true,
1000)</tt>
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>synth(<em>msgCount</em>,<em>msgSize</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
A source that synthetically generates
<tt><em>msgCount</em></tt> random messages of size <tt><em>msgSize</em></tt>.  This will generate non
printable characters.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>synthrndsize(<em>msgCount</em>,<em>minSize</em>,<em>maxSize</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
A source that synthetically
generates <tt><em>msgCount</em></tt> random messages of size between randomly <em>minSize</em> and <em>maxSize</em>.
This will generate non printable characters.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>nonlsynth(<em>msgCount</em>,<em>msgSize</em>)</tt>
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
A source that synthetically generates
<tt><em>msgCount</em></tt> random messages of size <tt><em>msgSize</em></tt>.  This converts all <tt>'\n'</tt>
chars into <tt>' '</tt> chars.  This will generate non-printable characters but since
all randomly generated <em>\n</em> are converted, sources dependent on <em>\n</em> as a
record separator can get uniformly sized data.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>asciisynth(<em>msgCount</em>,<em>msgSize</em>)</tt>
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
A source that synthetically generates
<tt><em>msgCount</em></tt> random messages of size <tt><em>msgSize</em></tt>.  This converts all <tt>'\n'</tt>
chars into <tt>' '</tt> chars, and all non ASCII characters into printable ASCII
characters.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>twitter("<em>username</em>","<em>pw</em>"[,"<em>url</em>"])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
(Unsupported) A source that
collects data from a twitter "spritzer" stream.  <tt><em>username</em></tt> is a twitter
username, <tt><em>pw</em></tt> is the password for the user, and <tt><em>url</em></tt> is the url for the
feed.  If not specified, <tt>http://stream.twitter.com/1/statuses/sample.json</tt> is
used by default the <tt><em>url</em></tt>. See <a href="http://apiwiki.twitter.com/Streaming-API">http://apiwiki.twitter.com/Streaming-API</a>-
Documentation for more details.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>irc("<em>server</em>",<em>port</em>, "<em>nick</em>","<em>chan</em>")</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
(Unsupported) An IRC channel
source.  Each line sent to the channel is a new event.  It attempts to connect
to <tt><em>server</em></tt> on TCP port <tt><em>port</em></tt> (standard is 6667).  When it connects it
attempts to take the nickname <tt><em>nick</em></tt>, and enter channel <tt><em>chan</em></tt> (like
<tt>#hadoop</tt>   ).
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>scribe[(+<em>port</em></tt>)] 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
A scribe source.  This provides a network socket that
is compatible with data generated by Facebook&#8217;s Scribe collection system.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>report[(periodMillis)]</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
This source polls the local physical node for its
report every <em>periodMillis</em> milliseconds and turns it into a new event.  The
attribute names seen from the node report page are present, and the values are
uninterpreted arrays of bytes.
</p>
</td>
</tr>
</table></div>
<h3 id="_flume_sinks_catalog">Flume Sinks Catalog</h3><div style="clear:left"></div>
<div class="hdlist"><div class="title">Flume&#8217;s Collector Tier Event Sinks</div><table>
<tr>
<td class="hdlist1">
<tt>collectorSink("<em>fsdir</em>","<em>fsfileprefix</em>", <em>rollmillis</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Collector sink.
<tt><em>fsdir</em></tt> is a fs directory URI such as <tt>hdfs://namenode/path</tt> or <tt>file:///
path</tt>.  <tt><em>fsfileprefix</em></tt> is a file name prefix for outputted files. Both of
these can use escape sequences documented to bucket data as documented in the
<strong>Output Bucketing</strong> section. <tt><em>rollmillis</em></tt> is the number of milliseconds
between when a HDFS file should be rolled (opened and closed).  The format for
data outputted by collectors is specified by the
<tt>flume.collector.output.format</tt> property.
</p>
</td>
</tr>
</table></div>
<div class="hdlist"><div class="title">Flume&#8217;s Agent Tier Event Sinks</div><table>
<tr>
<td class="hdlist1">
<tt>agentSink[("<em>machine</em>"[, <em>port</em>])]</tt>  
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Defaults to <tt>agentE2ESink</tt>
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>agentE2ESink[("<em>machine</em>"[, <em>port</em>])]</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Agent sink with write ahead log and
end-to-end ack.  Optional arguments specify a <tt><em>machine</em></tt>, and the TCP
<tt><em>port</em></tt> pointing to a <tt>collectorSource</tt>.  If none is specified, the values
specified by the <tt>flume.collector.event.host</tt> and the <tt>flume.collector.port</tt>
properties will be used.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>agentDFOSink[("<em>machine</em>"[, <em>port</em>])]</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
DiskFailover Agent sink that stores
to local disk on detected failure. This sink periodically checks with the
<tt><em>machine:port</em></tt> and resends events if becomes alive again. Optional arguments
specify a <tt><em>machine</em></tt>, and the TCP <tt><em>port</em></tt> pointing to a <tt>collectorSource</tt>.
If none is specified, the values specified by the <tt>flume.collector.event.host</tt>
and the <tt>flume.collector.port</tt> properties will be used.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>agentBESink[("<em>machine</em>"[, <em>port</em>])]</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
BestEffort Agent sink.  This drops
messages on failures and continues sending.  Optional arguments specify a
<tt><em>collector</em></tt>, and the TCP <tt><em>PORT</em></tt> pointing to a <tt>collectorSource</tt>.  If none
is specified, the values specified by the <tt>flume.collector.event.host</tt> and the
<tt>flume.collector.port</tt> properties will be used.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>agentE2EChain("<em>m1</em>[:_p1_]"[, "<em>m2</em>[:_p2_]"[,&#8230;]])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Agent sink
with write-ahead log and end-to-end ack and collector failover
chains. <tt><em>m1:p1</em></tt> specifies a machine and optional port of the primary
default collector.  If all failovers are exhausted due to failures,
and since data is already durable locally, it will back off attempts
to send down stream.  Optional arguments specify a list of failover
machine:port pairs in a ranked order.  If a primary collector is not
responding, the backups are used.  The primary collectors are checked
periodically to see if they have come back up.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>agentDFOChain("<em>m1</em>[:_p1_]"[, "<em>m2</em>[:_p2_]"[,&#8230;]])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
DiskFailover
Agent sink that first attempts to fail over to other
collectors. <tt><em>m1:p1</em></tt> specifies a machine and optional port of the
primary default collector.  If all failovers are exhausted due to
failures, it will store to local disk.  Optional arguments specify a
list of failover machine:port pairs in a ranked order.  If a primary
collector is not responding, the backups are used.  The primary
collectors are checked periodically to see if they have come back up.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>agentBEChain("<em>m1</em>[:_p1_]"[, "<em>m2</em>[:_p2_]"[,&#8230;]])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
BestEffort
Agent sink with collector failover chains.  <tt><em>m1:p1</em></tt> specifies a
machine and optional port of the primary default collector.  If all
failovers are exhausted due to failures, this drops messages. Optional
arguments specify a <tt><em>collector</em></tt>, and the TCP <tt><em>port</em></tt> of the
collector.  If none is specified, the values specified by the
<tt>flume.collector.event.host</tt> and the <tt>flume.collector.port</tt> properties
will be used.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>autoE2EChain</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
This sink is an <tt>agentE2EChain</tt> that has failover nodes
populated automatically by the master.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>autoDFOChain</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
This sink is an <tt>agentDFOChain</tt> that has failover nodes
populated automatically by the master.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>autoBEChain</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
This sink is an <tt>agentBEChain</tt> that has failover nodes
populated automatically by the master.
</p>
</td>
</tr>
</table></div>
<div class="dlist"><div class="title">Flume&#8217;s Logical Sinks</div><dl>
<dt class="hdlist1">
<tt>logicalSink("<em>logicalnode</em>")</tt> 
</dt>
<dd>
<p>
This sink creates an rpcSink that is
assigned a host and IP based on the name of a logical node.  This information
is maintained and automatically selected by the master.
</p>
</dd>
</dl></div>
<div class="hdlist"><div class="title">Flume&#8217;s Basic Sinks</div><table>
<tr>
<td class="hdlist1">
<tt>null</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Null sink. Events are dropped
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>console[("<em>formatter</em>")]</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Console sink.  Display events to process&#8217;s
stdout using the optionally specified output formatter.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>text("<em>txtfile</em>"[,"<em>formatter</em>"])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Textfile sink.  Write to text file
txtfile, using an optionally specified <tt><em>formatter</em></tt>.  If a file already
exists, this sink will attempt to overwrite it.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>seqfile("filename")</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Seqfile sink.  Write to a Hadoop sequence file
formatted file, with <tt>com.cloudera.flume.handlers.hdfs.WriteableEventKey</tt> keys
and <tt>com.cloudera.flume.handlers.hdfs.WriteableEvent</tt> values.  If a file
already exists, this sink will attempt to overwrite it.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>dfs("<em>hdfspath</em>")</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Hadoop dfs seqfile sink.  Write to a dfs path
in Flume- specific Hadoop seqfile record format. The <tt><em>hdfspath</em></tt> can
use escape sequences to bucket data as documented in the <strong>Output
Bucketing</strong> section.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>customdfs("<em>hdfspath</em>"[, "<em>format</em>"])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Hadoop dfs formatted file
sink.  The <em>hdfspath</em> string is <strong>not</strong> escaped. The output format of
writes to a dfs path in using specified output <em>format</em>.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
+escapedCustomDfs("<em>hdfspath</em>", "<em>file</em>", "<em>format</em>") 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Hadoop dfs formatted
file sink.  The <em>hdfspath</em> string is escaped and events will get written to
particular directories and filenames based on this string. The output format
of writes to a dfs path in using specified output <em>format</em>. The <tt><em>hdfspath</em></tt>
can use escape sequences documented to bucket data as documented in the
<strong>Output Bucketing</strong> section.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>rpcSink("<em>host</em>"[, <em>port</em>])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
A remote procedure call (RPC) sink that is
configured to send to machine <tt><em>host</em></tt> on TCP port <tt><em>port</em></tt>. Default port is 35861
and can be overridden by setting the <tt>flume.collector.event.port</tt> property.
Supports both Apache-Thrift and Apache-Avro RPC framework.
The type of RPC framework is specified by <tt>event.rpc.type</tt> property (THRIFT or AVRO),
the default is THRIFT.  Note that same RPC framework is used for rpcSource.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>syslogTcp("<em>host</em>"[,<em>port</em>])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Syslog TCP sink.  Write to host "host" on
port "port" in syslog over TCP format (syslog-ng compatible). Default port is
TCP 514.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>irc("<em>host</em>",<em>port</em>, "<em>nick</em>", "<em>chan</em>")</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
(Unsupported) An IRC channel
sink.  Each event is sent to the channel as a line.  It attempts to connect to
<tt><em>server</em></tt> on TCP port <tt><em>port</em></tt>.  When it connects it attempts to take the
nickname <tt><em>nick</em></tt>, and enter channel <tt><em>chan</em></tt> (like <tt>#hadoop</tt>).
</p>
</td>
</tr>
</table></div>
<h3 id="_flume_sink_decorator_catalog">Flume Sink Decorator Catalog</h3><div style="clear:left"></div>
<div class="hdlist"><div class="title">Flume&#8217;s Sink Decorators</div><table>
<tr>
<td class="hdlist1">
<tt>nullDeco</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
This is a decorator that just passes data through to its child sink.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>writeAhead(&#8230;)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Write-ahead decorator.  Provides durability by writing
events to disk before sending them.  This can be used as a buffering mechanism&#8201;&#8212;&#8201;receive and send are decoupled in different threads.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>ackedWriteAhead[(<em>maxmillis</em>)]</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Write-ahead decorator that adds
acknowledgement tags and checksums to events.  Provides durability by writing
events to disk before sending them.  This can be used as a buffering mechanism&#8201;&#8212;&#8201;receive and send are decoupled in different threads.  This generates and
tracks groups of events, and also notifies other components to check for
acknowledgements.  These checks for retries are done where there is an
exponential backoff that can top out at <em>maxmillis</em> milliseconds.  The default
value for <em>maxmillis</em> is <tt>flume.agent.logdir.maxage</tt> property.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>diskFailover[(<em>maxmillis</em>)]</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Disk failover decorator.  Events that enter
this decorator are sent to its sub sink.  In the event of a down stream error,
data is written to disk, and periodically these disk-buffered events are
retried.  These checks for retries are done where there is an exponential
backoff that can top out at <em>maxmillis</em> milliseconds.  The default value for
<em>maxmillis</em> is <tt>flume.agent.logdir.maxage</tt> property.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>ackInjector</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
This decorator injects an extra ack group start message on
open, tags appended events with an ack tag, and injects an extra ack group end
message.  These tags contain a checksum, for all the bodies of the events that
pass through the <tt>ackInjector</tt>.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>ackChecker</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
This decorator tracks ack group start, end, and checksum
values inserted by <tt>ackInjector</tt>.  If a group has arrived and its checksum is
correct, it sends notifications to other components.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>lazyOpen</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
This decorator tracks open/closed state of the sub sink but does
not actually open the sink until an append is called.  Thus if a this
decorator is opened and closed without any appends, the sub sink is never
opened.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>insistentOpen[(<em>max</em>[<em>init</em>[,<em>cumulativeMax</em>]],)]</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
An insistent open
attempts to open its subsink multiple times until it succeeds with the
specified backoff properties.  This is useful for starting a network client up
when the network server may not yet be up.  When an attempt to open the
subsink fails, this exponentially backs off and then retries the open.  <em>max</em>
is the max number of millis per backoff (default is Integer.MAX_VALUE).
<em>init</em> is the initial number of millis to back off on the first encountered
failure (default 1000). <em>cumulativeMax</em> is the maximum backoff allowed from a
single failure before an exception is forwarded (default is
Integer.MAX_VALUE).  Note that this synchronously blocks the open call until
the open succeeds or fails after <em>cumulativeMax</em> millis.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>stubbornAppend</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
A stubborn append normally passes through append
operations to its subsink.  It catches the first exception that a subsink&#8217;s
append method triggers, and then closes, opens, and reappends to the subsink.
If this second attempt fails, it throws an exception. This is useful in
conjunction with network sinks where connections can be broken.  The open/
close retry attempt is often sufficient to re-establish the connection.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>value("<em>attr</em>","<em>value</em>")</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The value decorator adds a new metadata
attribute <em>attr</em> with the value <em>value</em>.  Agents can mark their data with
specific tags for later demultiplexing.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>mask("<em>attr1</em>"[,"<em>attr2</em>", &#8230;])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The mask decorator outputs inputted
events that are modified so that all metadata <strong>except</strong> the attributes
specified pass through.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>select("<em>attr1</em>"[,"<em>attr2</em>", &#8230;])</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The select decorator outputs inputted
events that are modified so that <strong>only</strong> the metadata attributes specified pass
through.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>format("<em>pattern</em>")</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The format decorator outputs inputted events that are
modified so that their bodies are replaced with an escaped version of the
<tt><em>pattern</em></tt> argument.  Since checksums rely on body data, this should only be
used on unreliable flows or reporting flows. Inappropriate use may result in
message loss.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>regex("<em>regex</em>",idx,"<em>attr</em>")</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The regex decorator applies the regular
expression <em>regex</em>, extracts the <em>idx</em> th capture group, and writes this value
to the <em>attr</em> attribute.  The regex must use java-style escaping.  Thus a
regexs that want to use the <tt>\d</tt> macro need to be specified as "\\d".
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>split("<em>regex</em>",idx,"<em>attr</em>")</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The split decorator uses the
regular expression <em>regex</em> to split the body into tokens (not
including the splitter value). The <em>idx</em> is then written as the value
to the <em>attr</em> attribute.  The regex must use java-style escaping.
Thus, a regex that wants to use the <tt>\d</tt> macro must be specified as
"\\d".
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>batch(<em>n</em>,<em>maxlatency</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
buffers <em>n</em> events and then sends one
aggregate event.  If <em>maxlatency</em> millis have passed, all current
buffered events are sent out as an aggregate event.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>unbatch</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Unbatch takes an aggregate event generated by batch, splits it,
and then forwards its original events.  If an event is not an aggregate it is
just forwarded.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>gzip</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
gzips a serialized event.  This is useful when used in conjunction
with aggregate events.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>gunzip</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
gunzip&#8217;s a gzip&#8217;ed event.  If the event is not a gzip event, it is
just forwarded.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>intervalSampler(<em>n</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Interval sampler. Every <tt><em>n</em></tt> th event gets
forwarded.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>probSampler(<em>p</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Probability sampler. Every event has a probability <em>p</em>
(where 0.0 &le; <em>p</em> &le; 1.0) chance of being forwarded.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>reservoirSampler(<em>k</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Reservoir sampler.  When flushed, at most <em>k</em>
events are forwarded.  If more than <em>k</em> elements have entered this decorator,
exactly <em>k</em> events are forwarded.  All events that pass through have the same
probability of being selected.  NOTE: This will reorder the events being sent.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>delay(<em>ms</em>)</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
adds a <em>ms</em> millisecond delay before forwarding events down
the pipeline.  This blocks and prevents other events from entering the
pipeline.  This is useful for workload simulation in conjunction with
<tt>asciisynth</tt> sources.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>choke[(<em>choke-id</em>)]</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
Limits the transfer rate of data going into the sink.
The <tt>choke-id</tt> should have been registered on the physical node where this decorator is being created using the <tt>setChokeLimit</tt> command.
Refer to <strong>Limiting Data Transfer Rate between Source-Sink pairs</strong> section for more details.
</p>
</td>
</tr>
</table></div>
<h3 id="_flume_environment_variables">Flume Environment Variables</h3><div style="clear:left"></div>
<div class="paragraph"><p>This section describes several environment variables that affect how
Flume operates.  The <tt>flume</tt> script in <tt>./bin</tt> uses these system
environment variables.  Many of these variables are set by the
<tt>flume-daemon.sh</tt> script used when flume is run as a daemon.</p></div>
<div class="hdlist"><table>
<tr>
<td class="hdlist1">
<tt>FLUME_PID_DIR</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The directory where a flume node or flume
master will drop pid files corresponding to the daemon process.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>FLUME_CLASSPATH</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The custom java classpath environment variable
additions you want flume to run with.  These values are prepended to
the normal Flume generated CLASSPATH.  WARNING: The <tt>flume</tt> script
overwrites the standard CLASSPATH when it is executed.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>FLUME_LOG_DIR</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The directory where debugging logs
generated by the flume node or flume master are written.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>FLUME_LOGFILE</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
This sets the suffix for the logfiles generated by
flume node or flume master.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>FLUME_ROOT_LOGGER</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The log4j logging setting for the executing
command.  By default it is "INFO,console".
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>ZOOKEEPER_ROOT_LOGGER</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The log4j logging setting for a master&#8217;s
embedded zookeeper server&#8217;s logs.  By default it is "ERROR,console".
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>WATCHDOOG_ROOT_LOGGER</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The log4j logging setting for the logging
the watchdog that wraps flume nodes and flume masters generates.  By
default it is "INFO,console".
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>FLUME_CONF_DIR</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The directory where the <tt>flume-site.xml</tt> and
<tt>flume-conf.xml</tt> files flume node and flume master will use reside.
This defaults to <tt>./conf</tt> if a <tt>./conf/flume-conf.xml</tt> file is found,
or to <tt>/etc/flume/conf/</tt> if it <tt>flume-conf.xml</tt> is found there.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>HADOOP_HOME</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
The directory where Hadoop jars are expected to be
found.  If not specified it will use jars found in <tt>/usr/lib/hadoop</tt>
or <tt>./lib/</tt>.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>FLUME_DEVMODE</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
If this value is set to "true" the <tt>./libbuild</tt>
jars which include ant jars required to compile JSP servlets will be
included in the CLASSPATH.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>FLUME_VERBOSE</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
If it this is toggled, the flume script will print
out the command line being executed.
</p>
</td>
</tr>
<tr>
<td class="hdlist1">
<tt>FLUME_VERBOSE_JAVA</tt> 
<br />
</td>
<td class="hdlist2">
<p style="margin-top: 0;">
If this is toggled along with FLUME_VERBOSE,
the "-verbose" flag will be passed to the JVM running flume.
</p>
</td>
</tr>
</table></div>
<h3 id="_flume_site_xml_configuration_settings">flume-site.xml configuration settings</h3><div style="clear:left"></div>
<h3 id="_troubleshooting">Troubleshooting</h3><div style="clear:left"></div>
<h4 id="_what_are_the_default_ports">What are the default ports?</h4>
<div class="paragraph"><p>TCP ports are used in all situations.</p></div>
<div class="tableblock">
<table rules="all"
width="40%"
frame="border"
cellspacing="0" cellpadding="4">
<col width="33%" />
<col width="33%" />
<col width="33%" />
<tbody>
<tr>
<td align="left" valign="top"><p class="table">node collector port</p></td>
<td align="left" valign="top"><p class="table"><tt>flume.collector.port</tt></p></td>
<td align="left" valign="top"><p class="table">35853+</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">node status web server</p></td>
<td align="left" valign="top"><p class="table"><tt>flume.node.http.port</tt></p></td>
<td align="left" valign="top"><p class="table">35862+</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">master status web server</p></td>
<td align="left" valign="top"><p class="table"><tt>flume.master.http.port</tt></p></td>
<td align="left" valign="top"><p class="table">35871</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">master heartbeat port</p></td>
<td align="left" valign="top"><p class="table"><tt>flume.master.heartbeat.port</tt></p></td>
<td align="left" valign="top"><p class="table">35872</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">master admin/shell port</p></td>
<td align="left" valign="top"><p class="table"><tt>flume.master.admin.port</tt></p></td>
<td align="left" valign="top"><p class="table">35873</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">master gossip port</p></td>
<td align="left" valign="top"><p class="table"><tt>flume.master.gossip.port</tt></p></td>
<td align="left" valign="top"><p class="table">35890</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">master &#8594; zk port</p></td>
<td align="left" valign="top"><p class="table"><tt>flume.master.zk.client.port</tt></p></td>
<td align="left" valign="top"><p class="table">3181</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">zk &#8594; zk quorum port</p></td>
<td align="left" valign="top"><p class="table"><tt>flume.master.zk.server.quorum.port</tt></p></td>
<td align="left" valign="top"><p class="table">3182</p></td>
</tr>
<tr>
<td align="left" valign="top"><p class="table">zk &#8594; zk election port</p></td>
<td align="left" valign="top"><p class="table"><tt>flume.master.zk.server.election.port</tt></p></td>
<td align="left" valign="top"><p class="table">3183</p></td>
</tr>
</tbody>
</table>
</div>
<h4 id="_what_versions_of_hadoop_hdfs_can_i_use_how_do_i_change_this">What versions of Hadoop HDFS can I use?  How do I change this?</h4>
<div class="paragraph"><p>Currently, there are constraints writing to HDFS.  A Flume node
can only write to one version of Hadoop.  Although Hadoop&#8217;s HDFS API has been
fairly stable, HDFS clients are only guaranteed to be wire compatible with the
same major version of HDFS.  Cloudera&#8217;s testing used Hadoop HDFS 0.20.x and HDFS
0.18.x.  They are API compatible so all that is necessary to switch versions
is to swap out the Hadoop jar and restart the node that will write to the
other Hadoop version.</p></div>
<div class="paragraph"><p>You still need to configure this instance of Hadoop so that it talks to the
correct HDFS nameNode.  You configure the Hadoop client settings (such as
pointers to the name node) the same way as with Hadoop dataNodes or worker
nodes&#8201;&#8212;&#8201;modify and use <tt>conf/core-site.xml</tt>.</p></div>
<div class="paragraph"><p>By default, Flume checks for a Hadoop jar in <tt>/usr/lib/hadoop</tt>.  If it is not
present, it defaults to a jar found in its own lib directory, <tt>/usr/lib/flume/
lib</tt>.</p></div>
<h4 id="_why_doesn_8217_t_a_flume_node_appear_on_flume_master">Why doesn&#8217;t a Flume node appear on Flume Master?</h4>
<div class="paragraph"><p>If a node does not show up on the Master, you should first check to see if the
node is running.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt># jps | grep FlumeNode</tt></pre>
</div></div>
<div class="paragraph"><p>You can also check logs found in <tt>/usr/lib/flume/logs/flume-flume-node*.log</tt>.</p></div>
<div class="paragraph"><p>Common errors are error messages or warnings about being unable to contact
the masters.  This could be due to host misconfiguration, port
misconfiguration (35872 is the default heartbeat port), or fire walling
problems.</p></div>
<div class="paragraph"><p>Another possible error is to have a permissions problems with the local
machine&#8217;s write-ahead log directory.  On an out-of-the-box setup, this is in
the <tt>/tmp/flume/agent</tt> directory).  If a Flume Node is ever run as a user
other than <tt>flume</tt>, (especially if it was run as <tt>root</tt>), the directory needs
to be either deleted or its contents must have its permissions modified to
allow Flume to use it.</p></div>
<h4 id="_why_is_the_state_of_a_flume_node_changing_rapidly">Why is the state of a Flume node changing rapidly?</h4>
<div class="paragraph"><p>Nodes by default start and choose their <tt>hostname</tt> as their physical node
name.  Physical nodes names are supposed to be unique.  Unexpected results may
occur if multiple physical nodes are assigned the same name.</p></div>
<h4 id="_where_are_the_logs_for_troubleshooting_flume_itself">Where are the logs for troubleshooting Flume itself?</h4>
<div class="paragraph"><p>On Ubuntu-based installations, logs are written to <tt>/usr/lib/logs/</tt>.</p></div>
<div class="dlist"><dl>
<dt class="hdlist1">
Master logs   
</dt>
<dd>
<p>
<tt>/usr/lib/logs/flume-flume-master-<em>host</em>.log</tt>
</p>
</dd>
<dt class="hdlist1">
Master stdout 
</dt>
<dd>
<p>
<tt>/usr/lib/logs/flume-flume-master-<em>host</em>.out.*</tt>
</p>
</dd>
<dt class="hdlist1">
Node logs 
</dt>
<dd>
<p>
<tt>/usr/lib/ logs/flume-flume-node-<em>host</em>.log</tt>
</p>
</dd>
<dt class="hdlist1">
Node stdout 
</dt>
<dd>
<p>
<tt>/usr/lib/logs/flume-flume- node-<em>host</em>.out.*</tt>
</p>
</dd>
</dl></div>
<h4 id="_what_can_i_do_if_i_get_node_failure_due_to_out_of_file_handles">What can I do if I get node failure due to out of file handles?</h4>
<div class="paragraph"><p>There are two limits in Linux&#8201;&#8212;&#8201;the max number of allowable open files
(328838), and max number of allowable open files for a user (default 1024).
Sockets are file handles so this limits the number of open TCP connections
available.</p></div>
<div class="paragraph"><p>On Ubuntu, need to add a line to <tt>/etc/security/limits.conf</tt></p></div>
<div class="listingblock">
<div class="content">
<pre><tt>&lt;user&gt; hard nofile 10000</tt></pre>
</div></div>
<div class="paragraph"><p>The user should also add the following line to a <tt>~/.bash_profile</tt> to raise
the limit to the hard limit.</p></div>
<div class="listingblock">
<div class="content">
<pre><tt>ulimit -n 10000</tt></pre>
</div></div>
<h4 id="_failures_due_when_using_disk_failover_or_write_ahead_log">Failures due when using Disk Failover or Write Ahead Log</h4>
<div class="paragraph"><p>Flume currently relies on the file system for logging mechanisms. You must
make sure that the user running Flume has permissions to write to the
specified logging directory.</p></div>
<div class="paragraph"><p>Currently the default is to write to /tmp/flume.  In a production system you
should write to a directory that does not automatically get deleted on reboot.</p></div>
<h4 id="_can_i_write_data_to_amazon_s3">Can I write data to Amazon S3?</h4>
<div class="paragraph"><p>Yes.  In the collector sink or dfs sinks you can use the <tt>s3n://</tt> or <tt>s3://</tt>
prefixes to write to S3 buckets.</p></div>
<div class="paragraph"><p>First you must add some jars to your CLASSPATH in order to support s3 writing.
Here is a set that Cloudera has tested (other versions will likely work as well):</p></div>
<div class="ulist"><ul>
<li>
<p>
commons-codec-1.3.jar
</p>
</li>
<li>
<p>
commons-httpclient-3.0.1.jar
</p>
</li>
<li>
<p>
jets3t-0.6.1.jar
</p>
</li>
</ul></div>
<div class="paragraph"><p><tt>s3n</tt> uses the native s3 file system and has some limitations on individual
file sizes.  Files written with this method are compatible with other programs
like <tt>s3cmd</tt>.</p></div>
<div class="paragraph"><p><tt>s3</tt> writes using an overlay file system must go through Hadoop for file
system interaction.</p></div>
</div>
<h2 id="_glossary">Glossary</h2>
<div class="sectionbody">
<div class="dlist"><dl>
<dt class="hdlist1">
Agent
</dt>
<dd>
<p>
A Flume node located at the start of a flow that captures data from
external sources, ready for feeding downstream.
</p>
</dd>
<dt class="hdlist1">
Collector
</dt>
<dd>
<p>
A Flume node located at the end of a flow that delivers data to its eventual destination.
</p>
</dd>
<dt class="hdlist1">
Flow
</dt>
<dd>
<p>
A set of nodes wired together in sequence which together process data
from a single source into its eventual destination.
</p>
</dd>
<dt class="hdlist1">
Master
</dt>
<dd>
<p>
A service that controls the configuration of all nodes, and to
which all nodes report.
</p>
</dd>
<dt class="hdlist1">
Sink
</dt>
<dd>
<p>
The place where a node sends its data after all processing is done.
</p>
</dd>
<dt class="hdlist1">
Source
</dt>
<dd>
<p>
The place where a node gets its data stream.
</p>
</dd>
</dl></div>
</div>
<h2 id="_versions">Versions</h2>
<div class="sectionbody">
<h3 id="_history">history</h3><div style="clear:left"></div>
<div class="dlist"><dl>
<dt class="hdlist1">
v0.9.1 8/9/10
</dt>
<dd>
<p>
Improved error messages and visibility of property
configuration values.  First external contributions.  Fixed
reconfiguration hangs.  Improved implementing plugins documentation.
Updated scribe and syslog support.  Compression on output files.
</p>
</dd>
<dt class="hdlist1">
v0.9 6/29/10 
</dt>
<dd>
<p>
metrics and reporting framework, logical nodes+logical
names abstraction, wal/dfo isolation by flow, transformation-based
high level sinks.  Open source and initial push to github.
</p>
</dd>
<dt class="hdlist1">
v0.3 3/31/10 
</dt>
<dd>
<p>
ZK based master/multi-master, automatic failovers for
data and control planes.  flume shell. deb/rpm packaging.
</p>
</dd>
<dt class="hdlist1">
v0.2 1/21/09 
</dt>
<dd>
<p>
Different reliability modes: WAL 2.0, DFO, Best
effort. Output file escaping/bucketing.  Proliferation of many sink
and decorators.
</p>
</dd>
<dt class="hdlist1">
v0.1 11/23/09 
</dt>
<dd>
<p>
First installation deployment, users tests.
</p>
</dd>
<dt class="hdlist1">
v0.0 9/21/09 
</dt>
<dd>
<p>
First cut with current architecture (centralized
master, configuration language, web interface.)  First version of WAL.
Simple visualizations, samplers, thrift based rpc.
</p>
</dd>
<dt class="hdlist1">
pre history 7/21/09 
</dt>
<dd>
<p>
First commit.  design, experimental
implementations.  Initial implementation had individual agent and
collector programs, watchdog
</p>
<div class="literalblock">
<div class="content">
<pre><tt>     ______
    / ___//_  ______  ____
   / /_/ / / / /    \/ __/
  / __/ / /_/ / / / / __/
 / / /_/\____/_/_/_/\__/
/_/ Distributed Log Collection.</tt></pre>
</div></div>
</dd>
</dl></div>
</div>
</div>
<div id="footnotes"><hr /></div>
<div id="footer">
<div id="footer-text">
Version 0.9.1<br />
Last updated 2010-10-19 05:28:08 PDT
</div>
</div>
</body>
</html>
